
<!DOCTYPE html>

<html data-content_root="../../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>Optimizing with Composable Kernel — ROCm Documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../_static/pygments.css?v=a746c00c" rel="stylesheet" type="text/css"/>
<link href="../../_static/styles/sphinx-book-theme.css?v=a3416100" rel="stylesheet" type="text/css"/>
<link href="../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../_static/custom.css?v=da61d430" rel="stylesheet" type="text/css"/>
<link href="../../_static/rocm_header.css?v=4044f309" rel="stylesheet" type="text/css"/>
<link href="../../_static/rocm_footer.css?v=25204c5a" rel="stylesheet" type="text/css"/>
<link href="../../_static/fonts.css?v=fcff5274" rel="stylesheet" type="text/css"/>
<link href="../../_static/sphinx-design.min.css?v=87e54e7c" rel="stylesheet" type="text/css"/>
<link href="../../_static/rocm_custom.css?v=ace7df76" rel="stylesheet" type="text/css"/>
<link href="../../_static/rocm_rn.css?v=0e8af9ba" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/documentation_options.js?v=bc0531d1"></script>
<script src="../../_static/doctools.js?v=9a2dae69"></script>
<script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../_static/copybutton.js?v=f281be69"></script>
<script async="async" src="../../_static/code_word_breaks.js?v=327952c4"></script>
<script async="async" src="../../_static/renameVersionLinks.js?v=929fe5e4"></script>
<script async="async" src="../../_static/rdcMisc.js?v=01f88d96"></script>
<script async="async" src="../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
<script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
<script src="../../_static/design-tabs.js?v=f930bc37"></script>
<script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'how-to/llm-fine-tuning-optimization/optimizing-with-composable-kernel';</script>
<script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
<link href="rocm-stg.amd.com/how-to/llm-fine-tuning-optimization/optimizing-with-composable-kernel.html" rel="canonical"/>
<link href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico" rel="icon"/>
<link href="../../genindex.html" rel="index" title="Index"/>
<link href="../../search.html" rel="search" title="Search"/>
<link href="optimizing-triton-kernel.html" rel="next" title="Optimizing Triton kernels"/>
<link href="llm-inference-frameworks.html" rel="prev" title="LLM inference frameworks"/>
<meta content="vo35SZt_GASsTHAEmdww7AYKPCvZyzLvOXBl8guBME4" name="google-site-verification"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<aside aria-label="Announcement" class="bd-header-announcement">
<div class="bd-header-announcement__content">This page contains proposed changes for a future release of ROCm. Read the <a href="https://rocm.docs.amd.com/en/latest/" id="rocm-banner">latest Linux release of ROCm documentation</a> for your production environments.</div>
</aside>
<header class="common-header">
<nav class="navbar navbar-expand-xl">
<div class="container-fluid main-nav rocm-header">
<button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler collapsed" data-bs-target="#navbarSupportedContent" data-bs-toggle="collapse" data-tracking-information="mainMenuToggle" id="nav-icon" type="button">
<span></span>
<span></span>
<span></span>
</button>
<div class="header-logo">
<a class="navbar-brand" href="https://www.amd.com/">
<img alt="AMD Logo" class="d-inline-block align-text-top hover-opacity" src="../../_static/images/amd-header-logo.svg" title="AMD Logo" width="90"/>
</a>
<div class="vr vr mx-40 my-25"></div>
<a class="klavika-font hover-opacity" href="https://rocm.docs.amd.com/en/develop">ROCm™ Software Future Release</a>
<a class="header-all-versions" href="https://rocm.docs.amd.com/en/latest/release/versions.html">Version List</a>
</div>
<div class="icon-nav text-center d-flex ms-auto">
</div>
</div>
</nav>
<nav class="navbar navbar-expand-xl second-level-nav">
<div class="container-fluid main-nav">
<div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
<ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm" id="navgithub" role="button" target="_blank">
                                GitHub
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm/discussions" id="navcommunity" role="button" target="_blank">
                                Community
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://rocm.blogs.amd.com/" id="navblogs" role="button" target="_blank">
                                Blogs
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://www.amd.com/en/developer/resources/infinity-hub.html" id="navinfinity-hub" role="button" target="_blank">
                                Infinity Hub
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm/issues/new/choose" id="navsupport" role="button" target="_blank">
                                Support
                            </a>
</li>
</ul>
</div>
</div>
</nav>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<a class="navbar-brand logo" href="../../index.html">
<p class="title logo__title">ROCm Documentation</p>
</a></div>
<div class="sidebar-primary-item">
<script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
<div class="sidebar-primary-item"><nav aria-label="Main" class="bd-links bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../what-is-rocm.html">What is ROCm?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/release-notes.html">Release notes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Install</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/develop/">ROCm on Linux</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-windows/en/develop/">HIP SDK on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep-learning-rocm.html">Deep learning frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build-rocm.html">Build ROCm from source</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">How to</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../rocm-for-ai/index.html">Using ROCm for AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../rocm-for-ai/install.html">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rocm-for-ai/train-a-model.html">Training a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rocm-for-ai/hugging-face-models.html">Running models from Hugging Face</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rocm-for-ai/deploy-your-model.html">Deploying your model</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../rocm-for-hpc/index.html">Using ROCm for HPC</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Fine-tuning LLMs and inference optimization</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="overview.html">Conceptual overview</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="fine-tuning-and-inference.html">Fine-tuning and inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="single-gpu-fine-tuning-and-inference.html">Using a single accelerator</a></li>
<li class="toctree-l3"><a class="reference internal" href="multi-gpu-fine-tuning-and-inference.html">Using multiple accelerators</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="model-quantization.html">Model quantization techniques</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-acceleration-libraries.html">Model acceleration libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="llm-inference-frameworks.html">LLM inference frameworks</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Optimizing with Composable Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimizing-triton-kernel.html">Optimizing Triton kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="profiling-and-debugging.html">Profiling and debugging</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../system-optimization/index.html">System optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../system-optimization/mi300x.html">AMD Instinct MI300X</a></li>
<li class="toctree-l2"><a class="reference internal" href="../system-optimization/mi300a.html">AMD Instinct MI300A</a></li>
<li class="toctree-l2"><a class="reference internal" href="../system-optimization/mi200.html">AMD Instinct MI200</a></li>
<li class="toctree-l2"><a class="reference internal" href="../system-optimization/mi100.html">AMD Instinct MI100</a></li>
<li class="toctree-l2"><a class="reference internal" href="../system-optimization/w6000-v620.html">AMD RDNA 2</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tuning-guides/mi300x/index.html">AMD MI300X performance validation and tuning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../performance-validation/mi300x/vllm-benchmark.html">Performance validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tuning-guides/mi300x/system.html">System tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tuning-guides/mi300x/workload.html">Workload tuning</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/gpu-cluster-networking/en/develop/index.html">GPU cluster networking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-enabled-mpi.html">Using MPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../system-debugging.html">System debugging</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../conceptual/compiler-topics.html">Using advanced compiler features</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/index.html">ROCm compiler infrastructure</a></li>
<li class="toctree-l2"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/using-gpu-sanitizer.html">Using AddressSanitizer</a></li>
<li class="toctree-l2"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/openmp.html">OpenMP support</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../setting-cus.html">Setting the number of CUs</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/amd/rocm-examples">ROCm examples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Compatibility</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../compatibility/compatibility-matrix.html">Compatibility matrix</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/develop/reference/system-requirements.html">Linux system requirements</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-windows/en/develop/reference/system-requirements.html">Windows system requirements</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/develop/reference/3rd-party-support-matrix.html">Third-party support</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/user-kernel-space-compat-matrix.html">User and kernel-space support matrix</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/docker-image-support-matrix.html">Docker image support matrix</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/radeon/en/latest/index.html">Use ROCm on Radeon GPUs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Conceptual</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../conceptual/gpu-arch.html">GPU architecture overview</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../conceptual/gpu-arch/mi300.html">MI300 microarchitecture</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf">AMD Instinct MI300/CDNA3 ISA</a></li>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf">White paper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../conceptual/gpu-arch/mi300-mi200-performance-counters.html">MI300 and MI200 Performance counter</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../conceptual/gpu-arch/mi250.html">MI250 microarchitecture</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/instinct-mi200-cdna2-instruction-set-architecture.pdf">AMD Instinct MI200/CDNA2 ISA</a></li>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/documents/amd-cdna2-white-paper.pdf">White paper</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../conceptual/gpu-arch/mi100.html">MI100 microarchitecture</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/instinct-mi100-cdna1-shader-instruction-set-architecture%C2%A0.pdf">AMD Instinct MI100/CDNA1 ISA</a></li>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/documents/amd-cdna-whitepaper.pdf">White paper</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/gpu-memory.html">GPU memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/file-reorg.html">File structure (Linux FHS)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/gpu-isolation.html">GPU isolation techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/cmake-packages.html">Using CMake</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/More-about-how-ROCm-uses-PCIe-Atomics.html">ROCm &amp; PCIe atomics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/ai-pytorch-inception.html">Inception v3 with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/oversubscription.html">Oversubscription of hardware resources</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../reference/api-libraries.html">ROCm libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/rocm-tools.html">ROCm tools, compilers, and runtimes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/gpu-arch-specs.html">Accelerator and GPU hardware specifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/precision-support.html">Precision support</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contribute</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../contribute/contributing.html">Contributing to the ROCm docmentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../contribute/toolchain.html">ROCm documentation toolchain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../contribute/building.html">Building documentation</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/feedback.html">Providing feedback about the ROCm documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/license.html">ROCm licenses</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__primary" title="Toggle primary sidebar">
<span class="fa-solid fa-angle-right"></span>
</label></div>
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="index.html">Fine-tuning LLMs and inference optimization</a></li>
<li aria-current="page" class="breadcrumb-item active">Optimizing...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="article-header-buttons">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>
<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</button>
</div></div>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>Optimizing with Composable Kernel</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-overview-a-ck-gemm-instance">High-level overview: a CK GEMM instance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#template-parameter-definition">Template parameter definition</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-data-precision">Matrix data precision</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-data-layout">Matrix data layout</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-element-operation">Matrix element operation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tunable-parameters">Tunable parameters</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instantiating-and-running-the-templated-kernel">Instantiating and running the templated kernel</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-fused-int8-kernels-for-smoothquant-models">Developing fused INT8 kernels for SmoothQuant models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#operation-flow-analysis">Operation flow analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-the-complete-function">Developing the complete function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binding-to-python">Binding to Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#int8-model-inference-and-performance">INT8 model inference and performance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</nav>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<head>
<meta charset="utf-8"/>
<meta content="SmoothQuant model inference on AMD Instinct MI300X using Composable Kernel" name="description"/>
<meta content="Mixed Precision, Kernel, Inference, Linear Algebra" name="keywords"/>
</head>
<section class="tex2jax_ignore mathjax_ignore" id="optimizing-with-composable-kernel">
<h1>Optimizing with Composable Kernel<a class="headerlink" href="#optimizing-with-composable-kernel" title="Link to this heading">#</a></h1><div class="sd-container-fluid sd-sphinx-override sd-p-0 sd-mt-2 sd-mb-4 sd-p-2 sd-rounded-1 docutils" id="rocm-docs-core-article-info">
<div class="sd-row sd-row-cols-2 sd-gx-2 sd-gy-1 docutils">
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils" style="color:gray;">
    Applies to Linux
</div>
<div class="sd-col sd-d-flex-row sd-align-minor-center docutils">
<div class="sd-container-fluid sd-sphinx-override docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-3 sd-row-cols-md-3 sd-row-cols-lg-3 sd-gx-3 sd-gy-1 docutils">
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;"></p>
</div>
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;"><span class="sd-pr-2"><svg aria-hidden="true" class="sd-octicon sd-octicon-calendar" height="16.0px" version="1.1" viewbox="0 0 16 16" width="16.0px"><path d="M4.75 0a.75.75 0 01.75.75V2h5V.75a.75.75 0 011.5 0V2h1.25c.966 0 1.75.784 1.75 1.75v10.5A1.75 1.75 0 0113.25 16H2.75A1.75 1.75 0 011 14.25V3.75C1 2.784 1.784 2 2.75 2H4V.75A.75.75 0 014.75 0zm0 3.5h8.5a.25.25 0 01.25.25V6h-11V3.75a.25.25 0 01.25-.25h2zm-2.25 4v6.75c0 .138.112.25.25.25h10.5a.25.25 0 00.25-.25V7.5h-11z" fill-rule="evenodd"></path></svg></span>2024-07-22</p>
</div>
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;"><span class="sd-pr-2"><svg aria-hidden="true" class="sd-octicon sd-octicon-clock" height="16.0px" version="1.1" viewbox="0 0 16 16" width="16.0px"><path d="M1.5 8a6.5 6.5 0 1113 0 6.5 6.5 0 01-13 0zM8 0a8 8 0 100 16A8 8 0 008 0zm.5 4.75a.75.75 0 00-1.5 0v3.5a.75.75 0 00.471.696l2.5 1a.75.75 0 00.557-1.392L8.5 7.742V4.75z" fill-rule="evenodd"></path></svg></span>26 min read time</p>
</div>
</div>
</div>
</div>
</div>
</div>

<p>The AMD ROCm Composable Kernel (CK) library provides a programming model for writing performance-critical kernels for machine learning workloads. It generates a general-purpose kernel during the compilation phase through a C++ template, enabling developers to achieve operation fusions on different data precisions.</p>
<p>This article gives a high-level overview of CK General Matrix Multiplication (GEMM) kernel based on the design example of <code class="docutils literal notranslate"><span class="pre">03_gemm_bias_relu</span></code>. It also outlines the steps to construct the kernel and run it. Moreover, the article provides a detailed implementation of running SmoothQuant quantized INT8 models on AMD Instinct MI300X accelerators using CK.</p>
<section id="high-level-overview-a-ck-gemm-instance">
<h2>High-level overview: a CK GEMM instance<a class="headerlink" href="#high-level-overview-a-ck-gemm-instance" title="Link to this heading">#</a></h2>
<p>GEMM is a fundamental block in linear algebra, machine learning, and deep neural networks. It is defined as the operation:
<span class="math notranslate nohighlight">\(E = α \times (A \times B) + β \times (D)\)</span>, with A and B as matrix inputs, α and β as scalar inputs, and D as a pre-existing matrix.
Take the commonly used linear transformation in a fully connected layer as an example. These terms correspond to input activation (A), weight (B), bias (D), and output (E), respectively. The example employs a <code class="docutils literal notranslate"><span class="pre">DeviceGemmMultipleD_Xdl_CShuffle</span></code> struct from CK library as the fundamental instance to explore the compute capability of AMD Instinct accelerators for the computation of GEMM. The implementation of the instance contains two phases:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#template-parameter-definition">Template parameter definition</a></p></li>
<li><p><a class="reference internal" href="#instantiating-and-running-the-templated-kernel">Instantiating and running the templated kernel</a></p></li>
</ul>
<section id="template-parameter-definition">
<h3>Template parameter definition<a class="headerlink" href="#template-parameter-definition" title="Link to this heading">#</a></h3>
<p>The template parameters of the instance are grouped into four parameter types:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#matrix-data-precision"><span class="std std-ref">Parameters for determining matrix data precision</span></a></p></li>
<li><p><a class="reference internal" href="#matrix-data-layout"><span class="std std-ref">Parameters for determining matrix data layout</span></a></p></li>
<li><p><a class="reference internal" href="#matrix-element-operation"><span class="std std-ref">Parameters for determining extra operations on matrix elements</span></a></p></li>
<li><p><a class="reference internal" href="#tunable-parameters"><span class="std std-ref">Performance-oriented tunable parameters</span></a></p></li>
</ul>
<!-- 
================
 ### Figure 2
================ -->
<figure class="align-default" id="id5">
<img alt="../../_images/ck-template_parameters.jpg" src="../../_images/ck-template_parameters.jpg"/>
<figcaption>
<p><span class="caption-text">The template parameters of the selected GEMM kernel are classified into four groups. These template parameter groups should be defined properly before running the instance.</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="matrix-data-precision">
<span id="id1"></span><h4>Matrix data precision<a class="headerlink" href="#matrix-data-precision" title="Link to this heading">#</a></h4>
<p>A, B, D, and E are defined as half-precision floating-point datatypes. The multiply-add results of matrix A and B are added with a pre-existing matrix D (half-precision), and the final GEMM results are also half-precision floating-points.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">ADataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">F16</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">BDataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">F16</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">AccDataType</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="n">F32</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">CShuffleDataType</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">F16</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">DDataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">F16</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">EDataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">F16</span><span class="p">;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">ADataType</span></code> and <code class="docutils literal notranslate"><span class="pre">BDataType</span></code> denote the data precision of the A and B input matrices. <code class="docutils literal notranslate"><span class="pre">AccDataType</span></code> determines the data precision used for representing the multiply-add results of A and B elements. These results are stored in a <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> module in local data share (LDS), a low-latency and high-bandwidth explicitly-addressed memory used for synchronization within a workgroup LDS for later use.</p>
<p><code class="docutils literal notranslate"><span class="pre">CShuffleDataType</span></code> denotes the data precision of <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> in LDS.</p>
<p><code class="docutils literal notranslate"><span class="pre">DDataType</span></code> denotes the data precision of the pre-existing D matrix stored in GPU global memory, while <code class="docutils literal notranslate"><span class="pre">EDatatype</span></code> denotes the data precision of the final output. The CK kernel supports a fusion strategy so that <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> can be added with a single pre-existing matrix in the same GPU kernel for better performance.</p>
</section>
<section id="matrix-data-layout">
<span id="id2"></span><h4>Matrix data layout<a class="headerlink" href="#matrix-data-layout" title="Link to this heading">#</a></h4>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">ALayout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Row</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">BLayout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Col</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">DLayout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Row</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">ELayout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Row</span><span class="p">;</span>
</pre></div>
</div>
<p>Following the convention of various linear algebra libraries, CK assumes that the input matrix A is an M x K matrix, meaning the matrix has M rows and K columns. Similarly, matrix B is assumed to be K x N, meaning it has K rows and N columns. In computing, row-major order and column-major order are commonly used ways to store matrices in linear storage. After understanding the matrix storage pattern, the underlying optimized memory access manner can be applied to achieve better performance depending on the storage ordering of these matrices.</p>
</section>
<section id="matrix-element-operation">
<span id="id3"></span><h4>Matrix element operation<a class="headerlink" href="#matrix-element-operation" title="Link to this heading">#</a></h4>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">AElementOp</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="n">PassThrough</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">BElementOp</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="n">PassThrough</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">CDEElementOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">AddRelu</span><span class="p">;</span>
</pre></div>
</div>
<p>CK supports the pre-processing of the matrix before calculating GEMM, that is, <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">AElementOp(A)</span> <span class="pre">*</span> <span class="pre">BElementOp(B)</span></code>. It similarly supports the post-processing of GEMM results the same way, that is, <code class="docutils literal notranslate"><span class="pre">E</span> <span class="pre">=</span> <span class="pre">CDEElementOp(C,</span> <span class="pre">D)</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">AElementOp</span></code> and <code class="docutils literal notranslate"><span class="pre">BElementOp</span></code> determine the operation applied to matrix A and B separately before GEMM, which is achieved by binding the operation with a C++ struct function.</p>
<p>The above <code class="docutils literal notranslate"><span class="pre">PassThrough</span></code> denotes no operations are performed on the target matrix. <code class="docutils literal notranslate"><span class="pre">CDEELementOp</span></code> determines the operations applied to <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> output and matrix D. The following binding struct <code class="docutils literal notranslate"><span class="pre">AddRelu</span></code> shows an example of adding the <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> output and matrix D, and ReLU (Rectified Linear Unit) operations to the addition result. It then passes the results to matrix E.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">AddRelu</span>
<span class="p">{</span>
<span class="w">    </span><span class="n">__host__</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="k">operator</span><span class="p">()(</span><span class="n">ck</span><span class="o">::</span><span class="n">half_t</span><span class="o">&amp;</span><span class="w"> </span><span class="n">e</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">half_t</span><span class="o">&amp;</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">half_t</span><span class="o">&amp;</span><span class="w"> </span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="k">const</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">half_t</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">d</span><span class="p">;</span>
<span class="w">        </span><span class="n">e</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
</section>
<section id="tunable-parameters">
<span id="id4"></span><h4>Tunable parameters<a class="headerlink" href="#tunable-parameters" title="Link to this heading">#</a></h4>
<p>The CK instance includes a series of tunable template parameters to control the parallel granularity of the workload to achieve load balancing on different hardware platforms.</p>
<p>These parameters include Block Size, M/N/K Per Block, M/N per XDL, AK1, BK1, etc.</p>
<ul class="simple">
<li><p>Block Size determines the number of threads in the thread block.</p></li>
<li><p>M/N/K Per Block determines the size of tile that each thread block is responsible for calculating.</p></li>
<li><p>M/N Per XDL refers to M/N size for Instinct accelerator Matrix Fused Multiply Add (MFMA) instructions operating on a per-wavefront basis.</p></li>
<li><p>A/B K1 is related to the data type. It can be any value ranging from 1 to K Per Block. To achieve the optimal load/store performance, 128bit per load is suggested. In addition, the A/B loading parameters must be changed accordingly to match the A/B K1 value; otherwise, it will result in compilation errors.</p></li>
</ul>
<p>Conditions for achieving computational load balancing on different hardware platforms can vary.</p>
</section>
</section>
<section id="instantiating-and-running-the-templated-kernel">
<h3>Instantiating and running the templated kernel<a class="headerlink" href="#instantiating-and-running-the-templated-kernel" title="Link to this heading">#</a></h3>
<p>After determining the template parameters, we instantiate the kernel with actual arguments. Do one of the following:</p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">GetDeviceBuffer</span></code> from CK’s custom struct <code class="docutils literal notranslate"><span class="pre">DeviceMem</span></code> to pass the element values of the matrices that need to be calculated.</p></li>
<li><p>Allocate device buffer via <code class="docutils literal notranslate"><span class="pre">hipMalloc</span></code>. Ensure the device buffer size can fit the matrix size.</p></li>
<li><p>Pass matrix elements through the <code class="docutils literal notranslate"><span class="pre">data_ptr</span></code> method in the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> object if the matrix to be calculated is of <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> type.</p></li>
</ul>
<p>The row and column, and stride information of input matrices are also passed to the instance. For batched GEMM, you must pass in additional batch count and batch stride values. The extra operations for pre and post-processing are also passed with an actual argument; for example, α and β for GEMM scaling operations. Afterward, the instantiated kernel is launched by the invoker, as illustrated in Figure 3.</p>
<!-- 
================
 ### Figure 3
================ -->
<figure class="align-default" id="id6">
<img alt="../../_images/ck-kernel_launch.jpg" src="../../_images/ck-kernel_launch.jpg"/>
<figcaption>
<p><span class="caption-text">Templated kernel launching consists of kernel instantiation, making arguments by passing in actual application parameters, creating an invoker, and running the instance through the invoker.</span><a class="headerlink" href="#id6" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="developing-fused-int8-kernels-for-smoothquant-models">
<h2>Developing fused INT8 kernels for SmoothQuant models<a class="headerlink" href="#developing-fused-int8-kernels-for-smoothquant-models" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://github.com/mit-han-lab/smoothquant">SmoothQuant</a> (SQ) is a quantization algorithm that enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLM. The required GPU kernel functionalities used to accelerate the inference of SQ models on Instinct accelerators are shown in the following table.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id7">
<caption><span class="caption-text">Functionalities used to implement SmoothQuant model inference.</span><a class="headerlink" href="#id7" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Functionality descriptions</p></th>
<th class="head"><p>Corresponding wrappers</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(E = α \times (A \times B) + β \times (D)\)</span>, where A, B, D, E are INT8 2-D tensors;</p></td>
<td><p>E = Linear_ABDE_I8(A, B, D, <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="math notranslate nohighlight">\(E = RELU (α \times (A \times B) + β \times (D))\)</span>, where A, B, D, E are INT8 2-D tensors;</p></td>
<td><p>E = Linear_ReLU_ABDE_I8(A, B, D, <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(E = α \times (A \times B) + β \times (D)\)</span>, where A, B are INT8 2-D tensors, D and E are FP32 2-D tensors;</p></td>
<td><p>E = Linear_AB_I8_DE_F32(A, B, D, <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="math notranslate nohighlight">\(E = α \times (A \times B)\)</span>, where A, B, E are INT8 3-D tensors;</p></td>
<td><p>E = BMM_ABE_I8(A, B, <span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(E = α \times (A \times B)\)</span>, where A, B are INT8 3-D tensors, E is FP32 3-D tensor;</p></td>
<td><p>E = BMM_AB_I8_E_F32(A, B, <span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
</tr>
</tbody>
</table>
</div>
<section id="operation-flow-analysis">
<h3>Operation flow analysis<a class="headerlink" href="#operation-flow-analysis" title="Link to this heading">#</a></h3>
<p>The following section discusses the analysis of the operation flow of <code class="docutils literal notranslate"><span class="pre">Linear_ReLU_ABDE_I8</span></code>. The rest of the wrappers in Table 1 can be analyzed similarly.</p>
<p>The first operation in the process is to perform the multiplication of input matrices A and B. The resulting matrix C is then scaled with α to obtain T1. At the same time, the process performs a scaling operation on D elements to obtain T2. Afterward, the process performs matrix addition between T1 and T2, element activation calculation using ReLU, and element rounding sequentially. The operations to generate E1, E2, and E are encapsulated and completed by a user-defined template function in CK (given in the next sub-section). This template function is integrated into the fundamental instance directly during the compilation phase so that all these steps can be fused in a single GPU kernel.</p>
<!-- 
================
 ### Figure 4
================ -->
<figure class="align-default" id="id8">
<img alt="../../_images/ck-operation_flow.jpg" src="../../_images/ck-operation_flow.jpg"/>
<figcaption>
<p><span class="caption-text">Operation flow.</span><a class="headerlink" href="#id8" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The CK library contains many fundamental instances that implement different functions. Familiarize yourself with the names of various CK instances and determine whether they meet the target functional requirements.</p>
<p>Second, consider whether the format of input data meets your actual calculation needs. For SQ models, the 8-bit integer data format (INT8) is applied for matrix calculations.</p>
<p>Third, consider the platform for implementing CK instances. The instances suffixed with <code class="docutils literal notranslate"><span class="pre">xdl</span></code> only run on AMD Instinct accelerators after being compiled and cannot run on Radeon-series GPUs. This is due to the underlying device-specific instruction sets for implementing these basic instances.</p>
<p>Here, we use <a class="reference external" href="https://github.com/ROCm/composable_kernel/tree/develop/example/24_batched_gemm">DeviceBatchedGemmMultiD_Xdl</a> as the fundamental instance to implement the functionalities in the previous table.</p>
<!-- 
================
 ### Figure 5
================ -->
<figure class="align-default" id="id9">
<img alt="../../_images/ck-root_instance.jpg" src="../../_images/ck-root_instance.jpg"/>
<figcaption>
<p><span class="caption-text">Use the ‘DeviceBatchedGemmMultiD_Xdl’ instance as a root.</span><a class="headerlink" href="#id9" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The <code class="docutils literal notranslate"><span class="pre">DeviceBatchedGemmMultiD_Xdl</span></code> instance realizes the batched GEMM <code class="docutils literal notranslate"><span class="pre">BMM_ABE_I8</span></code> and <code class="docutils literal notranslate"><span class="pre">BMM_AB_I8_E_F32</span></code> kernels directly by using the proper input and output data precision types.</p>
<p>Based on the two batched GEMM kernels, GEMM kernel <code class="docutils literal notranslate"><span class="pre">Linear_ABDE_I8</span></code> and <code class="docutils literal notranslate"><span class="pre">Linear_AB_I8_DE_F32</span></code> can be implemented by expanding their input 2-D tensors to 3-D tensors. Then, the 3-D output tensors produced by the root instance are squeezed back to 2-D output tensors before returning back.</p>
<p>For example, unsqueeze A (M, K) to A (1, M, K) before assigning it into the root instance and squeeze E (1, M, N) to (M, N) after the calculations of the root instance return back. <code class="docutils literal notranslate"><span class="pre">Linear_ReLU_ABDE_I8</span></code> is implemented by adding a ReLU operation on the result output of <code class="docutils literal notranslate"><span class="pre">Linear_ABDE_I8</span></code>.</p>
</section>
<section id="developing-the-complete-function">
<h3>Developing the complete function<a class="headerlink" href="#developing-the-complete-function" title="Link to this heading">#</a></h3>
<p>The inference of SQ quantized models relies on using PyTorch and Transformer libraries, and a tensor type is used to represent matrices and vectors in <code class="docutils literal notranslate"><span class="pre">torch</span></code>, the C++ data types in CK need to be replaced with the <code class="docutils literal notranslate"><span class="pre">torch::tensor</span></code> type. The data types of the input and output matrices should be a <code class="docutils literal notranslate"><span class="pre">tensor</span></code> type.</p>
<p>In GEMM, the A and B inputs are two-dimensional matrices, and the required input matrices of the selected fundamental CK instance are three-dimensional matrices. Therefore, we must convert the input 2-D tensors to 3-D tensors, by using <code class="docutils literal notranslate"><span class="pre">tensor</span></code>’s <code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> method before passing these matrices to the instance. For batched GEMM in the preceding table, ignore this step.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Function input and output </span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">linear_relu_abde_i8</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">A_</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">B_</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">D_</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">  </span><span class="c1">// Convert torch::Tensor A_ (M, K) to torch::Tensor A (1, M, K) </span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A_</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Convert torch::Tensor B_ (K, N) to torch::Tensor A (1, K, N) </span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B_</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="p">...</span>
</pre></div>
</div>
<p>As shown in the following code block, we obtain M, N, and K values using input tensor size values. This stride size information is used to reshape the input vector D and allocate the storage space of tensor E. Stride reflects the exact size of continuous elements in memory, which are passed as important parameters to the fundamental instance for GPU kernel use.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Return the batch count from the size of dimension 0</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Return the M, N, K from the size of dimension 1 &amp; 2</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Initialize the stride size for A, B, D and E</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">stride_A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">stride_B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">stride_D0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">stride_E</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Initialize the stride size for batched A, B, D and E</span>
<span class="w">  </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_stride_A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">K</span><span class="p">;</span>
<span class="w">  </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_stride_B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">;</span>
<span class="w">  </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_stride_D0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">;</span>
<span class="w">  </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_stride_E</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Convert the tensor of 2-D to 3-D</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">D_</span><span class="p">.</span><span class="n">view</span><span class="p">({</span><span class="mi">1</span><span class="p">,</span><span class="mi">-1</span><span class="p">}).</span><span class="n">repeat</span><span class="p">({</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">});</span>

<span class="w">  </span><span class="c1">// Allocate memory for E</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">empty</span><span class="p">({</span><span class="n">batch_count</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">},</span><span class="w"> </span>
<span class="w">       </span><span class="n">torch</span><span class="o">::</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kInt8</span><span class="p">).</span><span class="n">device</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">device</span><span class="p">()));</span>
</pre></div>
</div>
<p>In the following code block, <code class="docutils literal notranslate"><span class="pre">ADataType</span></code>, <code class="docutils literal notranslate"><span class="pre">BDataType</span></code> and <code class="docutils literal notranslate"><span class="pre">D0DataType</span></code> are used to denote the data precision of the input tensors A, B and D, respectively. <code class="docutils literal notranslate"><span class="pre">EDataType</span></code> is used to denote the data precision of output tensor E. These parameters are specified to <code class="docutils literal notranslate"><span class="pre">I8</span></code> data format (8-bit integer data format) to meet the kernel’s design requirements.</p>
<p><code class="docutils literal notranslate"><span class="pre">AccDataType</span></code> determines the data precision used to represent the multiply-add results of A and B elements. Generally, a larger range data type is applied to store the multiply-add results of A and B to avoid result overflow; <code class="docutils literal notranslate"><span class="pre">I32</span></code> is applied in this case. The <code class="docutils literal notranslate"><span class="pre">CShuffleDataType</span> <span class="pre">I32</span></code> data type indicates that the multiply-add results continue to be stored in LDS as an <code class="docutils literal notranslate"><span class="pre">I32</span></code> data format. All of this is implemented through the following code block.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Data precision </span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ADataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">I8</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">BDataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">I8</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">AccDataType</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="n">I32</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">CShuffleDataType</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">I32</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">D0DataType</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="n">I8</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">DsDataType</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">Tuple</span><span class="o">&lt;</span><span class="n">D0DataType</span><span class="o">&gt;</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">EDataType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">I8</span><span class="p">;</span>
</pre></div>
</div>
<p>Following the convention of various linear algebra libraries, row-major and column-major orders are used to denote the ways of storing matrices in linear storage. The advantage of specifying matrix B as column major is that all the relevant matrix elements are stored continuously in GPU global memory when a row in A is multiplied by a column in B, which can help GPU achieve data consistency access to improve access performance.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Specify tensor order</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ALayout</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">RowMajor</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">BLayout</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">ColumnMajor</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">D0Layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">RowMajor</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">DsLayout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">Tuple</span><span class="o">&lt;</span><span class="n">D0Layout</span><span class="o">&gt;</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ELayout</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">RowMajor</span><span class="p">;</span>
</pre></div>
</div>
<p>In CK, <code class="docutils literal notranslate"><span class="pre">PassThrough</span></code> is a struct denoting if an operation is applied to the tensor it binds to. To fuse the operations between E1, E2, and E introduced in section <a class="reference internal" href="#operation-flow-analysis">Operation flow analysis</a>, we define a custom C++ struct, <code class="docutils literal notranslate"><span class="pre">ScaleScaleAddRelu</span></code>, and bind it to <code class="docutils literal notranslate"><span class="pre">CDEELementOp</span></code>. It determines the operations that will be applied to <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> (A×B results), tensor D, α, and β.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// No operations bound to the elements of A and B </span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">AElementOp</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="n">PassThrough</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">BElementOp</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="n">PassThrough</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Operations bound to the elements of C, D and E</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">CDEElementOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ScaleScaleAddRelu</span><span class="p">;</span>
</pre></div>
</div>
<p>In the binding struct, <code class="docutils literal notranslate"><span class="pre">operator()</span></code> performs an addition operation between <code class="docutils literal notranslate"><span class="pre">CShuffle</span></code> and matrix D, a ReLU operation on the addition results, and a rounding operation on the output elements. It then returns the results to E.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">ScaleScaleAddRelu</span><span class="w"> </span><span class="p">{</span>

<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;&gt;</span>
<span class="w">  </span><span class="n">__host__</span><span class="w"> </span><span class="n">__device__</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">void</span>
<span class="w">  </span><span class="k">operator</span><span class="p">()</span><span class="o">&lt;</span><span class="n">I8</span><span class="p">,</span><span class="w"> </span><span class="n">I32</span><span class="p">,</span><span class="w"> </span><span class="n">I8</span><span class="o">&gt;</span><span class="p">(</span><span class="n">I8</span><span class="o">&amp;</span><span class="w"> </span><span class="n">e</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">I32</span><span class="o">&amp;</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">I8</span><span class="o">&amp;</span><span class="w"> </span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="k">const</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// Scale AxB result with alpha</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">F32</span><span class="w"> </span><span class="n">c_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">type_convert</span><span class="o">&lt;</span><span class="n">F32</span><span class="o">&gt;</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">alpha</span><span class="p">;</span>

<span class="w">      </span><span class="c1">// Scale D with beta</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">F32</span><span class="w"> </span><span class="n">d_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">type_convert</span><span class="o">&lt;</span><span class="n">F32</span><span class="o">&gt;</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">beta</span><span class="p">;</span>

<span class="w">      </span><span class="c1">// Perform addition operation</span>
<span class="w">      </span><span class="n">F32</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c_scale</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">d_scale</span><span class="p">;</span>
<span class="w">      </span>
<span class="w">      </span><span class="c1">// Perform RELU operation</span>
<span class="w">      </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">      </span><span class="c1">// Perform rounding operation </span>
<span class="w">      </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">127</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="mi">127</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">temp</span><span class="p">;</span>
<span class="w">      </span>
<span class="w">      </span><span class="c1">// Return to E</span>
<span class="w">      </span><span class="n">e</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">type_convert</span><span class="o">&lt;</span><span class="n">I8</span><span class="o">&gt;</span><span class="p">(</span><span class="n">temp</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">    </span>
<span class="w">  </span><span class="n">F32</span><span class="w"> </span><span class="n">alpha</span><span class="p">;</span>
<span class="w">  </span><span class="n">F32</span><span class="w"> </span><span class="n">beta</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The original input tensors need to be padded to meet GPU tile-based parallelism.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">static</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">GemmDefault</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">tensor_operation</span><span class="o">::</span><span class="n">device</span><span class="o">::</span><span class="n">GemmSpecialization</span><span class="o">::</span><span class="n">MNKPadding</span><span class="p">;</span>
</pre></div>
</div>
<p>The template parameters of the target fundamental instance are initialized with the above parameters and includes default tunable parameters. For specific tuning methods, see <a class="reference internal" href="#tunable-parameters">Tunable parameters</a>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">DeviceOpInstance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ck</span><span class="o">::</span><span class="n">tensor_operation</span><span class="o">::</span><span class="n">device</span><span class="o">::</span><span class="n">DeviceBatchedGemmMultiD_Xdl</span><span class="o">&lt;</span><span class="w"> </span>
<span class="w">    </span><span class="c1">// Tensor layout</span>
<span class="w">    </span><span class="n">ALayout</span><span class="p">,</span><span class="w"> </span><span class="n">BLayout</span><span class="p">,</span><span class="w"> </span><span class="n">DsLayout</span><span class="p">,</span><span class="w"> </span><span class="n">ELayout</span><span class="p">,</span><span class="w"> </span>
<span class="w">    </span><span class="c1">// Tensor data type</span>
<span class="w">    </span><span class="n">ADataType</span><span class="p">,</span><span class="w"> </span><span class="n">BDataType</span><span class="p">,</span><span class="w"> </span><span class="n">AccDataType</span><span class="p">,</span><span class="w"> </span><span class="n">CShuffleDataType</span><span class="p">,</span><span class="w"> </span><span class="n">DsDataType</span><span class="p">,</span><span class="w"> </span><span class="n">EDataType</span><span class="p">,</span><span class="w">  </span>
<span class="w">    </span><span class="c1">// Tensor operation</span>
<span class="w">    </span><span class="n">AElementOp</span><span class="p">,</span><span class="w">  </span><span class="n">BElementOp</span><span class="p">,</span><span class="w"> </span><span class="n">CDEElementOp</span><span class="p">,</span><span class="w">  </span>
<span class="w">    </span><span class="c1">// Padding strategy  </span>
<span class="w">    </span><span class="n">GemmDefault</span><span class="p">,</span>
<span class="w">    </span><span class="c1">// Tunable parameters        </span>
<span class="w">    </span><span class="n">tunable</span><span class="w"> </span><span class="n">parameters</span><span class="o">&gt;</span><span class="p">;</span>
</pre></div>
</div>
<p>Return the address of the first element of tensors:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">A_ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">ADataType</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">B_ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">BDataType</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">D0_ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">D</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">D0DataType</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">E_ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">E</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">EDataType</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
<p>The fundamental instance is then initialized and run with actual arguments:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">device_op</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="n">DeviceOpInstance</span><span class="p">{};</span>
<span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">invoker</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device_op</span><span class="p">.</span><span class="n">MakeInvoker</span><span class="p">();</span>
<span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">argument</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device_op</span><span class="p">.</span><span class="n">MakeArgument</span><span class="p">(</span>
<span class="w">    </span><span class="n">A_ref</span><span class="p">,</span><span class="w"> </span><span class="n">B_ref</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">D0_ref</span><span class="p">},</span><span class="w"> </span><span class="n">E_ref</span><span class="p">,</span>
<span class="w">    </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span>
<span class="w">    </span><span class="n">batch_count</span><span class="p">,</span>
<span class="w">    </span><span class="n">stride_A</span><span class="p">,</span><span class="w"> </span><span class="n">stride_B</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">stride_D0</span><span class="p">},</span><span class="w"> </span><span class="n">stride_E</span><span class="p">,</span>
<span class="w">    </span><span class="n">batch_stride_A</span><span class="p">,</span><span class="w"> </span><span class="n">batch_stride_B</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">batch_stride_D0</span><span class="p">},</span><span class="w"> </span><span class="n">batch_stride_E</span><span class="p">,</span>
<span class="w">    </span><span class="n">AElementOp</span><span class="p">{},</span><span class="w"> </span><span class="n">BElementOp</span><span class="p">{},</span><span class="w"> </span><span class="n">CDEElementOp</span><span class="p">{</span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">});</span>

<span class="n">invoker</span><span class="p">.</span><span class="n">Run</span><span class="p">(</span><span class="n">argument</span><span class="p">,</span><span class="w"> </span><span class="n">StreamConfig</span><span class="p">{</span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">});</span>
</pre></div>
</div>
<p>The output of the fundamental instance is a calculated batched matrix E (batch, M, N). Before the return, it needs to be converted to a 2-D matrix if a normal GEMM result is required.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Convert (1, M, N) to (M, N) </span>
<span class="k">return</span><span class="w"> </span><span class="n">E</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="binding-to-python">
<h3>Binding to Python<a class="headerlink" href="#binding-to-python" title="Link to this heading">#</a></h3>
<p>Since these functions are written in C++ and <code class="docutils literal notranslate"><span class="pre">torch::Tensor</span></code>, you can use <code class="docutils literal notranslate"><span class="pre">pybind11</span></code> to bind the functions and import them as Python modules. For the example, the necessary binding code for exposing the functions in the table spans but a few lines.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span>

<span class="n">PYBIND11_MODULE</span><span class="p">(</span><span class="n">TORCH_EXTENSION_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">){</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"linear_ab_i8_de_f32"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">linear_ab_i8_de_f32</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"linear_relu_abde_i8"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">linear_relu_abde_i8</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"linear_abde_i8"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">linear_abde_i8</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"bmm_abe_i8"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">bmm_abe_i8</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"bmm_ab_i8_e_f32"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">bmm_ab_i8_e_f32</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Build the C++ extension by writing a <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> script that uses <code class="docutils literal notranslate"><span class="pre">setuptools</span></code> to compile the C++ code. A reference implementation of the <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> script is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">setuptools</span> <span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">find_packages</span>
<span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">cpp_extension</span>
<span class="kn">from</span> <span class="nn">torch.utils.cpp_extension</span> <span class="kn">import</span> <span class="n">BuildExtension</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CC"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"hipcc"</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CXX"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"hipcc"</span>

<span class="n">sources</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">'torch_int/kernels/linear.cpp'</span><span class="p">,</span>
    <span class="s1">'torch_int/kernels/bmm.cpp'</span><span class="p">,</span>
    <span class="s1">'torch_int/kernels/pybind.cpp'</span><span class="p">,</span> 
<span class="p">]</span>

<span class="n">include_dirs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'torch_int/kernels/include'</span><span class="p">]</span>
<span class="n">extra_link_args</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'libutility.a'</span><span class="p">]</span>
<span class="n">extra_compile_args</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'-O3'</span><span class="p">,</span><span class="s1">'-DNDEBUG'</span><span class="p">,</span> <span class="s1">'-std=c++17'</span><span class="p">,</span> <span class="s1">'--offload-arch=gfx942'</span><span class="p">,</span> <span class="s1">'-DCK_ENABLE_INT8'</span><span class="p">,</span> <span class="s1">'-D__HIP_PLATFORM_AMD__=1'</span><span class="p">]</span>

<span class="n">setup</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">'torch_int'</span><span class="p">,</span>
    <span class="n">ext_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="n">cpp_extension</span><span class="o">.</span><span class="n">CUDAExtension</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">'torch_int.rocm'</span><span class="p">,</span>
            <span class="n">sources</span><span class="o">=</span><span class="n">sources</span><span class="p">,</span>
            <span class="n">include_dirs</span><span class="o">=</span><span class="n">include_dirs</span><span class="p">,</span>
            <span class="n">extra_link_args</span><span class="o">=</span><span class="n">extra_link_args</span><span class="p">,</span>
            <span class="n">extra_compile_args</span><span class="o">=</span><span class="n">extra_compile_args</span>
            <span class="p">),</span>
    <span class="p">],</span>
    <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">'build_ext'</span><span class="p">:</span> <span class="n">BuildExtension</span><span class="o">.</span><span class="n">with_options</span><span class="p">(</span><span class="n">use_ninja</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">},</span>
    <span class="n">packages</span><span class="o">=</span><span class="n">find_packages</span><span class="p">(</span>
        <span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s1">'notebook'</span><span class="p">,</span> <span class="s1">'scripts'</span><span class="p">,</span> <span class="s1">'tests'</span><span class="p">]),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Run <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span></code> to build and install the extension. It should look something like Figure 6:</p>
<!-- 
================
 ### Figure 6
================ -->
<figure class="align-default" id="id10">
<img alt="../../_images/ck-compilation.jpg" src="../../_images/ck-compilation.jpg"/>
<figcaption>
<p><span class="caption-text">Compilation and installation of the INT8 kernels.</span><a class="headerlink" href="#id10" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="int8-model-inference-and-performance">
<h3>INT8 model inference and performance<a class="headerlink" href="#int8-model-inference-and-performance" title="Link to this heading">#</a></h3>
<p>The implementation architecture of running SmoothQuant models on MI300X GPUs is illustrated in Figure 7, where (a) shows the decoder layer composition components of the target model, (b) shows the major implementation class for the decoder layer components, and (c) denotes the underlying GPU kernels implemented by CK instance.</p>
<!-- 
================
 ### Figure 7
================ -->
<figure class="align-default" id="id11">
<img alt="../../_images/ck-inference_flow.jpg" src="../../_images/ck-inference_flow.jpg"/>
<figcaption>
<p><span class="caption-text">The implementation architecture of running SmoothQuant models on AMD MI300X accelerators.</span><a class="headerlink" href="#id11" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For the target <a class="reference external" href="https://huggingface.co/mit-han-lab/opt-13b-smoothquant">SQ quantized model</a>, each decoder layer contains three major components: attention calculation, layer normalization, and linear transformation in fully connected layers.  The corresponding implementation classes for these components are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Int8OPTAttention</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">W8A8B8O8LinearReLU</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">W8A8BF32OF32Linear</span></code></p></li>
</ul>
<p>These classes’ underlying implementation logits will harness the functions in previous table. Note that for the example, the <code class="docutils literal notranslate"><span class="pre">LayerNormQ</span></code> module is implemented by the torch native module.</p>
<p>Testing environment:
The hardware platform used for testing equips with 256 AMD EPYC 9534 64-Core Processor, 8 AMD Instinct MI300X accelerators and 1.5T memory. The testing was done in a publicly available Docker image from Docker Hub:
<a class="reference external" href="https://hub.docker.com/layers/rocm/pytorch/rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2/images/sha256-f6ea7cee8aae299c7f6368187df7beed29928850c3929c81e6f24b34271d652b"><code class="docutils literal notranslate"><span class="pre">rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2</span></code></a></p>
<p>The tested models are OPT-1.3B, 2.7B, 6.7B and 13B FP16 models and the corresponding SmoothQuant INT8 OPT models were obtained from Hugging Face.</p>
<p>Note that since the default values were used for the tunable parameters of the fundamental instance, the performance of the INT8 kernel is suboptimal.</p>
<p>Figure 8 shows the performance comparisons between the original FP16 and the SmoothQuant-quantized INT8 models on a single MI300X accelerator. The GPU memory footprints of SmoothQuant-quantized models are significantly reduced. It also indicates the per-sample inference latency is significantly reduced for all SmoothQuant-quantized OPT models (illustrated in (b)). Notably, the performance of the CK instance-based INT8 kernel steadily improves with an increase in model size.</p>
<!-- 
================
 ### Figure 8
================ -->
<figure class="align-default" id="id12">
<img alt="../../_images/ck-comparisons.jpg" src="../../_images/ck-comparisons.jpg"/>
<figcaption>
<p><span class="caption-text">Performance comparisons between the original FP16 and the SmoothQuant-quantized INT8 models on a single MI300X accelerator.</span><a class="headerlink" href="#id12" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For accuracy comparisons between the original FP16 and INT8 models, the evaluation is done by using the first 1,000 samples from the LAMBADA dataset’s validation set. We employ the same Last Token Prediction Accuracy method introduced in <a class="reference external" href="https://github.com/mit-han-lab/smoothquant/blob/main/examples/smoothquant_opt_real_int8_demo.ipynb">SmoothQuant Real-INT8 Inference for PyTorch</a> as our evaluation metric. The comparison results are shown in Table 2.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id13">
<caption><span class="caption-text">The inference accuracy comparisons of SmoothQuant quantized models on Instinct MI300X.</span><a class="headerlink" href="#id13" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>Models</p></th>
<th class="head"><p>Hugging Face FP16 model accuracy</p></th>
<th class="head"><p>SmoothQuant quantized INT8 model accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>opt-1.3B</p></td>
<td><p>0.72</p></td>
<td><p>0.70</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>opt-2.7B</p></td>
<td><p>0.76</p></td>
<td><p>0.75</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>opt-6.7B</p></td>
<td><p>0.80</p></td>
<td><p>0.79</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>opt-13B</p></td>
<td><p>0.79</p></td>
<td><p>0.77</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>CK provides a rich set of template parameters for generating flexible accelerated computing kernels for difference application scenarios.</p>
<p>CK supports multiple instruction sets of AMD Instinct GPUs, operator fusion and different data precisions. Its composability helps users quickly construct operator performance verification.</p>
<p>With CK, you can build more effective AI applications with higher flexibility and better performance on different AMD accelerator platforms.</p>
</section>
</section>
</article>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="llm-inference-frameworks.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">LLM inference frameworks</p>
</div>
</a>
<a class="right-next" href="optimizing-triton-kernel.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Optimizing Triton kernels</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> Contents
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-overview-a-ck-gemm-instance">High-level overview: a CK GEMM instance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#template-parameter-definition">Template parameter definition</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-data-precision">Matrix data precision</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-data-layout">Matrix data layout</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-element-operation">Matrix element operation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tunable-parameters">Tunable parameters</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instantiating-and-running-the-templated-kernel">Instantiating and running the templated kernel</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-fused-int8-kernels-for-smoothquant-models">Developing fused INT8 kernels for SmoothQuant models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#operation-flow-analysis">Operation flow analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-the-complete-function">Developing the complete function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binding-to-python">Binding to Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#int8-model-inference-and-performance">INT8 model inference and performance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
<p>
</p>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<footer class="rocm-footer">
<div class="container-lg">
<section class="bottom-menu menu py-45">
<div class="row d-flex align-items-center">
<div class="col-12 text-center">
<ul>
<li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
<li><a href="https://rocm.docs.amd.com/en/develop/about/license.html">ROCm Licenses and Disclaimers</a></li>
<li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
<li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
<li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
<li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
<li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
<li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
<!-- OneTrust Cookies Settings button start -->
<li><a class="ot-sdk-show-settings" href="#cookie-settings" id="ot-sdk-btn">Cookie Settings</a></li>
<!-- OneTrust Cookies Settings button end -->
</ul>
</div>
</div>
<div class="row d-flex align-items-center">
<div class="col-12 text-center">
<div>
<span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
</div>
</div>
</div>
</section>
</div>
</footer>
<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
</body>
</html>