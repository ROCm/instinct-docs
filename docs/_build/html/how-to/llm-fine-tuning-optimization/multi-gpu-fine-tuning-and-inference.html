
<!DOCTYPE html>

<html data-content_root="../../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Model fine-tuning and inference on a multi-GPU system" name="description"/>
<meta content="ROCm, LLM, fine-tuning, usage, tutorial, multi-GPU, distributed, inference" name="keywords"/>
<title>Fine-tuning and inference using multiple accelerators — ROCm Documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../_static/pygments.css?v=a746c00c" rel="stylesheet" type="text/css"/>
<link href="../../_static/styles/sphinx-book-theme.css?v=a3416100" rel="stylesheet" type="text/css"/>
<link href="../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../_static/custom.css?v=da61d430" rel="stylesheet" type="text/css"/>
<link href="../../_static/rocm_header.css?v=4044f309" rel="stylesheet" type="text/css"/>
<link href="../../_static/rocm_footer.css?v=25204c5a" rel="stylesheet" type="text/css"/>
<link href="../../_static/fonts.css?v=fcff5274" rel="stylesheet" type="text/css"/>
<link href="../../_static/sphinx-design.min.css?v=87e54e7c" rel="stylesheet" type="text/css"/>
<link href="../../_static/rocm_custom.css?v=ace7df76" rel="stylesheet" type="text/css"/>
<link href="../../_static/rocm_rn.css?v=0e8af9ba" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/documentation_options.js?v=bc0531d1"></script>
<script src="../../_static/doctools.js?v=9a2dae69"></script>
<script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../_static/copybutton.js?v=f281be69"></script>
<script async="async" src="../../_static/code_word_breaks.js?v=327952c4"></script>
<script async="async" src="../../_static/renameVersionLinks.js?v=929fe5e4"></script>
<script async="async" src="../../_static/rdcMisc.js?v=01f88d96"></script>
<script async="async" src="../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
<script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
<script src="../../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'how-to/llm-fine-tuning-optimization/multi-gpu-fine-tuning-and-inference';</script>
<script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
<link href="rocm-stg.amd.com/how-to/llm-fine-tuning-optimization/multi-gpu-fine-tuning-and-inference.html" rel="canonical"/>
<link href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico" rel="icon"/>
<link href="../../genindex.html" rel="index" title="Index"/>
<link href="../../search.html" rel="search" title="Search"/>
<link href="model-quantization.html" rel="next" title="Model quantization techniques"/>
<link href="single-gpu-fine-tuning-and-inference.html" rel="prev" title="Fine-tuning and inference using a single accelerator"/>
<meta content="vo35SZt_GASsTHAEmdww7AYKPCvZyzLvOXBl8guBME4" name="google-site-verification"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<aside aria-label="Announcement" class="bd-header-announcement">
<div class="bd-header-announcement__content">This page contains proposed changes for a future release of ROCm. Read the <a href="https://rocm.docs.amd.com/en/latest/" id="rocm-banner">latest Linux release of ROCm documentation</a> for your production environments.</div>
</aside>
<header class="common-header">
<nav class="navbar navbar-expand-xl">
<div class="container-fluid main-nav rocm-header">
<button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler collapsed" data-bs-target="#navbarSupportedContent" data-bs-toggle="collapse" data-tracking-information="mainMenuToggle" id="nav-icon" type="button">
<span></span>
<span></span>
<span></span>
</button>
<div class="header-logo">
<a class="navbar-brand" href="https://www.amd.com/">
<img alt="AMD Logo" class="d-inline-block align-text-top hover-opacity" src="../../_static/images/amd-header-logo.svg" title="AMD Logo" width="90"/>
</a>
<div class="vr vr mx-40 my-25"></div>
<a class="klavika-font hover-opacity" href="https://rocm.docs.amd.com/en/develop">ROCm™ Software Future Release</a>
<a class="header-all-versions" href="https://rocm.docs.amd.com/en/latest/release/versions.html">Version List</a>
</div>
<div class="icon-nav text-center d-flex ms-auto">
</div>
</div>
</nav>
<nav class="navbar navbar-expand-xl second-level-nav">
<div class="container-fluid main-nav">
<div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
<ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm" id="navgithub" role="button" target="_blank">
                                GitHub
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm/discussions" id="navcommunity" role="button" target="_blank">
                                Community
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://rocm.blogs.amd.com/" id="navblogs" role="button" target="_blank">
                                Blogs
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://www.amd.com/en/developer/resources/infinity-hub.html" id="navinfinity-hub" role="button" target="_blank">
                                Infinity Hub
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm/issues/new/choose" id="navsupport" role="button" target="_blank">
                                Support
                            </a>
</li>
</ul>
</div>
</div>
</nav>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<a class="navbar-brand logo" href="../../index.html">
<p class="title logo__title">ROCm Documentation</p>
</a></div>
<div class="sidebar-primary-item">
<script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
<div class="sidebar-primary-item"><nav aria-label="Main" class="bd-links bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../what-is-rocm.html">What is ROCm?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/release-notes.html">Release notes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Install</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/develop/">ROCm on Linux</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-windows/en/develop/">HIP SDK on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep-learning-rocm.html">Deep learning frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build-rocm.html">Build ROCm from source</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">How to</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../rocm-for-ai/index.html">Using ROCm for AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../rocm-for-ai/install.html">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rocm-for-ai/train-a-model.html">Training a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rocm-for-ai/hugging-face-models.html">Running models from Hugging Face</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rocm-for-ai/deploy-your-model.html">Deploying your model</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../rocm-for-hpc/index.html">Using ROCm for HPC</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Fine-tuning LLMs and inference optimization</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="overview.html">Conceptual overview</a></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="fine-tuning-and-inference.html">Fine-tuning and inference</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="single-gpu-fine-tuning-and-inference.html">Using a single accelerator</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Using multiple accelerators</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="model-quantization.html">Model quantization techniques</a></li>
<li class="toctree-l2"><a class="reference internal" href="model-acceleration-libraries.html">Model acceleration libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="llm-inference-frameworks.html">LLM inference frameworks</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimizing-with-composable-kernel.html">Optimizing with Composable Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="optimizing-triton-kernel.html">Optimizing Triton kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="profiling-and-debugging.html">Profiling and debugging</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../system-optimization/index.html">System optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../system-optimization/mi300x.html">AMD Instinct MI300X</a></li>
<li class="toctree-l2"><a class="reference internal" href="../system-optimization/mi300a.html">AMD Instinct MI300A</a></li>
<li class="toctree-l2"><a class="reference internal" href="../system-optimization/mi200.html">AMD Instinct MI200</a></li>
<li class="toctree-l2"><a class="reference internal" href="../system-optimization/mi100.html">AMD Instinct MI100</a></li>
<li class="toctree-l2"><a class="reference internal" href="../system-optimization/w6000-v620.html">AMD RDNA 2</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tuning-guides/mi300x/index.html">AMD MI300X performance validation and tuning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../performance-validation/mi300x/vllm-benchmark.html">Performance validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tuning-guides/mi300x/system.html">System tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tuning-guides/mi300x/workload.html">Workload tuning</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/gpu-cluster-networking/en/develop/index.html">GPU cluster networking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-enabled-mpi.html">Using MPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../system-debugging.html">System debugging</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../conceptual/compiler-topics.html">Using advanced compiler features</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/index.html">ROCm compiler infrastructure</a></li>
<li class="toctree-l2"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/using-gpu-sanitizer.html">Using AddressSanitizer</a></li>
<li class="toctree-l2"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/openmp.html">OpenMP support</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../setting-cus.html">Setting the number of CUs</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/amd/rocm-examples">ROCm examples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Compatibility</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../compatibility/compatibility-matrix.html">Compatibility matrix</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/develop/reference/system-requirements.html">Linux system requirements</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-windows/en/develop/reference/system-requirements.html">Windows system requirements</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/develop/reference/3rd-party-support-matrix.html">Third-party support</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/user-kernel-space-compat-matrix.html">User and kernel-space support matrix</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/docker-image-support-matrix.html">Docker image support matrix</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/radeon/en/latest/index.html">Use ROCm on Radeon GPUs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Conceptual</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../conceptual/gpu-arch.html">GPU architecture overview</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../conceptual/gpu-arch/mi300.html">MI300 microarchitecture</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf">AMD Instinct MI300/CDNA3 ISA</a></li>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf">White paper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../conceptual/gpu-arch/mi300-mi200-performance-counters.html">MI300 and MI200 Performance counter</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../conceptual/gpu-arch/mi250.html">MI250 microarchitecture</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/instinct-mi200-cdna2-instruction-set-architecture.pdf">AMD Instinct MI200/CDNA2 ISA</a></li>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/documents/amd-cdna2-white-paper.pdf">White paper</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../conceptual/gpu-arch/mi100.html">MI100 microarchitecture</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/instinct-mi100-cdna1-shader-instruction-set-architecture%C2%A0.pdf">AMD Instinct MI100/CDNA1 ISA</a></li>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/documents/amd-cdna-whitepaper.pdf">White paper</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/gpu-memory.html">GPU memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/file-reorg.html">File structure (Linux FHS)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/gpu-isolation.html">GPU isolation techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/cmake-packages.html">Using CMake</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/More-about-how-ROCm-uses-PCIe-Atomics.html">ROCm &amp; PCIe atomics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/ai-pytorch-inception.html">Inception v3 with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conceptual/oversubscription.html">Oversubscription of hardware resources</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../reference/api-libraries.html">ROCm libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/rocm-tools.html">ROCm tools, compilers, and runtimes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/gpu-arch-specs.html">Accelerator and GPU hardware specifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/precision-support.html">Precision support</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contribute</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../contribute/contributing.html">Contributing to the ROCm docmentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../contribute/toolchain.html">ROCm documentation toolchain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../contribute/building.html">Building documentation</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/feedback.html">Providing feedback about the ROCm documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/license.html">ROCm licenses</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__primary" title="Toggle primary sidebar">
<span class="fa-solid fa-angle-right"></span>
</label></div>
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="index.html">Fine-tuning LLMs and inference optimization</a></li>
<li class="breadcrumb-item"><a class="nav-link" href="fine-tuning-and-inference.html">Fine-tuning and inference</a></li>
<li aria-current="page" class="breadcrumb-item active">Fine-tuning...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="article-header-buttons">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>
<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</button>
</div></div>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>Fine-tuning and inference using multiple accelerators</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-base-implementation-environment">Setting up the base implementation environment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-accelerate-for-fine-tuning-and-inference">Hugging Face Accelerate for fine-tuning and inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchtune-for-fine-tuning-and-inference">torchtune for fine-tuning and inference</a></li>
</ul>
</nav>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="fine-tuning-and-inference-using-multiple-accelerators">
<h1>Fine-tuning and inference using multiple accelerators<a class="headerlink" href="#fine-tuning-and-inference-using-multiple-accelerators" title="Link to this heading">#</a></h1><div class="sd-container-fluid sd-sphinx-override sd-p-0 sd-mt-2 sd-mb-4 sd-p-2 sd-rounded-1 docutils" id="rocm-docs-core-article-info">
<div class="sd-row sd-row-cols-2 sd-gx-2 sd-gy-1 docutils">
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils" style="color:gray;">
    Applies to Linux
</div>
<div class="sd-col sd-d-flex-row sd-align-minor-center docutils">
<div class="sd-container-fluid sd-sphinx-override docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-3 sd-row-cols-md-3 sd-row-cols-lg-3 sd-gx-3 sd-gy-1 docutils">
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;"></p>
</div>
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;"><span class="sd-pr-2"><svg aria-hidden="true" class="sd-octicon sd-octicon-calendar" height="16.0px" version="1.1" viewbox="0 0 16 16" width="16.0px"><path d="M4.75 0a.75.75 0 01.75.75V2h5V.75a.75.75 0 011.5 0V2h1.25c.966 0 1.75.784 1.75 1.75v10.5A1.75 1.75 0 0113.25 16H2.75A1.75 1.75 0 011 14.25V3.75C1 2.784 1.784 2 2.75 2H4V.75A.75.75 0 014.75 0zm0 3.5h8.5a.25.25 0 01.25.25V6h-11V3.75a.25.25 0 01.25-.25h2zm-2.25 4v6.75c0 .138.112.25.25.25h10.5a.25.25 0 00.25-.25V7.5h-11z" fill-rule="evenodd"></path></svg></span>2024-08-08</p>
</div>
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;"><span class="sd-pr-2"><svg aria-hidden="true" class="sd-octicon sd-octicon-clock" height="16.0px" version="1.1" viewbox="0 0 16 16" width="16.0px"><path d="M1.5 8a6.5 6.5 0 1113 0 6.5 6.5 0 01-13 0zM8 0a8 8 0 100 16A8 8 0 008 0zm.5 4.75a.75.75 0 00-1.5 0v3.5a.75.75 0 00.471.696l2.5 1a.75.75 0 00.557-1.392L8.5 7.742V4.75z" fill-rule="evenodd"></path></svg></span>10 min read time</p>
</div>
</div>
</div>
</div>
</div>
</div>

<p>This section explains how to fine-tune a model on a multi-accelerator system. See
<a class="reference internal" href="single-gpu-fine-tuning-and-inference.html"><span class="doc">Single-accelerator fine-tuning</span></a> for a single accelerator or GPU setup.</p>
<section id="environment-setup">
<span id="fine-tuning-llms-multi-gpu-env"></span><h2>Environment setup<a class="headerlink" href="#environment-setup" title="Link to this heading">#</a></h2>
<p>This section was tested using the following hardware and software environment.</p>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><th class="stub"><p>Hardware</p></th>
<td><p>4 AMD Instinct MI300X accelerators</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Software</p></th>
<td><p>ROCm 6.1, Ubuntu 22.04, PyTorch 2.1.2, Python 3.10</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Libraries</p></th>
<td><p><code class="docutils literal notranslate"><span class="pre">transformers</span></code> <code class="docutils literal notranslate"><span class="pre">datasets</span></code> <code class="docutils literal notranslate"><span class="pre">accelerate</span></code> <code class="docutils literal notranslate"><span class="pre">huggingface-hub</span></code> <code class="docutils literal notranslate"><span class="pre">peft</span></code> <code class="docutils literal notranslate"><span class="pre">trl</span></code> <code class="docutils literal notranslate"><span class="pre">scipy</span></code></p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Base model</p></th>
<td><p><code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-2-7b-chat-hf</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<section id="setting-up-the-base-implementation-environment">
<span id="fine-tuning-llms-multi-gpu-env-setup"></span><h3>Setting up the base implementation environment<a class="headerlink" href="#setting-up-the-base-implementation-environment" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p>Install PyTorch for ROCm. Refer to the
<a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/develop/install/3rd-party/pytorch-install.html" title="(in ROCm installation on Linux v6.2.4)"><span class="xref std std-doc">PyTorch installation guide</span></a>. For consistent
installation, it’s recommended to use official ROCm prebuilt Docker images with the framework pre-installed.</p></li>
<li><p>In the Docker container, check the availability of ROCM-capable accelerators using the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>rocm-smi<span class="w"> </span>--showproductname
</pre></div>
</div>
</li>
<li><p>Check that your accelerators are available to PyTorch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Is a ROCm-GPU detected? "</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"How many ROCm-GPUs are detected? "</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">())</span>
</pre></div>
</div>
<p>If successful, your output should look like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>print<span class="o">(</span><span class="s2">"Is a ROCm-GPU detected? "</span>,<span class="w"> </span>torch.cuda.is_available<span class="o">())</span>
Is<span class="w"> </span>a<span class="w"> </span>ROCm-GPU<span class="w"> </span>detected?<span class="w">  </span>True
&gt;&gt;&gt;<span class="w"> </span>print<span class="o">(</span><span class="s2">"How many ROCm-GPUs are detected? "</span>,<span class="w"> </span>torch.cuda.device_count<span class="o">())</span>
How<span class="w"> </span>many<span class="w"> </span>ROCm-GPUs<span class="w"> </span>are<span class="w"> </span>detected?<span class="w">  </span><span class="m">4</span>
</pre></div>
</div>
</li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>During training and inference, you can check the memory usage by running the <code class="docutils literal notranslate"><span class="pre">rocm-smi</span></code> command in your terminal.
This tool helps you see shows which accelerators or GPUs are involved.</p>
</div>
</section>
</section>
<section id="hugging-face-accelerate-for-fine-tuning-and-inference">
<span id="fine-tuning-llms-multi-gpu-hugging-face-accelerate"></span><h2>Hugging Face Accelerate for fine-tuning and inference<a class="headerlink" href="#hugging-face-accelerate-for-fine-tuning-and-inference" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://huggingface.co/docs/accelerate/en/index">Hugging Face Accelerate</a> is a library that simplifies turning raw
PyTorch code for a single accelerator into code for multiple accelerators for LLM fine-tuning and inference. It is
integrated with <a class="reference external" href="https://huggingface.co/docs/transformers/en/index">Transformers</a> allowing you to scale your PyTorch
code while maintaining performance and flexibility.</p>
<p>As a brief example of model fine-tuning and inference using multiple GPUs, let’s use Transformers and load in the Llama
2 7B model.</p>
<p>Here, let’s reuse the code in <a class="reference internal" href="single-gpu-fine-tuning-and-inference.html#fine-tuning-llms-single-gpu-download-model-dataset"><span class="std std-ref">Single-accelerator fine-tuning</span></a>
to load the base model and tokenizer.</p>
<p>Now, it’s important to adjust how you load the model. Add the <code class="docutils literal notranslate"><span class="pre">device_map</span></code> parameter to your base model configuration.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">"meta-llama/Llama-2-7b-chat-hf"</span>

<span class="c1"># Load base model to GPU memory</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">base_model_name</span><span class="p">,</span>
        <span class="n">device_map</span> <span class="o">=</span> <span class="s2">"auto"</span><span class="p">,</span>
        <span class="n">trust_remote_code</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="o">...</span>
<span class="c1"># Run training</span>
<span class="n">sft_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can let Accelerate handle the device map computation by setting <code class="docutils literal notranslate"><span class="pre">device_map</span></code> to one of the supported options
(<code class="docutils literal notranslate"><span class="pre">"auto"</span></code>, <code class="docutils literal notranslate"><span class="pre">"balanced"</span></code>, <code class="docutils literal notranslate"><span class="pre">"balanced_low_0"</span></code>, <code class="docutils literal notranslate"><span class="pre">"sequential"</span></code>).</p>
<p>It’s recommended to set the <code class="docutils literal notranslate"><span class="pre">device_map</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">“auto”</span></code> to allow Accelerate to automatically and
efficiently allocate the model given the available resources (4 accelerators in this case).</p>
<p>When you have more GPU memory available than the model size, here is the difference between each <code class="docutils literal notranslate"><span class="pre">device_map</span></code>
option:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">"auto"</span></code> and <code class="docutils literal notranslate"><span class="pre">"balanced"</span></code> evenly split the model on all available GPUs, making it possible for you to use a
batch size greater than 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"balanced_low_0"</span></code> evenly splits the model on all GPUs except the first
one, and only puts on GPU 0 what does not fit on the others. This
option is great when you need to use GPU 0 for some processing of the
outputs, like when using the generate function for Transformers
models.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"sequential"</span></code> will fit what it can on GPU 0, then move on GPU 1 and so forth. Not all GPUs might be used.</p></li>
</ul>
</div>
<p>After loading the model in this way, the model is fully ready to use the resources available to it.</p>
</section>
<section id="torchtune-for-fine-tuning-and-inference">
<span id="fine-tuning-llms-multi-gpu-torchtune"></span><h2>torchtune for fine-tuning and inference<a class="headerlink" href="#torchtune-for-fine-tuning-and-inference" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://pytorch.org/torchtune/main/">torchtune</a> is a PyTorch-native library for easy single and multi-accelerator or
GPU model fine-tuning and inference with LLMs.</p>
<ol class="arabic">
<li><p>Install torchtune using pip.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install torchtune with PyTorch release 2.2.2+</span>
pip<span class="w"> </span>install<span class="w"> </span>torchtune

<span class="c1"># To confirm that the package is installed correctly</span>
tune<span class="w"> </span>--help
</pre></div>
</div>
<p>The output should look like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>usage:<span class="w"> </span>tune<span class="w"> </span><span class="o">[</span>-h<span class="o">]</span><span class="w"> </span><span class="o">{</span>download,ls,cp,run,validate<span class="o">}</span><span class="w"> </span>...

Welcome<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>TorchTune<span class="w"> </span>CLI!

options:
<span class="w">  </span>-h,<span class="w"> </span>--help<span class="w">            </span>show<span class="w"> </span>this<span class="w"> </span><span class="nb">help</span><span class="w"> </span>message<span class="w"> </span>and<span class="w"> </span><span class="nb">exit</span>

subcommands:
<span class="w">  </span><span class="o">{</span>download,ls,cp,run,validate<span class="o">}</span>
</pre></div>
</div>
</li>
<li><p>torchtune recipes are designed around easily composable components and workable training loops, with minimal abstraction
getting in the way of fine-tuning. Run <code class="docutils literal notranslate"><span class="pre">tune</span> <span class="pre">ls</span></code> to show built-in torchtune configuration recipes.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>RECIPE<span class="w">                                   </span>CONFIG
full_finetune_single_device<span class="w">              </span>llama2/7B_full_low_memory
<span class="w">                                         </span>llama3/8B_full_single_device
<span class="w">                                         </span>mistral/7B_full_low_memory
full_finetune_distributed<span class="w">                </span>llama2/7B_full
<span class="w">                                         </span>llama2/13B_full
<span class="w">                                         </span>llama3/8B_full
<span class="w">                                         </span>mistral/7B_full
<span class="w">                                         </span>gemma/2B_full
lora_finetune_single_device<span class="w">              </span>llama2/7B_lora_single_device
<span class="w">                                         </span>llama2/7B_qlora_single_device
<span class="w">                                         </span>llama3/8B_lora_single_device
<span class="w">                                         </span>llama3/8B_qlora_single_device
<span class="w">                                         </span>llama2/13B_qlora_single_device
<span class="w">                                         </span>mistral/7B_lora_single_device
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">RECIPE</span></code> column shows the easy-to-use and workable fine-tuning and inference recipes for popular fine-tuning
techniques (such as LoRA). The <code class="docutils literal notranslate"><span class="pre">CONFIG</span></code> column lists the YAML configurations for easily configuring training,
evaluation, quantization, or inference recipes.</p>
<p>The snippet shows the architecture of a model’s YAML configuration file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model arguments</span>
<span class="nt">model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.models.llama2.lora_llama2_7b</span>
<span class="w">  </span><span class="nt">lora_attn_modules</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'q_proj'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'v_proj'</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">apply_lora_to_mlp</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">apply_lora_to_output</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">lora_rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">  </span><span class="nt">lora_alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>

<span class="nt">tokenizer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.models.llama2.llama2_tokenizer</span>
<span class="w">  </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/tmp/Llama-2-7b-hf/tokenizer.model</span>

<span class="c1"># Dataset and sampler</span>
<span class="nt">dataset</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.datasets.alpaca_cleaned_dataset</span>
<span class="w">  </span><span class="nt">train_on_input</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
</pre></div>
</div>
</li>
<li><p>This configuration file defines the fine-tuning base model path, data set, hyper-parameters for optimizer and scheduler,
and training data type. To download the base model for fine-tuning, run the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>download<span class="w"> </span>meta-llama/Llama-2-7b-hf<span class="w"> </span>--output-dir<span class="w"> </span>/tmp/Llama-2-7b-hf<span class="w"> </span>--hf-token
</pre></div>
</div>
<p>The output directory argument for <code class="docutils literal notranslate"><span class="pre">--output-dir</span></code> should map the model path specified in YAML config file.</p>
</li>
<li><p>To launch <code class="docutils literal notranslate"><span class="pre">lora_finetune_distributed</span></code> on four devices, run the following
command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>run<span class="w"> </span>--nnodes<span class="w"> </span><span class="m">1</span><span class="w"> </span>--nproc_per_node<span class="w"> </span><span class="m">4</span><span class="w"> </span>lora_finetune_distributed<span class="w"> </span>--config<span class="w"> </span>llama2/7B_lora
</pre></div>
</div>
<p>If successful, you should something like the following output:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>INFO:torchtune.utils.logging:FSDP<span class="w"> </span>is<span class="w"> </span>enabled.<span class="w"> </span>Instantiating<span class="w"> </span>Model<span class="w"> </span>on<span class="w"> </span>CPU<span class="w"> </span><span class="k">for</span><span class="w"> </span>Rank<span class="w"> </span><span class="m">0</span><span class="w"> </span>...
INFO:torchtune.utils.logging:Model<span class="w"> </span>instantiation<span class="w"> </span>took<span class="w"> </span><span class="m">7</span>.32<span class="w"> </span>secs
INFO:torchtune.utils.logging:Memory<span class="w"> </span>Stats<span class="w"> </span>after<span class="w"> </span>model<span class="w"> </span>init:
<span class="o">{</span><span class="s1">'peak_memory_active'</span>:<span class="w"> </span><span class="m">9</span>.478172672,<span class="w"> </span><span class="s1">'peak_memory_alloc'</span>:<span class="w"> </span><span class="m">8</span>.953868288,<span class="w"> </span><span class="s1">'peak_memory_reserved'</span>:<span class="w"> </span><span class="m">11</span>.112808448<span class="o">}</span>
INFO:torchtune.utils.logging:Optimizer<span class="w"> </span>and<span class="w"> </span>loss<span class="w"> </span>are<span class="w"> </span>initialized.
INFO:torchtune.utils.logging:Dataset<span class="w"> </span>and<span class="w"> </span>Sampler<span class="w"> </span>are<span class="w"> </span>initialized.
INFO:torchtune.utils.logging:Learning<span class="w"> </span>rate<span class="w"> </span>scheduler<span class="w"> </span>is<span class="w"> </span>initialized.
<span class="m">1</span><span class="p">|</span><span class="m">111</span><span class="p">|</span>Loss:<span class="w"> </span><span class="m">1</span>.5790324211120605:<span class="w">   </span><span class="m">7</span>%<span class="p">|</span>█<span class="w">                                          </span><span class="p">|</span><span class="w"> </span><span class="m">114</span>/1618
</pre></div>
</div>
</li>
</ol>
<p>Read more about inference frameworks in <a class="reference internal" href="llm-inference-frameworks.html"><span class="doc">LLM inference frameworks</span></a>.</p>
</section>
</section>
</article>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="single-gpu-fine-tuning-and-inference.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Fine-tuning and inference using a single accelerator</p>
</div>
</a>
<a class="right-next" href="model-quantization.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Model quantization techniques</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> Contents
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment-setup">Environment setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-base-implementation-environment">Setting up the base implementation environment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-accelerate-for-fine-tuning-and-inference">Hugging Face Accelerate for fine-tuning and inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchtune-for-fine-tuning-and-inference">torchtune for fine-tuning and inference</a></li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
<p>
</p>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<footer class="rocm-footer">
<div class="container-lg">
<section class="bottom-menu menu py-45">
<div class="row d-flex align-items-center">
<div class="col-12 text-center">
<ul>
<li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
<li><a href="https://rocm.docs.amd.com/en/develop/about/license.html">ROCm Licenses and Disclaimers</a></li>
<li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
<li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
<li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
<li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
<li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
<li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
<!-- OneTrust Cookies Settings button start -->
<li><a class="ot-sdk-show-settings" href="#cookie-settings" id="ot-sdk-btn">Cookie Settings</a></li>
<!-- OneTrust Cookies Settings button end -->
</ul>
</div>
</div>
<div class="row d-flex align-items-center">
<div class="col-12 text-center">
<div>
<span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
</div>
</div>
</div>
</section>
</div>
</footer>
<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
</body>
</html>