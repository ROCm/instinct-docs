
<!DOCTYPE html>

<html data-content_root="../../../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="AMD Instinct MI300X workload tuning" name="description"/>
<meta content="AMD, Instinct, MI300X, HPC, tuning, BIOS settings, NBIO, ROCm, environment variable, performance, HIP, Triton, PyTorch TunableOp, vLLM, RCCL, MIOpen, accelerator, GPU, resource utilization" name="keywords"/>
<title>AMD Instinct MI300X workload optimization — ROCm Documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css?v=a746c00c" rel="stylesheet" type="text/css"/>
<link href="../../../_static/styles/sphinx-book-theme.css?v=a3416100" rel="stylesheet" type="text/css"/>
<link href="../../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../../_static/custom.css?v=da61d430" rel="stylesheet" type="text/css"/>
<link href="../../../_static/rocm_header.css?v=4044f309" rel="stylesheet" type="text/css"/>
<link href="../../../_static/rocm_footer.css?v=25204c5a" rel="stylesheet" type="text/css"/>
<link href="../../../_static/fonts.css?v=fcff5274" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-design.min.css?v=87e54e7c" rel="stylesheet" type="text/css"/>
<link href="../../../_static/rocm_custom.css?v=ace7df76" rel="stylesheet" type="text/css"/>
<link href="../../../_static/rocm_rn.css?v=0e8af9ba" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/documentation_options.js?v=bc0531d1"></script>
<script src="../../../_static/doctools.js?v=9a2dae69"></script>
<script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../../_static/copybutton.js?v=f281be69"></script>
<script async="async" src="../../../_static/code_word_breaks.js?v=327952c4"></script>
<script async="async" src="../../../_static/renameVersionLinks.js?v=929fe5e4"></script>
<script async="async" src="../../../_static/rdcMisc.js?v=01f88d96"></script>
<script async="async" src="../../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
<script src="../../../_static/design-tabs.js?v=f930bc37"></script>
<script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'how-to/tuning-guides/mi300x/workload';</script>
<script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
<link href="rocm-stg.amd.com/how-to/tuning-guides/mi300x/workload.html" rel="canonical"/>
<link href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico" rel="icon"/>
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="../../gpu-enabled-mpi.html" rel="next" title="GPU-enabled Message Passing Interface"/>
<link href="system.html" rel="prev" title="AMD Instinct MI300X system optimization"/>
<meta content="vo35SZt_GASsTHAEmdww7AYKPCvZyzLvOXBl8guBME4" name="google-site-verification"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<aside aria-label="Announcement" class="bd-header-announcement">
<div class="bd-header-announcement__content">This page contains proposed changes for a future release of ROCm. Read the <a href="https://rocm.docs.amd.com/en/latest/" id="rocm-banner">latest Linux release of ROCm documentation</a> for your production environments.</div>
</aside>
<header class="common-header">
<nav class="navbar navbar-expand-xl">
<div class="container-fluid main-nav rocm-header">
<button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler collapsed" data-bs-target="#navbarSupportedContent" data-bs-toggle="collapse" data-tracking-information="mainMenuToggle" id="nav-icon" type="button">
<span></span>
<span></span>
<span></span>
</button>
<div class="header-logo">
<a class="navbar-brand" href="https://www.amd.com/">
<img alt="AMD Logo" class="d-inline-block align-text-top hover-opacity" src="../../../_static/images/amd-header-logo.svg" title="AMD Logo" width="90"/>
</a>
<div class="vr vr mx-40 my-25"></div>
<a class="klavika-font hover-opacity" href="https://rocm.docs.amd.com/en/develop">ROCm™ Software Future Release</a>
<a class="header-all-versions" href="https://rocm.docs.amd.com/en/latest/release/versions.html">Version List</a>
</div>
<div class="icon-nav text-center d-flex ms-auto">
</div>
</div>
</nav>
<nav class="navbar navbar-expand-xl second-level-nav">
<div class="container-fluid main-nav">
<div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
<ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm" id="navgithub" role="button" target="_blank">
                                GitHub
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm/discussions" id="navcommunity" role="button" target="_blank">
                                Community
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://rocm.blogs.amd.com/" id="navblogs" role="button" target="_blank">
                                Blogs
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://www.amd.com/en/developer/resources/infinity-hub.html" id="navinfinity-hub" role="button" target="_blank">
                                Infinity Hub
                            </a>
</li>
<li class="nav-item">
<a aria-expanded="false" class="nav-link top-level header-menu-links" href="https://github.com/ROCm/ROCm/issues/new/choose" id="navsupport" role="button" target="_blank">
                                Support
                            </a>
</li>
</ul>
</div>
</div>
</nav>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<a class="navbar-brand logo" href="../../../index.html">
<p class="title logo__title">ROCm Documentation</p>
</a></div>
<div class="sidebar-primary-item">
<script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
<div class="sidebar-primary-item"><nav aria-label="Main" class="bd-links bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../what-is-rocm.html">What is ROCm?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/release-notes.html">Release notes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Install</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/develop/">ROCm on Linux</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-windows/en/develop/">HIP SDK on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep-learning-rocm.html">Deep learning frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../build-rocm.html">Build ROCm from source</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">How to</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../rocm-for-ai/index.html">Using ROCm for AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../rocm-for-ai/install.html">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rocm-for-ai/train-a-model.html">Training a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rocm-for-ai/hugging-face-models.html">Running models from Hugging Face</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rocm-for-ai/deploy-your-model.html">Deploying your model</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../rocm-for-hpc/index.html">Using ROCm for HPC</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../llm-fine-tuning-optimization/index.html">Fine-tuning LLMs and inference optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../llm-fine-tuning-optimization/overview.html">Conceptual overview</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../llm-fine-tuning-optimization/fine-tuning-and-inference.html">Fine-tuning and inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../llm-fine-tuning-optimization/single-gpu-fine-tuning-and-inference.html">Using a single accelerator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../llm-fine-tuning-optimization/multi-gpu-fine-tuning-and-inference.html">Using multiple accelerators</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../llm-fine-tuning-optimization/model-quantization.html">Model quantization techniques</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../llm-fine-tuning-optimization/model-acceleration-libraries.html">Model acceleration libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../llm-fine-tuning-optimization/llm-inference-frameworks.html">LLM inference frameworks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../llm-fine-tuning-optimization/optimizing-with-composable-kernel.html">Optimizing with Composable Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../llm-fine-tuning-optimization/optimizing-triton-kernel.html">Optimizing Triton kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../llm-fine-tuning-optimization/profiling-and-debugging.html">Profiling and debugging</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../system-optimization/index.html">System optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../system-optimization/mi300x.html">AMD Instinct MI300X</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../system-optimization/mi300a.html">AMD Instinct MI300A</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../system-optimization/mi200.html">AMD Instinct MI200</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../system-optimization/mi100.html">AMD Instinct MI100</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../system-optimization/w6000-v620.html">AMD RDNA 2</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">AMD MI300X performance validation and tuning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../performance-validation/mi300x/vllm-benchmark.html">Performance validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="system.html">System tuning</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Workload tuning</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/gpu-cluster-networking/en/develop/index.html">GPU cluster networking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-enabled-mpi.html">Using MPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../system-debugging.html">System debugging</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../conceptual/compiler-topics.html">Using advanced compiler features</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/index.html">ROCm compiler infrastructure</a></li>
<li class="toctree-l2"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/using-gpu-sanitizer.html">Using AddressSanitizer</a></li>
<li class="toctree-l2"><a class="reference external" href="https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/openmp.html">OpenMP support</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../setting-cus.html">Setting the number of CUs</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/amd/rocm-examples">ROCm examples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Compatibility</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../compatibility/compatibility-matrix.html">Compatibility matrix</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/develop/reference/system-requirements.html">Linux system requirements</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-windows/en/develop/reference/system-requirements.html">Windows system requirements</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/develop/reference/3rd-party-support-matrix.html">Third-party support</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/user-kernel-space-compat-matrix.html">User and kernel-space support matrix</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/docker-image-support-matrix.html">Docker image support matrix</a></li>
<li class="toctree-l1"><a class="reference external" href="https://rocm.docs.amd.com/projects/radeon/en/latest/index.html">Use ROCm on Radeon GPUs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Conceptual</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../conceptual/gpu-arch.html">GPU architecture overview</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../conceptual/gpu-arch/mi300.html">MI300 microarchitecture</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf">AMD Instinct MI300/CDNA3 ISA</a></li>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf">White paper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../conceptual/gpu-arch/mi300-mi200-performance-counters.html">MI300 and MI200 Performance counter</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../conceptual/gpu-arch/mi250.html">MI250 microarchitecture</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/instinct-mi200-cdna2-instruction-set-architecture.pdf">AMD Instinct MI200/CDNA2 ISA</a></li>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/documents/amd-cdna2-white-paper.pdf">White paper</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../conceptual/gpu-arch/mi100.html">MI100 microarchitecture</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/TechDocs/instinct-mi100-cdna1-shader-instruction-set-architecture%C2%A0.pdf">AMD Instinct MI100/CDNA1 ISA</a></li>
<li class="toctree-l3"><a class="reference external" href="https://www.amd.com/system/files/documents/amd-cdna-whitepaper.pdf">White paper</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../conceptual/gpu-memory.html">GPU memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../conceptual/file-reorg.html">File structure (Linux FHS)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../conceptual/gpu-isolation.html">GPU isolation techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../conceptual/cmake-packages.html">Using CMake</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../conceptual/More-about-how-ROCm-uses-PCIe-Atomics.html">ROCm &amp; PCIe atomics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../conceptual/ai-pytorch-inception.html">Inception v3 with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../conceptual/oversubscription.html">Oversubscription of hardware resources</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../reference/api-libraries.html">ROCm libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/rocm-tools.html">ROCm tools, compilers, and runtimes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/gpu-arch-specs.html">Accelerator and GPU hardware specifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/precision-support.html">Precision support</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contribute</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../contribute/contributing.html">Contributing to the ROCm docmentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../contribute/toolchain.html">ROCm documentation toolchain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contribute/building.html">Building documentation</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contribute/feedback.html">Providing feedback about the ROCm documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about/license.html">ROCm licenses</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__primary" title="Toggle primary sidebar">
<span class="fa-solid fa-angle-right"></span>
</label></div>
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../../../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="index.html">AMD MI300X tuning guides</a></li>
<li aria-current="page" class="breadcrumb-item active">AMD...</li>
</ul>
</nav>
</div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="article-header-buttons">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>
<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</button>
</div></div>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>AMD Instinct MI300X workload optimization</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workload-tuning-strategy">Workload tuning strategy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measure-the-current-workload">Measure the current workload</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mi300x-profiling-start">Identify tuning requirements</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-profiling-tools">High-level profiling tools</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-level-profiling-tools">Kernel-level profiling tools</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analyze-and-tune">Analyze and tune</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-model-inference-with-vllm">Optimize model inference with vLLM</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-tunable-configurations">Auto-tunable configurations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-tuning">Manual tuning</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterate-and-validate">Iterate and validate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#profiling-tools">Profiling tools</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-profiling-tools">ROCm profiling tools</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rocprofiler">ROCProfiler</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#omniperf">Omniperf</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#omnitrace">Omnitrace</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-performance-optimization">vLLM performance optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximize-throughput">Maximize throughput</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-vllm-on-multiple-gpus">Run vLLM on multiple GPUs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choose-different-attention-backends">Choose different attention backends</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-fp8-kv-cache-data-type">Use fp8 KV-cache data type</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enable-chunked-prefill">Enable chunked prefill</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-tensor-parallelism-and-gemm-performance">Optimize tensor parallelism and GEMM performance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gemm-tuning-steps">GEMM tuning steps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-tunableop">PyTorch TunableOp</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-inductor-triton-tuning-knobs">PyTorch inductor Triton tuning knobs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-library-tuning">ROCm library tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gemm-general-matrix-multiplication">GEMM (general matrix multiplication)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hipblaslt-benchmarking">hipBLASLt benchmarking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hipblaslt-backend-assembly-generator-tuning">hipBLASLt backend assembly generator tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#tune-hipblaslt-s-backend-assembly-generator">Tune hipBLASLt’s backend assembly generator</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#update-logic-yaml-files">Update logic YAML files</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tensile-optimization-and-performance-tuning">Tensile optimization and performance tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-composable-kernel-gemm-kernels">Optimizing Composable Kernel GEMM kernels</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#miopen">MIOpen</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution">Convolution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-in-miopen">Tuning in MIOpen</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-fastest-kernel">Finding the fastest kernel</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl">RCCL</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#use-all-eight-gpus">Use all eight GPUs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#disable-numa-auto-balancing">Disable NUMA auto-balancing</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#disable-acs-for-multi-node-rccl">Disable ACS for multi-node RCCL</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#run-rccl-unittests">Run RCCL-Unittests</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#npkit-profiler">NPKit profiler</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl-tests">RCCL-tests</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#use-one-process-per-gpu-mode">Use one-process-per-GPU mode</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl-in-e2e-workloads">RCCL in E2E workloads</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#triton-kernel-performance-optimization">Triton kernel performance optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-tunable-kernel-configurations-and-environment-variables">Auto-tunable kernel configurations and environment variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-gpu-resource-utilization">Overall GPU resource utilization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlir-analysis">MLIR analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#isa-assembly-analysis">ISA assembly analysis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-performance-optimization">HIP performance optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallel-execution-and-gpu-hardware-utilization">Parallel execution and GPU hardware utilization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-usage-optimization">Memory usage optimization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostic-and-performance-analysis">Diagnostic and performance analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-memory-access-faults">Debug memory access faults</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-the-occupancy-of-a-kernel">Compute the occupancy of a kernel</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#special-considerations">Special considerations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gpu-communications">Multi-GPU communications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-fsdp-and-rccl-settings">Multi-node FSDP and RCCL settings</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="amd-instinct-mi300x-workload-optimization">
<h1>AMD Instinct MI300X workload optimization<a class="headerlink" href="#amd-instinct-mi300x-workload-optimization" title="Link to this heading">#</a></h1><div class="sd-container-fluid sd-sphinx-override sd-p-0 sd-mt-2 sd-mb-4 sd-p-2 sd-rounded-1 docutils" id="rocm-docs-core-article-info">
<div class="sd-row sd-row-cols-2 sd-gx-2 sd-gy-1 docutils">
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils" style="color:gray;">
    Applies to Linux
</div>
<div class="sd-col sd-d-flex-row sd-align-minor-center docutils">
<div class="sd-container-fluid sd-sphinx-override docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-3 sd-row-cols-md-3 sd-row-cols-lg-3 sd-gx-3 sd-gy-1 docutils">
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;"></p>
</div>
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;"><span class="sd-pr-2"><svg aria-hidden="true" class="sd-octicon sd-octicon-calendar" height="16.0px" version="1.1" viewbox="0 0 16 16" width="16.0px"><path d="M4.75 0a.75.75 0 01.75.75V2h5V.75a.75.75 0 011.5 0V2h1.25c.966 0 1.75.784 1.75 1.75v10.5A1.75 1.75 0 0113.25 16H2.75A1.75 1.75 0 011 14.25V3.75C1 2.784 1.784 2 2.75 2H4V.75A.75.75 0 014.75 0zm0 3.5h8.5a.25.25 0 01.25.25V6h-11V3.75a.25.25 0 01.25-.25h2zm-2.25 4v6.75c0 .138.112.25.25.25h10.5a.25.25 0 00.25-.25V7.5h-11z" fill-rule="evenodd"></path></svg></span>2024-10-30</p>
</div>
<div class="sd-col sd-col-auto sd-d-flex-row sd-align-minor-center docutils">
<p class="sd-p-0 sd-m-0" style="color:gray;"><span class="sd-pr-2"><svg aria-hidden="true" class="sd-octicon sd-octicon-clock" height="16.0px" version="1.1" viewbox="0 0 16 16" width="16.0px"><path d="M1.5 8a6.5 6.5 0 1113 0 6.5 6.5 0 01-13 0zM8 0a8 8 0 100 16A8 8 0 008 0zm.5 4.75a.75.75 0 00-1.5 0v3.5a.75.75 0 00.471.696l2.5 1a.75.75 0 00.557-1.392L8.5 7.742V4.75z" fill-rule="evenodd"></path></svg></span>62 min read time</p>
</div>
</div>
</div>
</div>
</div>
</div>

<p>This document provides guidelines for optimizing the performance of AMD
Instinct™ MI300X accelerators, with a particular focus on GPU kernel
programming, high-performance computing (HPC), and deep learning operations
using PyTorch. It delves into specific workloads such as
<a class="reference internal" href="#mi300x-vllm-optimization"><span class="std std-ref">model inference</span></a>, offering strategies to
enhance efficiency.</p>
<p>The following topics highlight <a class="reference internal" href="#mi300x-auto-tune"><span class="std std-ref">auto-tunable configurations</span></a>
that streamline optimization as well as advanced techniques like
<a class="reference internal" href="#mi300x-triton-kernel-performance-optimization"><span class="std std-ref">Triton kernel optimization</span></a> for
meticulous tuning.</p>
<section id="workload-tuning-strategy">
<h2>Workload tuning strategy<a class="headerlink" href="#workload-tuning-strategy" title="Link to this heading">#</a></h2>
<p>By following a structured approach, you can systematically address
performance issues and enhance the efficiency of your workloads on AMD Instinct
MI300X accelerators.</p>
<section id="measure-the-current-workload">
<h3>Measure the current workload<a class="headerlink" href="#measure-the-current-workload" title="Link to this heading">#</a></h3>
<p>Begin by evaluating the performance of your workload in its current state. This
involves running benchmarks and collecting performance data to establish a
baseline. Understanding how your workload behaves under different conditions
provides critical insights into where improvements are needed.</p>
</section>
<section id="mi300x-profiling-start">
<span id="identify-tuning-requirements"></span><h3>Identify tuning requirements<a class="headerlink" href="#mi300x-profiling-start" title="Link to this heading">#</a></h3>
<p>Analyze the collected performance data to identify areas where tuning is
required. This could involve detecting bottlenecks in CPU, GPU, memory, or data
transfer. Understanding these requirements will help direct your optimization
efforts more effectively.</p>
<p>Profiling is a fundamental step in workload tuning. It allows you to gather
detailed information about how your workload utilizes system resources, and
where potential inefficiencies lie. Profiling tools can provide insights into
both high-level and granular performance metrics. See <a class="reference internal" href="#mi300x-profiling-tools"><span class="std std-ref">Profiling tools</span></a>.</p>
<section id="high-level-profiling-tools">
<h4>High-level profiling tools<a class="headerlink" href="#high-level-profiling-tools" title="Link to this heading">#</a></h4>
<p>For a broad overview, use tools like the
<a class="reference internal" href="#mi300x-pytorch-profiler"><span class="std std-ref">PyTorch Profiler</span></a>, which helps in
understanding how PyTorch operations are executed and where time is spent. This
is particularly useful for developers new to workload tuning, as it provides a
comprehensive view without requiring in-depth knowledge of lower-level
operations.</p>
</section>
<section id="kernel-level-profiling-tools">
<h4>Kernel-level profiling tools<a class="headerlink" href="#kernel-level-profiling-tools" title="Link to this heading">#</a></h4>
<p>When profiling indicates that GPUs are a performance bottleneck, delve deeper
into kernel-level profiling. Tools such as the
<a class="reference internal" href="#mi300x-rocr-debug-agent"><span class="std std-ref">ROCr Debug Agent</span></a>,
<a class="reference internal" href="#mi300x-rocprof"><span class="std std-ref">ROCProfiler</span></a>, and
<a class="reference internal" href="#mi300x-omniperf"><span class="std std-ref">Omniperf</span></a> offer detailed insights
into GPU kernel execution. These tools can help isolate problematic GPU
operations and provide data needed for targeted optimizations.</p>
</section>
</section>
<section id="analyze-and-tune">
<h3>Analyze and tune<a class="headerlink" href="#analyze-and-tune" title="Link to this heading">#</a></h3>
<p>Based on the insights gained from profiling, focus your tuning efforts on the
identified bottlenecks. This might involve optimizing specific kernel
operations, adjusting memory access patterns, or modifying computational
algorithms.</p>
<p>The following subsections discuss optimization ranging from high-level and more
automated strategies to more involved, hands-on optimization.</p>
<section id="optimize-model-inference-with-vllm">
<h4>Optimize model inference with vLLM<a class="headerlink" href="#optimize-model-inference-with-vllm" title="Link to this heading">#</a></h4>
<p>vLLM provides tools and techniques specifically designed for efficient model
inference on AMD Instinct MI300X accelerators. See <a class="reference internal" href="../../llm-fine-tuning-optimization/llm-inference-frameworks.html#fine-tuning-llms-vllm"><span class="std std-ref">vLLM inference</span></a>
for installation guidance. Optimizing performance with vLLM
involves configuring tensor parallelism, leveraging advanced features, and
ensuring efficient execution. Here’s how to optimize vLLM performance:</p>
<ul class="simple">
<li><p>Tensor parallelism: Configure the
<a class="reference internal" href="#mi300x-vllm-optimize-tp-gemm"><span class="std std-ref">tensor-parallel-size parameter</span></a> to distribute
tensor computations across multiple GPUs. Adjust parameters such as
<code class="docutils literal notranslate"><span class="pre">batch-size</span></code>, <code class="docutils literal notranslate"><span class="pre">input-len</span></code>, and <code class="docutils literal notranslate"><span class="pre">output-len</span></code> based on your workload.</p></li>
<li><p>Configuration for vLLM: Set <a class="reference internal" href="#mi300x-vllm-optimization"><span class="std std-ref">parameters</span></a>
according to workload requirements. Benchmark performance to understand
characteristics and identify bottlenecks.</p></li>
<li><p>Benchmarking and performance metrics: Measure latency and throughput to
evaluate performance.</p></li>
</ul>
</section>
<section id="auto-tunable-configurations">
<span id="mi300x-auto-tune"></span><h4>Auto-tunable configurations<a class="headerlink" href="#auto-tunable-configurations" title="Link to this heading">#</a></h4>
<p>Auto-tunable configurations can significantly streamline performance
optimization by automatically adjusting parameters based on workload
characteristics. For example:</p>
<ul class="simple">
<li><p>PyTorch: Utilize <a class="reference internal" href="#mi300x-torchinductor-tuning"><span class="std std-ref">PyTorch’s built-in auto-tuning features</span></a>,
such as the <a class="reference internal" href="#mi300x-tunableop"><span class="std std-ref">TunableOp</span></a> module, which helps in
optimizing operation performance by exploring different configurations.</p></li>
<li><p>MIOpen: Leverage <a class="reference internal" href="#mi300x-miopen-tuning"><span class="std std-ref">MIOpen’s auto-tuning capabilities</span></a>
for convolutional operations and other primitives to find optimal settings for
your specific hardware.</p></li>
<li><p>Triton: Use <a class="reference internal" href="#mi300x-autotunable-kernel-config"><span class="std std-ref">Triton’s auto-tuning features</span></a>
to explore various kernel configurations and automatically select the
best-performing ones.</p></li>
</ul>
</section>
<section id="manual-tuning">
<h4>Manual tuning<a class="headerlink" href="#manual-tuning" title="Link to this heading">#</a></h4>
<p>Advanced developers can manually adjust parameters and configurations to
optimize performance. Both Triton and HIP involve manual tuning aspects.</p>
<ul class="simple">
<li><p>ROCm libraries: Optimize GPU performance by adjusting various parameters and
configurations within <a class="reference internal" href="#mi300x-rocm-library-tuning"><span class="std std-ref">ROCm libraries</span></a>. This
approach involves hands-on optimization to maximize efficiency for specific
workloads.</p></li>
<li><p>Triton: Tune Triton kernels by adjusting parameters tailored to
your workload to
<a class="reference internal" href="#mi300x-triton-gpu-utilization"><span class="std std-ref">optimize GPU resource utilization</span></a> and
better <a class="reference internal" href="#mi300x-assembly-analysis"><span class="std std-ref">leverage specific hardware features</span></a>.</p></li>
<li><p>HIP: Profile and <a class="reference internal" href="#mi300x-hip-optimization"><span class="std std-ref">optimize HIP kernels</span></a> by
optimizing parallel execution, memory access patterns, and other aspects.</p></li>
</ul>
</section>
</section>
<section id="iterate-and-validate">
<h3>Iterate and validate<a class="headerlink" href="#iterate-and-validate" title="Link to this heading">#</a></h3>
<p>Optimization is an iterative process. After applying tuning changes, re-profile
the workload to validate improvements and ensure that the changes have had the
desired effect. Continuous iteration helps refine the performance gains and
address any new bottlenecks that may emerge.</p>
<p>ROCm provides a prebuilt optimized Docker image that has everything required to implement
the tips in this section. It includes ROCm, vLLM, PyTorch, and tuning files in the CSV
format. For more information, see <a class="reference internal" href="../../performance-validation/mi300x/vllm-benchmark.html"><span class="doc">LLM inference performance validation on AMD Instinct MI300X</span></a>.</p>
</section>
</section>
<section id="profiling-tools">
<span id="mi300x-profiling-tools"></span><h2>Profiling tools<a class="headerlink" href="#profiling-tools" title="Link to this heading">#</a></h2>
<p>AMD profiling tools provide valuable insights into how efficiently your
application utilizes hardware and help diagnose potential bottlenecks that
contribute to poor performance. Developers targeting AMD GPUs have multiple
tools available depending on their specific profiling needs.</p>
<ul class="simple">
<li><p>ROCProfiler tool collects kernel execution performance
metrics. For more information, see the
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler/en/amd-master/index.html" title="(in rocprofiler Documentation v2.0.0)"><span class="xref std std-doc">ROCProfiler</span></a>
documentation.</p></li>
<li><p>Omniperf builds upon ROCProfiler but provides more guided analysis.
For more information, see
<a class="reference external" href="https://rocm.docs.amd.com/projects/omniperf/en/amd-staging/index.html" title="(in ROCm Compute Profiler v3.0.0)"><span class="xref std std-doc">Omniperf documentation</span></a>.</p></li>
</ul>
<p>Refer to <a class="reference internal" href="../../llm-fine-tuning-optimization/profiling-and-debugging.html"><span class="doc">Profiling and debugging</span></a>
to explore commonly used profiling tools and their usage patterns.</p>
<p>Once performance bottlenecks are identified, you can implement an <em>informed</em> workload
tuning strategy. If kernels are the bottleneck, consider:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#mi300x-tunableop"><span class="std std-ref">Auto-tuning in PyTorch with TunableOp</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-miopen-tuning"><span class="std std-ref">Auto-tuning in MIOpen</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-autotunable-kernel-config"><span class="std std-ref">Triton auto-tunable kernel configurations</span></a></p></li>
</ul>
<p>If auto-tuning does not meet your requirements, consider
<a class="reference internal" href="#mi300x-triton-kernel-performance-optimization"><span class="std std-ref">Triton kernel performance optimization</span></a>.</p>
<p>If the issue is multi-GPU scale-out, try
<a class="reference internal" href="#mi300x-rccl"><span class="std std-ref">RCCL tuning and configuration</span></a>.</p>
<p>This section discusses profiling and debugging tools and some of their common usage patterns with ROCm applications.</p>
<section id="pytorch-profiler">
<span id="mi300x-pytorch-profiler"></span><h3>PyTorch Profiler<a class="headerlink" href="#pytorch-profiler" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/profiler.html">PyTorch Profiler</a> can be invoked inside Python scripts, letting you
collect CPU and GPU performance metrics while the script is running. See the <a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">PyTorch Profiler tutorial</a> for more information.</p>
<p>You can then visualize and view these metrics using an open-source profile visualization tool like
<a class="reference external" href="https://ui.perfetto.dev">Perfetto UI</a>.</p>
<ol class="arabic">
<li><p>Use the following snippet to invoke PyTorch Profiler in your code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>
<span class="kn">from</span> <span class="nn">torch.profiler</span> <span class="kn">import</span> <span class="n">profile</span><span class="p">,</span> <span class="n">record_function</span><span class="p">,</span> <span class="n">ProfilerActivity</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">])</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">"model_inference"</span><span class="p">):</span>
        <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">prof</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="s2">"resnet18_profile.json"</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Profile results in <code class="docutils literal notranslate"><span class="pre">resnet18_profile.json</span></code> can be viewed by the Perfetto visualization tool. Go to
<a class="reference external" href="https://ui.perfetto.dev">https://ui.perfetto.dev</a> and import the file. In your Perfetto visualization, you’ll see that the upper section
shows transactions denoting the CPU activities that launch GPU kernels while the lower section shows the actual GPU
activities where it processes the <code class="docutils literal notranslate"><span class="pre">resnet18</span></code> inferences layer by layer.</p>
<figure class="align-default" id="id2">
<img alt="../../../_images/perfetto-trace.svg" src="../../../_images/perfetto-trace.svg"/><figcaption>
<p><span class="caption-text">Perfetto trace visualization example.</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
</li>
</ol>
</section>
<section id="rocm-profiling-tools">
<h3>ROCm profiling tools<a class="headerlink" href="#rocm-profiling-tools" title="Link to this heading">#</a></h3>
<p>Heterogenous systems, where programs run on both CPUs and GPUs, introduce additional complexities. Understanding the
critical path and kernel execution is all the more important; so, performance tuning is a necessary component in the
benchmarking process.</p>
<p>With AMD’s profiling tools, developers are able to gain important insight into how efficiently their application is
using hardware resources and effectively diagnose potential bottlenecks contributing to poor performance. Developers
working with AMD Instinct accelerators have multiple tools depending on their specific profiling needs; these are:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#mi300x-rocprof"><span class="std std-ref">ROCProfiler</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-omniperf"><span class="std std-ref">Omniperf</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-omnitrace"><span class="std std-ref">Omnitrace</span></a></p></li>
</ul>
<section id="rocprofiler">
<span id="mi300x-rocprof"></span><h4>ROCProfiler<a class="headerlink" href="#rocprofiler" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/rocprofiler/en/amd-master/index.html" title="(in rocprofiler Documentation v2.0.0)"><span class="xref std std-doc">ROCProfiler</span></a> is primarily a low-level API for accessing and extracting GPU hardware performance
metrics, commonly called <em>performance counters</em>. These counters quantify the performance of the underlying architecture
showcasing which pieces of the computational pipeline and memory hierarchy are being utilized.</p>
<p>Your ROCm installation contains a script or executable command called <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> which provides the ability to list all
available hardware counters for your specific accelerator or GPU, and run applications while collecting counters during
their execution.</p>
<p>This <code class="docutils literal notranslate"><span class="pre">rocprof</span></code> utility also depends on the <a class="reference external" href="https://rocm.docs.amd.com/projects/roctracer/en/amd-master/index.html" title="(in roctracer Documentation v4.1.0)"><span class="xref std std-doc">ROCTracer and ROC-TX libraries</span></a>, giving it the
ability to collect timeline traces of the accelerator software stack as well as user-annotated code regions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">rocprof</span></code> is a CLI-only utility so input and output takes the format of <code class="docutils literal notranslate"><span class="pre">.txt</span></code> and CSV files. These
formats provide a raw view of the data and puts the onus on the user to parse and analyze. Therefore, <code class="docutils literal notranslate"><span class="pre">rocprof</span></code>
gives the user full access and control of raw performance profiling data, but requires extra effort to analyze the
collected data.</p>
</div>
</section>
<section id="omniperf">
<span id="mi300x-omniperf"></span><h4>Omniperf<a class="headerlink" href="#omniperf" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/omniperf/en/amd-staging/index.html" title="(in ROCm Compute Profiler v3.0.0)"><span class="xref std std-doc">Omniperf</span></a> is a system performance profiler for high-performance computing (HPC) and
machine learning (ML) workloads using Instinct accelerators. Under the hood, Omniperf uses
<a class="reference internal" href="#mi300x-rocprof"><span class="std std-ref">ROCProfiler</span></a> to collect hardware performance counters. The Omniperf tool performs
system profiling based on all approved hardware counters for Instinct
accelerator architectures. It provides high level performance analysis features including System Speed-of-Light, IP
block Speed-of-Light, Memory Chart Analysis, Roofline Analysis, Baseline Comparisons, and more.</p>
<p>Omniperf takes the guesswork out of profiling by removing the need to provide text input files with lists of counters
to collect and analyze raw CSV output files as is the case with ROC-profiler. Instead, Omniperf automates the collection
of all available hardware counters in one command and provides a graphical interface to help users understand and
analyze bottlenecks and stressors for their computational workloads on AMD Instinct accelerators.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Omniperf collects hardware counters in multiple passes, and will therefore re-run the application during each pass
to collect different sets of metrics.</p>
</div>
<figure class="align-default" id="id3">
<img alt="../../../_images/omniperf-analysis.png" src="../../../_images/omniperf-analysis.png"/>
<figcaption>
<p><span class="caption-text">Omniperf memory chat analysis panel.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In brief, Omniperf provides details about hardware activity for a particular GPU kernel. It also supports both
a web-based GUI or command-line analyzer, depending on your preference.</p>
</section>
<section id="omnitrace">
<span id="mi300x-omnitrace"></span><h4>Omnitrace<a class="headerlink" href="#omnitrace" title="Link to this heading">#</a></h4>
<p><span class="xref std std-doc">Omnitrace</span> is a comprehensive profiling and tracing tool for parallel applications,
including HPC and ML packages, written in C, C++, Fortran, HIP, OpenCL, and Python which execute on the CPU or CPU and
GPU. It is capable of gathering the performance information of functions through any combination of binary
instrumentation, call-stack sampling, user-defined regions, and Python interpreter hooks.</p>
<p>Omnitrace supports interactive visualization of comprehensive traces in the web browser in addition to high-level
summary profiles with <code class="docutils literal notranslate"><span class="pre">mean/min/max/stddev</span></code> statistics. Beyond runtime
information, Omnitrace supports the collection of system-level metrics such as CPU frequency, GPU temperature, and GPU
utilization. Process and thread level metrics such as memory usage, page faults, context switches, and numerous other
hardware counters are also included.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When analyzing the performance of an application, it is best not to assume you know where the performance
bottlenecks are and why they are happening. Omnitrace is the ideal tool for characterizing where optimization would
have the greatest impact on the end-to-end execution of the application and to discover what else is happening on the
system during a performance bottleneck.</p>
</div>
<figure class="align-default" id="id4">
<img alt="../../../_images/omnitrace-timeline.png" src="../../../_images/omnitrace-timeline.png"/>
<figcaption>
<p><span class="caption-text">Omnitrace timeline trace example.</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For details usage and examples of using these tools, refer to the
<a class="reference external" href="https://rocm.blogs.amd.com/software-tools-optimization/profilers/README.html">Introduction to profiling tools for AMD hardware</a>
developer blog.</p>
</section>
</section>
</section>
<section id="vllm-performance-optimization">
<span id="mi300x-vllm-optimization"></span><h2>vLLM performance optimization<a class="headerlink" href="#vllm-performance-optimization" title="Link to this heading">#</a></h2>
<p>The following performance tips are not <em>specific</em> to vLLM – they are general
but relevant in this context. You can tune the following vLLM parameters to
achieve optimal request latency and throughput performance.</p>
<ul class="simple">
<li><p>As described in <a class="reference internal" href="../../system-optimization/mi300x.html#mi300x-env-vars"><span class="std std-ref">Environment variables</span></a>, the environment
variable <code class="docutils literal notranslate"><span class="pre">HIP_FORCE_DEV_KERNARG</span></code> can improve vLLM performance. Set it to
<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">HIP_FORCE_DEV_KERNARG=1</span></code>.</p></li>
<li><p>vLLM is based on PyTorch. Therefore, the suggestions in the
<a class="reference internal" href="#mi300x-tunableop"><span class="std std-ref">TunableOp section</span></a> are also applicable to vLLM tuning
as long as the PyTorch version is 2.3 or later.</p></li>
<li><p>Set the <a class="reference internal" href="#mi300x-rccl"><span class="std std-ref">RCCL environment variable</span></a> <code class="docutils literal notranslate"><span class="pre">NCCL_MIN_NCHANNELS</span></code>
to <code class="docutils literal notranslate"><span class="pre">112</span></code> to increase the number of channels on MI300X to potentially improve
the performance.</p></li>
</ul>
<p>The following subsections describe vLLM-specific suggestions for performance.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensor_parallel_size</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_model_len</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_memory_utilization</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enforce_eager</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kv_cache_dtype</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_len</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_len</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enforce_eager</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_chunked_prefill</span></code></p></li>
</ul>
<p>Refer to <a class="reference external" href="https://docs.vllm.ai/en/latest/models/performance.html">vLLM documentation</a>
for additional performance tips. <a class="reference internal" href="../../llm-fine-tuning-optimization/llm-inference-frameworks.html#fine-tuning-llms-vllm"><span class="std std-ref">vLLM inference</span></a> describes vLLM
usage with ROCm.</p>
<p>ROCm provides a prebuilt optimized Docker image for validating the performance
of LLM inference with vLLM on the MI300X accelerator. The Docker image includes
ROCm, vLLM, PyTorch, and tuning files in the CSV format. For more information,
see <a class="reference internal" href="../../performance-validation/mi300x/vllm-benchmark.html"><span class="doc">LLM inference performance validation on AMD Instinct MI300X</span></a>.</p>
<section id="maximize-throughput">
<h3>Maximize throughput<a class="headerlink" href="#maximize-throughput" title="Link to this heading">#</a></h3>
<p>The general guideline is to maximize per-node throughput. Specify proper
GPU memory utilization to run as many instances of vLLM as possible on a
single GPU. However, too many instances can result in no memory for
KV-cache.</p>
<p>You can run vLLM on MI300X (gfx942), for example, using model weights
for <code class="docutils literal notranslate"><span class="pre">llama2</span></code> (<code class="docutils literal notranslate"><span class="pre">7b</span></code>, <code class="docutils literal notranslate"><span class="pre">13b</span></code>, <code class="docutils literal notranslate"><span class="pre">70b</span></code>) and <code class="docutils literal notranslate"><span class="pre">llama3</span></code> models (<code class="docutils literal notranslate"><span class="pre">8b</span></code>,
<code class="docutils literal notranslate"><span class="pre">70b</span></code>).</p>
<p>As described in the
<a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/data-sheets/amd-instinct-mi300x-data-sheet.pdf">AMD Instinct MI300X Accelerator</a>
data sheet, the GPU memory capacity is 192 GB. This means you can run
llama2-70b and llama3-70b models on one GPU.</p>
<p>To maximize the accumulated throughput, you can also run eight instances
vLLM simultaneously on one MI300X node (with eight GPUs). To do so, use
the GPU isolation environment variable <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code>.</p>
<p>For example, this script runs eight instances of vLLM for throughput
benchmarking at the same time:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span>i<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>seq<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">7</span><span class="k">)</span><span class="p">;</span>
<span class="k">do</span>
<span class="w">    </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="s2">"</span><span class="nv">$i</span><span class="s2">"</span><span class="w"> </span>python3<span class="w"> </span>/app/vllm/benchmarks/benchmark_throughput.py<span class="w"> </span>-tp<span class="w"> </span><span class="m">1</span><span class="w"> </span>--dataset<span class="w"> </span><span class="s2">"/path/to/dataset/ShareGPT_V3_unfiltered_cleaned_split.json"</span><span class="w"> </span>--model<span class="w"> </span>/path/to/model<span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>
</pre></div>
</div>
<p>Run two instances of <code class="docutils literal notranslate"><span class="pre">llama3-8b</span></code> model at the same time on one single GPU
by specifying <code class="docutils literal notranslate"><span class="pre">--gpu-memory-utilization</span></code> to 0.4 (40%), as below (on GPU
0):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python3
/vllm-workspace/benchmarks/benchmark_throughput.py<span class="w"> </span>--gpu-memory-utilization
<span class="m">0</span>.4<span class="w"> </span>--dataset
<span class="s2">"/path/to/dataset/ShareGPT_V3_unfiltered_cleaned_split.json"</span><span class="w"> </span>--model
/path/to/model<span class="w"> </span><span class="p">&amp;</span>

<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python3
/vllm-workspace/benchmarks/benchmark_throughput.py<span class="w"> </span>--gpu-memory-utilization
<span class="m">0</span>.4<span class="w"> </span>--dataset
<span class="s2">"/path/to/dataset/ShareGPT_V3_unfiltered_cleaned_split.json"</span><span class="w"> </span>--model
/path/to/model<span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>Similarly, use the <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> environment variable to specify
which GPU (0-7) will run those instances.</p>
</section>
<section id="run-vllm-on-multiple-gpus">
<span id="mi300x-vllm-multiple-gpus"></span><h3>Run vLLM on multiple GPUs<a class="headerlink" href="#run-vllm-on-multiple-gpus" title="Link to this heading">#</a></h3>
<p>The two main reasons to use multiple GPUs:</p>
<ul class="simple">
<li><p>The model size is too big to run vLLM using one GPU as it results
CUDA/HIP Out of Memory.</p></li>
<li><p>To achieve better latency.</p></li>
</ul>
<p>To run one vLLM instance on multiple GPUs, use the <code class="docutils literal notranslate"><span class="pre">-tp</span></code> or
<code class="docutils literal notranslate"><span class="pre">--tensor-parallel-size</span></code> option to specify multiple GPUs. Optionally, use the
<code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> environment variable to specify the GPUs.</p>
<p>For example, you can use two GPUs to start an API server on port 8000 as
below:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.api_server<span class="w"> </span>--model<span class="w"> </span>/path/to/model<span class="w"> </span>--dtype
float16<span class="w"> </span>-tp<span class="w"> </span><span class="m">2</span><span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>To achieve both latency and throughput performance for serving, you can
run multiple API servers on different GPUs by specifying different ports
for each server and use <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> to specify the GPUs for
each server, for example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.api_server<span class="w"> </span>--model
/path/to/model<span class="w"> </span>--dtype<span class="w"> </span>float16<span class="w"> </span>-tp<span class="w"> </span><span class="m">2</span><span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="p">&amp;</span>

<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">2</span>,3<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.api_server<span class="w"> </span>--model
/path/to/model<span class="w"> </span>--dtype<span class="w"> </span>float16<span class="w"> </span>-tp<span class="w"> </span><span class="m">2</span><span class="w"> </span>--port<span class="w"> </span><span class="m">8001</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#mi300x-vllm-optimize-tp-gemm"><span class="std std-ref">Optimize tensor parallelism and GEMM performance</span></a> for additional optimization suggestions.</p>
</section>
<section id="choose-different-attention-backends">
<h3>Choose different attention backends<a class="headerlink" href="#choose-different-attention-backends" title="Link to this heading">#</a></h3>
<p>vLLM on ROCm supports three different attention backends, each suitable for
different use cases and performance requirements:</p>
<ul class="simple">
<li><p><strong>Triton Flash Attention</strong> - For benchmarking, run vLLM scripts at
least once as a warm-up step so Triton can perform auto-tuning before
collecting benchmarking numbers. This is the default setting.</p></li>
<li><p><strong>Composable Kernel (CK) Flash Attention</strong> - To use CK Flash Attention, specify
the environment variable as <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">VLLM_USE_TRITON_FLASH_ATTN=0</span></code>.</p></li>
<li><p><strong>PyTorch naive attention</strong> - To use naive attention (PyTorch SDPA math
backend), either build the Docker image without Flash Attention by passing
<code class="docutils literal notranslate"><span class="pre">--build-arg</span> <span class="pre">BUILD_FA="0"</span></code> during Docker build, or
<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">uninstall</span> <span class="pre">flash-attn</span></code> inside the container, and export <code class="docutils literal notranslate"><span class="pre">VLLM_USE_TRITON_FLASH_ATTN=0</span></code> when running the vLLM instance.</p></li>
</ul>
<p>Refer to <a class="reference internal" href="../../llm-fine-tuning-optimization/model-acceleration-libraries.html#acceleration-flash-attention"><span class="std std-ref">Model acceleration libraries</span></a>
to learn more about Flash Attention with Triton or CK backends.</p>
</section>
<section id="use-fp8-kv-cache-data-type">
<h3>Use fp8 KV-cache data type<a class="headerlink" href="#use-fp8-kv-cache-data-type" title="Link to this heading">#</a></h3>
<p>Using <code class="docutils literal notranslate"><span class="pre">fp8</span> <span class="pre">kv-cache</span> <span class="pre">dtype</span></code> can improve performance as it reduces the size
of <code class="docutils literal notranslate"><span class="pre">kv-cache</span></code>. As a result, it reduces the cost required for reading and
writing the <code class="docutils literal notranslate"><span class="pre">kv-cache</span></code>.</p>
<p>To use this feature, specify <code class="docutils literal notranslate"><span class="pre">--kv-cache-dtype</span></code> as <code class="docutils literal notranslate"><span class="pre">fp8</span></code>.</p>
<p>To specify the quantization scaling config, use the
<code class="docutils literal notranslate"><span class="pre">--quantization-param-path</span></code> parameter. If the parameter isn’t specified,
the default scaling factor of <code class="docutils literal notranslate"><span class="pre">1</span></code> is used, which can lead to less accurate
results. To generate <code class="docutils literal notranslate"><span class="pre">kv-cache</span></code> scaling JSON file, see <a class="reference external" href="https://github.com/vllm-project/vllm/blob/main/examples/fp8/README.md">FP8 KV
Cache</a>
in the vLLM GitHub repository.</p>
<p>Two sample Llama scaling configuration files are in vLLM for <code class="docutils literal notranslate"><span class="pre">llama2-70b</span></code> and
<code class="docutils literal notranslate"><span class="pre">llama2-7b</span></code>.</p>
<p>If building the vLLM using
<a class="reference external" href="https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm">Dockerfile.rocm</a>
for <code class="docutils literal notranslate"><span class="pre">llama2-70b</span></code> scale config, find the file at
<code class="docutils literal notranslate"><span class="pre">/vllm-workspace/tests/fp8_kv/llama2-70b-fp8-kv/kv_cache_scales.json</span></code> at
runtime.</p>
<p>Below is a sample command to run benchmarking with this feature enabled
for the <code class="docutils literal notranslate"><span class="pre">llama2-70b</span></code> model:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>/vllm-workspace/benchmarks/benchmark_throughput.py<span class="w"> </span>--model
/path/to/llama2-70b-model<span class="w"> </span>--kv-cache-dtype<span class="w"> </span><span class="s2">"fp8"</span>
--quantization-param-path
<span class="s2">"/vllm-workspace/tests/fp8_kv/llama2-70b-fp8-kv/kv_cache_scales.json"</span>
--input-len<span class="w"> </span><span class="m">512</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">256</span><span class="w"> </span>--num-prompts<span class="w"> </span><span class="m">500</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As of the writing of this document, this feature enhances
performance when a single GPU is used (with a tensor-parallel size of
1).</p>
</div>
</section>
<section id="enable-chunked-prefill">
<h3>Enable chunked prefill<a class="headerlink" href="#enable-chunked-prefill" title="Link to this heading">#</a></h3>
<p>Another vLLM performance tip is to enable chunked prefill to improve
throughput. Chunked prefill allows large prefills to be chunked into
smaller chunks and batched together with decode requests.</p>
<p>You can enable the feature by specifying <code class="docutils literal notranslate"><span class="pre">--enable-chunked-prefill</span></code> in the
command line or setting <code class="docutils literal notranslate"><span class="pre">enable_chunked_prefill=True</span></code> in the LLM
constructor.</p>
<p>As stated in <a class="reference external" href="https://docs.vllm.ai/en/latest/models/performance.html#chunked-prefill">vLLM’s documentation,</a>,
you can tune the performance by changing <code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code>. By
default, it is set to 512 and optimized for ITL (inter-token latency).
Smaller <code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code> achieves better ITL because there are
fewer prefills interrupting decodes.
Higher <code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code> achieves better TTFT (time to the first
token) as you can put more prefill to the batch.</p>
<p>You might experience noticeable throughput improvements when
benchmarking on a single GPU or 8 GPUs using the vLLM throughput
benchmarking script along with the ShareGPT dataset as input.</p>
<p>In the case of fixed <code class="docutils literal notranslate"><span class="pre">input-len</span></code>/<code class="docutils literal notranslate"><span class="pre">output-len</span></code>, for some configurations,
enabling chunked prefill increases the throughput. For some other
configurations, the throughput may be worse and elicit a need to tune
parameter <code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code> (for example, increasing <code class="docutils literal notranslate"><span class="pre">max_num_batched_tokens</span></code> value to 4096 or larger).</p>
</section>
<section id="optimize-tensor-parallelism-and-gemm-performance">
<span id="mi300x-vllm-optimize-tp-gemm"></span><h3>Optimize tensor parallelism and GEMM performance<a class="headerlink" href="#optimize-tensor-parallelism-and-gemm-performance" title="Link to this heading">#</a></h3>
<p>You can use tensor parallelism to improve performance in model inference
tasks by distributing tensor computations across multiple GPUs.
The <a class="reference external" href="https://github.com/ROCm/vllm">ROCm vLLM</a> fork supports two modes
to run tensor parallelism: <code class="docutils literal notranslate"><span class="pre">ray</span></code> and <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> which (the default in ROCm
for performance reasons).</p>
<ul>
<li><p>To use <a class="reference external" href="https://pytorch.org/docs/stable/elastic/run.html">torchrun</a>,
use the following command where <code class="docutils literal notranslate"><span class="pre">$WORLD_SIZE</span></code> is the number of GPUs or number
of workers to use per node. In the case of <code class="docutils literal notranslate"><span class="pre">nnodes=1</span></code> (that is, the number of
nodes is 1), it’s the same as the <code class="docutils literal notranslate"><span class="pre">tensor-parallel-size</span></code> or <code class="docutils literal notranslate"><span class="pre">-tp</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>torchrun<span class="w"> </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--nproc-per-node<span class="o">=</span><span class="nv">$WORLD_SIZE</span><span class="w"> </span>YOUR_PYTHON_SCRIPT.py<span class="w"> </span><span class="o">(</span>--tensor-parallel-size<span class="w"> </span><span class="nv">$WORLD_SIZE</span><span class="w"> </span>..<span class="w"> </span>other_script_args...<span class="o">)</span>
</pre></div>
</div>
</li>
<li><p>To use <code class="docutils literal notranslate"><span class="pre">ray</span></code>, specify the <code class="docutils literal notranslate"><span class="pre">--worker-use-ray</span></code> flag. The following script
example uses <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> to run latency benchmarking using <code class="docutils literal notranslate"><span class="pre">ray</span></code>
for <code class="docutils literal notranslate"><span class="pre">input-len</span></code> of 512, <code class="docutils literal notranslate"><span class="pre">output-len</span></code> of 512, and <code class="docutils literal notranslate"><span class="pre">batch-size</span></code> of 1:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">tp</span><span class="o">=</span><span class="nv">$1</span>

torchrun<span class="w"> </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--nproc-per-node<span class="o">=</span><span class="nv">$tp</span><span class="w"> </span>benchmarks/benchmark_latency.py<span class="w"> </span>--worker-use-ray<span class="w"> </span>--model<span class="w"> </span><span class="nv">$MODEL</span><span class="w"> </span>--batch-size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--input-len<span class="w"> </span><span class="m">512</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">512</span><span class="w"> </span>--tensor-parallel-size<span class="w"> </span><span class="nv">$tp</span><span class="w"> </span>--num-iters<span class="w"> </span><span class="m">10</span>
</pre></div>
</div>
<p>The first parameter of the script <code class="docutils literal notranslate"><span class="pre">tp</span></code> specifies the <code class="docutils literal notranslate"><span class="pre">tensor-parallel</span></code> size
(1 to 8).</p>
</li>
</ul>
<section id="gemm-tuning-steps">
<h4>GEMM tuning steps<a class="headerlink" href="#gemm-tuning-steps" title="Link to this heading">#</a></h4>
<p>This section describes the process of optimizing the parameters and
configurations of GEMM operations to improve their performance on specific
hardware. This involves finding the optimal settings for memory usage,
computation, and hardware resources to achieve faster and more efficient
matrix multiplication.</p>
<p>Follow these steps to perform GEMM tuning with ROCm vLLM:</p>
<ol class="arabic">
<li><p>Set various environment variables to specify paths for tuning files and
enable debugging options:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_UNTUNE_FILE</span><span class="o">=</span><span class="s2">"/tmp/vllm_untuned.csv"</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_TUNE_FILE</span><span class="o">=</span><span class="s2">"</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span><span class="s2">/vllm/tuned.csv"</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">HIP_FORCE_DEV_KERNARG</span><span class="o">=</span><span class="m">1</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">DEBUG_CLR_GRAPH_PACKET_CAPTURE</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
</li>
<li><p>Perform a tuning run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">VLLM_TUNE_GEMM</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>torchrun<span class="w"> </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--nproc-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>vllm/benchmarks/benchmark_latency.py<span class="w"> </span>--batch-size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--input-len<span class="w"> </span><span class="m">2048</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">128</span><span class="w"> </span>--model<span class="w"> </span>/models/llama-2-70b-chat-hf/<span class="w"> </span>-tp<span class="w"> </span><span class="m">8</span>

python<span class="w"> </span><span class="nv">$PATH_TO_GRADLIB</span>/gemm_tuner.py<span class="w"> </span>--input<span class="w"> </span>/tmp/vllm_untuned.csv<span class="w"> </span>--tuned_file<span class="w"> </span>vllm/tuned.csv
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">$PATH_TO_GRADLIB</span></code> is the installation path of <code class="docutils literal notranslate"><span class="pre">gradlib</span></code>. To find
where <code class="docutils literal notranslate"><span class="pre">gradlib</span></code> is, you can run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">show</span> <span class="pre">gradlib</span></code> and then update the
above path to something like <code class="docutils literal notranslate"><span class="pre">/opt/conda/envs/py_3.9/lib/python3.9/site-packages/gradlib/gemm_tuner.py</span></code></p>
</li>
<li><p>Do a measurement run to verify performance improvements:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">VLLM_TUNE_GEMM</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>torchrun<span class="w"> </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--nproc-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>vllm/benchmarks/benchmark_latency.py<span class="w"> </span>--batch-size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--input-len<span class="w"> </span><span class="m">2048</span><span class="w"> </span>--output-len<span class="w"> </span><span class="m">128</span><span class="w"> </span>--model<span class="w"> </span>/models/llama-2-70b-chat-hf/<span class="w"> </span>-tp<span class="w"> </span><span class="m">8</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
<section id="pytorch-tunableop">
<span id="mi300x-tunableop"></span><h2>PyTorch TunableOp<a class="headerlink" href="#pytorch-tunableop" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md">TunableOp</a>
is a feature used to define and optimize kernels that can have tunable parameters. This is useful in
optimizing the performance of custom kernels by exploring different parameter configurations to find the most efficient
setup. See more about PyTorch TunableOp in <a class="reference internal" href="../../llm-fine-tuning-optimization/model-acceleration-libraries.html#fine-tuning-llms-pytorch-tunableop"><span class="std std-ref">Model acceleration libraries</span></a>.</p>
<p>You can easily manipulate the behavior TunableOp through environment variables, though you could use the C++ interface
<code class="docutils literal notranslate"><span class="pre">at::cuda::tunable::getTuningContext()</span></code>. A Python interface to the <code class="docutils literal notranslate"><span class="pre">TuningContext</span></code> does not yet exist.</p>
<p>The three most important environment variables are:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">PYTORCH_TUNABLEOP_ENABLED</span></code></dt><dd><p>Default is <code class="docutils literal notranslate"><span class="pre">0</span></code>. Set to <code class="docutils literal notranslate"><span class="pre">1</span></code> to enable. This is the main on/off switch for
all TunableOp implementations.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">PYTORCH_TUNABLEOP_TUNING</span></code></dt><dd><p>Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>. Set to <code class="docutils literal notranslate"><span class="pre">0</span></code> to disable. When enabled, if a tuned entry
isn’t found, run the tuning step and record the entry.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">PYTORCH_TUNABLEOP_VERBOSE</span></code></dt><dd><p>Default is <code class="docutils literal notranslate"><span class="pre">0</span></code>. Set to <code class="docutils literal notranslate"><span class="pre">1</span></code> if you want to see TunableOp in action.</p>
</dd>
</dl>
<p>Use these environment variables to enable TunableOp for any
applications or libraries that use PyTorch (2.3 or later). For more
information, see <a class="github reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md">pytorch/pytorch</a>
on GitHub.</p>
<p>You can check how TunableOp performs in two steps:</p>
<ol class="arabic">
<li><p>Enable TunableOp and tuning. Optionally enable verbose mode:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">PYTORCH_TUNABLEOP_ENABLED</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">PYTORCH_TUNABLEOP_VERBOSE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>your_script.sh
</pre></div>
</div>
</li>
<li><p>Enable TunableOp and disable tuning and measure.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">PYTORCH_TUNABLEOP_ENABLED</span><span class="o">=</span><span class="m">1</span><span class="w">  </span><span class="nv">PYTORCH_TUNABLEOP_TUNING</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>your_script.sh
</pre></div>
</div>
</li>
</ol>
</section>
<section id="pytorch-inductor-triton-tuning-knobs">
<span id="mi300x-torchinductor-tuning"></span><h2>PyTorch inductor Triton tuning knobs<a class="headerlink" href="#pytorch-inductor-triton-tuning-knobs" title="Link to this heading">#</a></h2>
<p>The following are suggestions for optimizing matrix multiplication (GEMM) and
convolution (<code class="docutils literal notranslate"><span class="pre">conv</span></code>) operations in PyTorch using <code class="docutils literal notranslate"><span class="pre">inductor</span></code>, a part of the
PyTorch compilation framework. The goal is to leverage Triton to achieve better
performance.</p>
<p>Learn more about TorchInductor environment variables and usage in
<a class="reference external" href="https://pytorch.org/docs/2.3/torch.compiler_inductor_profiling.html">PyTorch documentation</a>.</p>
<p>To tune Triton kernels with <code class="docutils literal notranslate"><span class="pre">gemm</span></code> and convolution ops (<code class="docutils literal notranslate"><span class="pre">conv</span></code>), use the
<code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> function with the <code class="docutils literal notranslate"><span class="pre">max-autotune</span></code> mode. This benchmarks a
predefined list of Triton configurations and selects the fastest one for each
shape. See the configurations in PyTorch source code:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/pytorch/blob/a1d02b423c6b4ccacd25ebe86de43f650463bbc6/torch/_inductor/kernel/conv.py#L51">conv configs for max-autotune</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/pytorch/blob/a1d02b423c6b4ccacd25ebe86de43f650463bbc6/torch/_inductor/kernel/mm_common.py#L118">matmul configs for max-autotune</a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Triton is not used if regular <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/develop/index.html" title="(in MIOpen Documentation v3.3.0)"><span class="xref std std-doc">MIOpen</span></a> or
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocBLAS/en/develop/index.html" title="(in rocBLAS Documentation v4.4.0)"><span class="xref std std-doc">rocBLAS</span></a> performs faster for a specific operation.</p>
</div>
<ul>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">torch._inductor.config.max_autotune</span> <span class="pre">=</span> <span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_MAX_AUTOTUNE=1</span></code>.</p></li>
<li><p>Or, for more fine-grained control:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch._inductor.config.max_autotune_gemm</span> <span class="pre">=</span> <span class="pre">True</span></code></dt><dd><p>To enable tuning or lowering of <code class="docutils literal notranslate"><span class="pre">mm</span></code>/<code class="docutils literal notranslate"><span class="pre">conv</span></code>s.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">torch._inductor.config.max_autotune.pointwise</span> <span class="pre">=</span> <span class="pre">True</span></code></dt><dd><p>To enable tuning for <code class="docutils literal notranslate"><span class="pre">pointwise</span></code>/<code class="docutils literal notranslate"><span class="pre">reduction</span></code> ops.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">torch._inductor.max_autotune_gemm_backends</span></code> or <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_MAX_AUTOTUNE_GEMM_BACKENDS</span></code></dt><dd><p>Selects the candidate backends for <code class="docutils literal notranslate"><span class="pre">mm</span></code> auto-tuning. Defaults to
<code class="docutils literal notranslate"><span class="pre">TRITON,ATEN</span></code>.
Limiting this to <code class="docutils literal notranslate"><span class="pre">TRITON</span></code> might improve performance by
enabling more fused <code class="docutils literal notranslate"><span class="pre">mm</span></code> kernels instead of going to rocBLAS.</p>
</dd>
</dl>
</li>
<li><p>For further <code class="docutils literal notranslate"><span class="pre">mm</span></code> tuning, tuning <code class="docutils literal notranslate"><span class="pre">coordinate_descent</span></code> might improve
performance.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch._inductor.config.coordinate_descent_tuning</span> <span class="pre">=</span> <span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_COORDINATE_DESCENT_TUNING=1</span></code></p>
</li>
<li><p>Inference can see large improvements on AMD GPUs by utilizing
<code class="docutils literal notranslate"><span class="pre">torch._inductor.config.freezing=True</span></code> or the <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_FREEZING=1</span></code> variable, which
in-lines weights as constants and enables constant folding optimizations.</p></li>
<li><p>Enabling <code class="docutils literal notranslate"><span class="pre">inductor</span></code>’s cpp_wrapper might improve overhead. This generates
C++ code which launches Triton binaries directly with
<code class="docutils literal notranslate"><span class="pre">hipModuleLaunchKernel</span></code> and relies on <cite>hipification</cite>.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch._inductor.config.cpp_wrapper=True</span></code> or <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_CPP_WRAPPER=1</span></code></p>
</li>
<li><p>Convolution workloads may see a performance benefit by specifying
<code class="docutils literal notranslate"><span class="pre">torch._inductor.config.layout_optimization=True</span></code> or <code class="docutils literal notranslate"><span class="pre">TORCHINDUCTOR_LAYOUT_OPTIMIZATION=1</span></code>.
This can help performance by enforcing <code class="docutils literal notranslate"><span class="pre">channel_last</span></code> memory format on the
convolution in TorchInductor, avoiding any unnecessary transpose operations.
Note that <code class="docutils literal notranslate"><span class="pre">PYTORCH_MIOPEN_SUGGEST_NHWC=1</span></code> is recommended if using this.</p></li>
<li><p>To extract the Triton kernels generated by <code class="docutils literal notranslate"><span class="pre">inductor</span></code>, set the environment variable
<code class="docutils literal notranslate"><span class="pre">TORCH_COMPILE_DEBUG=1</span></code>, which will create a <code class="docutils literal notranslate"><span class="pre">torch_compile_debug/</span></code> directory
in the current path. The wrapper codes generated by <code class="docutils literal notranslate"><span class="pre">inductor</span></code> are in one or more
<code class="docutils literal notranslate"><span class="pre">output_code.py</span></code> files corresponding to the FX graphs associated with the model.
The Triton kernels are defined in these generated codes.</p></li>
</ul>
</section>
<section id="rocm-library-tuning">
<span id="mi300x-rocm-library-tuning"></span><h2>ROCm library tuning<a class="headerlink" href="#rocm-library-tuning" title="Link to this heading">#</a></h2>
<p>ROCm library tuning involves optimizing the performance of routine computational
operations (such as GEMM) provided by ROCm libraries like
<a class="reference internal" href="#mi300x-hipblaslt"><span class="std std-ref">hipBLASLt</span></a>, <a class="reference internal" href="#mi300x-ck"><span class="std std-ref">Composable Kernel</span></a>,
<a class="reference internal" href="#mi300x-miopen"><span class="std std-ref">MIOpen</span></a>, and <a class="reference internal" href="#mi300x-rccl"><span class="std std-ref">RCCL</span></a>. This tuning aims
to maximize efficiency and throughput on Instinct MI300X accelerators to gain
improved application performance.</p>
<section id="gemm-general-matrix-multiplication">
<span id="mi300x-library-gemm"></span><h3>GEMM (general matrix multiplication)<a class="headerlink" href="#gemm-general-matrix-multiplication" title="Link to this heading">#</a></h3>
<section id="hipblaslt-benchmarking">
<span id="mi300x-hipblaslt"></span><h4>hipBLASLt benchmarking<a class="headerlink" href="#hipblaslt-benchmarking" title="Link to this heading">#</a></h4>
<p>The GEMM library
<a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLASLt/en/latest/index.html">hipBLASLt</a>
provides a benchmark tool for its supported operations. Refer to the
<a class="reference external" href="https://github.com/ROCm/hipBLASLt/blob/develop/clients/benchmarks/README.md">documentation</a>
for details.</p>
<ul>
<li><p>Example 1: Benchmark mix fp8 GEMM</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">HIP_FORCE_DEV_KERNARG</span><span class="o">=</span><span class="m">1</span><span class="w">  </span>hipblaslt-bench<span class="w"> </span>--alpha<span class="w"> </span><span class="m">1</span><span class="w"> </span>--beta<span class="w"> </span><span class="m">0</span><span class="w"> </span>-r
f16_r<span class="w"> </span>--a_type<span class="w"> </span>f16_r<span class="w"> </span>--b_type<span class="w"> </span>f8_r<span class="w"> </span>--compute_type<span class="w"> </span>f32_f16_r
--initialization<span class="w"> </span>trig_float<span class="w">  </span>--cold_iters<span class="w"> </span><span class="m">100</span><span class="w"> </span>-i<span class="w"> </span><span class="m">1000</span><span class="w"> </span>--rotating<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
</li>
<li><p>Example 2: Benchmark forward epilogues and backward epilogues</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_RELU: "--activation_type</span> <span class="pre">relu";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_BIAS: "--bias_vector";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_RELU_BIAS: "--activation_type</span> <span class="pre">relu</span> <span class="pre">--bias_vector";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_GELU: "--activation_type</span> <span class="pre">gelu";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_DGELU":</span> <span class="pre">--activation_type</span> <span class="pre">gelu</span> <span class="pre">--gradient";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_GELU_BIAS: "--activation_type</span> <span class="pre">gelu</span> <span class="pre">--bias_vector";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_GELU_AUX: "--activation_type</span> <span class="pre">gelu</span> <span class="pre">--use_e";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_GELU_AUX_BIAS: "--activation_type</span> <span class="pre">gelu</span> <span class="pre">--bias_vector</span> <span class="pre">--use_e";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_DGELU_BGRAD: "--activation_type</span> <span class="pre">gelu</span> <span class="pre">--bias_vector</span> <span class="pre">--gradient";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_BGRADA: "--bias_vector</span> <span class="pre">--gradient</span> <span class="pre">--bias_source</span> <span class="pre">a";</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIPBLASLT_EPILOGUE_BGRADB: </span> <span class="pre">"--bias_vector</span> <span class="pre">--gradient</span> <span class="pre">--bias_source</span> <span class="pre">b";</span></code></p></li>
</ul>
</li>
</ul>
</section>
<section id="hipblaslt-backend-assembly-generator-tuning">
<h4>hipBLASLt backend assembly generator tuning<a class="headerlink" href="#hipblaslt-backend-assembly-generator-tuning" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/hipBLASLt/en/develop/index.html" title="(in hipBLASLt Documentation v0.12.0)"><span class="xref std std-doc">hipBLASLt</span></a> has a backend assembly generator in
<a class="reference external" href="https://github.com/ROCm/hipBLASLt/tree/develop/tensilelite">hipBLASLt’s GitHub repository</a>,
named TensileLite. TensileLite is used to tune the backend assembly generator to
achieve optimal performance. Here’s how to tune hipBLASLt using TensileLite:</p>
<section id="tune-hipblaslt-s-backend-assembly-generator">
<h5>Tune hipBLASLt’s backend assembly generator<a class="headerlink" href="#tune-hipblaslt-s-backend-assembly-generator" title="Link to this heading">#</a></h5>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/hipBLASLt/tensilelite
./Tensile/bin/Tensile<span class="w"> </span>config.yaml<span class="w"> </span>output_path
</pre></div>
</div>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">config.yaml</span></code></dt><dd><p>This file contains the parameters and settings for the tuning process. Here’s
a breakdown of the important sections:</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">GlobalParameters</span></code></dt><dd><p>The set of parameters which provides context for the entire tuning exercise.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">0</span></code> for <code class="docutils literal notranslate"><span class="pre">NumElementsToValidate</span></code> is suggested for performance tuning to avoid validation overhead.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">globalParameters</span><span class="p">[</span><span class="s2">"NumElementsToValidate"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">BenchmarkProblems</span></code></dt><dd><p>Defines the set of kernel specifications as well as the size definitions
for the tuning exercise.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ProblemType</span></code> (<code class="docutils literal notranslate"><span class="pre">OperationType</span></code>, <code class="docutils literal notranslate"><span class="pre">DataType</span></code>, <code class="docutils literal notranslate"><span class="pre">TransposeA</span></code>, <code class="docutils literal notranslate"><span class="pre">TransposeB</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">BenchmarkCommonParameters</span></code> (the same parameters for all solutions)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ForkParameters</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">BenchmarkFinalParameters</span></code> (<code class="docutils literal notranslate"><span class="pre">ProblemSizes</span></code>)</p></li>
</ul>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">LibraryLogic</span></code></dt><dd><p>Specifies the target environment and platform.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ScheduleName</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">aldebaran</span></code> is MI200</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">aquavanjaram</span></code> is MI300</p></li>
</ul>
</li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ls
aldebaran<span class="w">  </span>aquavanjaram<span class="w">  </span>navi31<span class="w">  </span>navi32
</pre></div>
</div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">LibraryLogic</span><span class="p">:</span>
<span class="w">  </span><span class="nt">ScheduleName</span><span class="p">:</span><span class="w"> </span><span class="s">"aldebaran"</span>
<span class="w">  </span><span class="nt">DeviceNames</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">Device 0050</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">Device 0052</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">Device 0054</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">Device 0062</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">Device 7400</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">ArchitectureName</span><span class="p">:</span><span class="w"> </span><span class="s">"gfx90a"</span>
</pre></div>
</div>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">LibraryClient</span></code></dt><dd><p>If defined, this will enable step 4 of the tuning process, which means the final
library will be created.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>ls
aldebaran_Cijk_Ailk_Bjlk_S.yaml
</pre></div>
</div>
</dd>
</dl>
</dd>
</dl>
<figure class="align-center" id="id5">
<img alt="TensileLite tuning flow" src="../../../_images/tensilelite-tuning-flow.png"/>
<figcaption>
<p><span class="caption-text">TensileLite tuning flow</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="update-logic-yaml-files">
<h5>Update logic YAML files<a class="headerlink" href="#update-logic-yaml-files" title="Link to this heading">#</a></h5>
<p>The logic YAML files in hipBLASLt are located in
<code class="docutils literal notranslate"><span class="pre">library/src/amd_detail/rocblaslt/src/Tensile/Logic/asm_full/</span></code>.</p>
<p>To merge the YAML files from the tuned results in TensileLite, use the
<code class="docutils literal notranslate"><span class="pre">merge.py</span></code> located in <code class="docutils literal notranslate"><span class="pre">tensilelite/Tensile/Utilities</span></code> with the following
command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>merge.py<span class="w"> </span>original_dir<span class="w"> </span>new_tuned_yaml_dir<span class="w"> </span>output_dir
</pre></div>
</div>
<p>The following table describes the logic YAML files.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Logic YAML</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Equality</span></code></p></td>
<td><p>Update the equality file when your tuned YAML is
an exact tuning.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GridBased</span></code></p></td>
<td><p>Update the gridbased file when your tuned YAML is
a grid-based tuning.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">FreeSize</span></code></p></td>
<td><p>Update the freesize file when your tuned YAML
contains confidential sizes, or others. Note that
freesize YAML files do not require any problem size.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="tensile-optimization-and-performance-tuning">
<h4>Tensile optimization and performance tuning<a class="headerlink" href="#tensile-optimization-and-performance-tuning" title="Link to this heading">#</a></h4>
<dl class="simple">
<dt>MI16x16 versus MI32x32</dt><dd><p>MI16x16 outperforms MI32x32 due to its superior power efficiency. The MI16x16
format refers to the <code class="docutils literal notranslate"><span class="pre">v_mfma</span></code> instruction (such as
<code class="docutils literal notranslate"><span class="pre">v_mfma_f32_16x16x16f16</span></code>). See
<a class="reference external" href="https://llvm.org/docs/AMDGPU/AMDGPUAsmGFX940.html#vop3p">https://llvm.org/docs/AMDGPU/AMDGPUAsmGFX940.html#vop3p</a>.</p>
</dd>
<dt>Clock differences among XCDs</dt><dd><p>There can be a clock speed variation of 3% to 10% among different XCDs.
Typically, XCD0 has the highest clock speed, while XCD7 has the lowest on
MI300X. For optimal efficiency calculations on MI300X, use the XCD with the
lowest average clock speed. If the average clock speed of XCD0 is used,
target efficiencies (such as, 95% for DGEMM HPL cases with K=512) may not be
achievable.</p>
</dd>
<dt><cite>WorkGroupMapping</cite></dt><dd><p>To maximize L2 cache efficiency, use multiples of the XCD number. For MI300X,
this means using multiples of 8 (such as, 24, 32, 40).</p>
</dd>
<dt>GEMM stride issues</dt><dd><p>On MI300, if the matrix stride in GEMM is a multiple of 512 bytes, it can lead to
Tagram channel hotspotting issues, causing a significant performance drop, especially for TN
transpose cases. This can increase the latency of VMEM instructions and cause
a notable performance drop. To avoid this, use stride padding to ensure the
stride is not a multiple of 512 bytes (for instance, for TN F16 GEMM, set
<code class="docutils literal notranslate"><span class="pre">lda</span> <span class="pre">=</span> <span class="pre">ldb</span> <span class="pre">=</span> <span class="pre">K</span> <span class="pre">+</span> <span class="pre">128</span></code> when <code class="docutils literal notranslate"><span class="pre">K</span> <span class="pre">%</span> <span class="pre">256</span> <span class="pre">==</span> <span class="pre">0</span></code>).</p>
</dd>
</dl>
</section>
<section id="optimizing-composable-kernel-gemm-kernels">
<span id="mi300x-ck"></span><h4>Optimizing Composable Kernel GEMM kernels<a class="headerlink" href="#optimizing-composable-kernel-gemm-kernels" title="Link to this heading">#</a></h4>
<p>The performance of a GEMM kernel is significantly influenced by the input
values. The performance hierarchy based on input value types, from highest to
lowest, is as follows:</p>
<ul class="simple">
<li><p>Case 1: [all 0]</p></li>
<li><p>Case 2: [all identical integers]</p></li>
<li><p>Case 3: [random integers]</p></li>
<li><p>Case 4: [random floats]</p></li>
</ul>
<p>There can be more than a 20 percent performance drop between Case 1 and Case 4,
and a 10 percent drop between random integers and random floats.</p>
<p>Additionally, <code class="docutils literal notranslate"><span class="pre">bf16</span></code> matrix core execution is noticeably faster than <code class="docutils literal notranslate"><span class="pre">f16</span></code>.</p>
<p>Distributing workgroups with data sharing on the same XCD can enhance
performance (reduce latency) and improve benchmarking stability.</p>
<p>CK provides a rich set of template parameters for generating flexible accelerated
computing kernels for difference application scenarios.</p>
<p>See <a class="reference internal" href="../../llm-fine-tuning-optimization/optimizing-with-composable-kernel.html"><span class="doc">Optimizing with Composable Kernel</span></a>
for an overview of Composable Kernel GEMM kernels, information on tunable
parameters, and examples.</p>
</section>
</section>
<section id="miopen">
<span id="mi300x-miopen"></span><h3>MIOpen<a class="headerlink" href="#miopen" title="Link to this heading">#</a></h3>
<p>MIOpen is AMD’s open-source, deep learning primitives library for GPUs. It
implements fusion to optimize for memory bandwidth and GPU launch overheads,
providing an auto-tuning infrastructure to overcome the large design space of
problem configurations.</p>
<section id="convolution">
<h4>Convolution<a class="headerlink" href="#convolution" title="Link to this heading">#</a></h4>
<p>Many of MIOpen kernels have parameters which affect
their performance. Setting these kernel parameters to optimal values
for a given convolution problem, allows reaching the best possible
throughput. The optimal values of these kernel parameters are saved
in PerfDb (Performance database). PerfDb is populated through
tuning. To manipulate the tuning level, use the environment variable
<code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_ENFORCE</span></code> (1-6). Optimal values of kernel parameters are
used to benchmark all applicable convolution kernels for the given
convolution problem. These values reside in the FindDb. To manipulate
how to find the best performing kernel for a given convolution
problem, use the environment variable <code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_MODE</span></code> (1-5).</p>
</section>
<section id="tuning-in-miopen">
<span id="mi300x-miopen-tuning"></span><h4>Tuning in MIOpen<a class="headerlink" href="#tuning-in-miopen" title="Link to this heading">#</a></h4>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_ENFORCE=DB_UPDATE</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code></dt><dd><p>Performs auto-tuning and update to the PerfDb.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_ENFORCE=SEARCH</span></code>, <code class="docutils literal notranslate"><span class="pre">3</span></code></dt><dd><p>Only perform auto-tuning if PerfDb does not contain optimized value for a
given convolution problem</p>
</dd>
</dl>
<p>What does <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/develop/conceptual/perfdb.html" title="(in MIOpen Documentation v3.3.0)"><span class="xref std std-doc">PerfDb</span></a> look like?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
 <span class="mi">2</span><span class="n">x128x56xNHWCxF</span><span class="p">,</span> <span class="p">[</span>
                  <span class="n">ConvAsm1x1U</span>          <span class="p">:</span>  <span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span> <span class="p">;</span>       <span class="o">//</span> <span class="n">optimum</span> <span class="n">kernel</span> <span class="n">params</span> <span class="k">for</span> <span class="n">convolution</span> <span class="n">problem</span> <span class="mi">2</span><span class="n">x128x56xNHWCxF</span>
                  <span class="n">ConvOclDirectFwd1x1</span>  <span class="p">:</span> <span class="mi">1</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">;</span>     <span class="o">//</span> <span class="n">optimum</span> <span class="n">kernel</span> <span class="n">params</span> <span class="k">for</span> <span class="n">convolution</span> <span class="n">problem</span> <span class="mi">2</span><span class="n">x128x56xNHWCxF</span>
                  <span class="p">],</span>
<span class="mi">2</span><span class="n">x992x516xNHWCxF</span><span class="p">,</span> <span class="p">[</span>
                  <span class="n">ConvAsm1x1U</span>          <span class="p">:</span>  <span class="mi">64</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">41</span><span class="p">,</span><span class="mi">6</span> <span class="p">;</span>    <span class="o">//</span> <span class="n">optimum</span> <span class="n">kernel</span> <span class="n">params</span> <span class="k">for</span> <span class="n">convolution</span> <span class="n">problem</span> <span class="mi">2</span><span class="n">x992x516xNHWCxF</span>
                  <span class="n">ConvOclDirectFwd1x1</span>  <span class="p">:</span> <span class="mi">54</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">23</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span>  <span class="o">//</span> <span class="n">optimum</span> <span class="n">kernel</span> <span class="n">params</span> <span class="k">for</span> <span class="n">convolution</span> <span class="n">problem</span> <span class="mi">2</span><span class="n">x992x516xNHWCxF</span>
                  <span class="p">]</span>
 <span class="o">...</span>
<span class="p">]</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/develop/conceptual/perfdb.html" title="(in MIOpen Documentation v3.3.0)"><span>Using the performance database</span></a> for more information.</p>
</section>
<section id="finding-the-fastest-kernel">
<h4>Finding the fastest kernel<a class="headerlink" href="#finding-the-fastest-kernel" title="Link to this heading">#</a></h4>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_MODE=NORMAL</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code></dt><dd><p>Benchmark all the solvers and return a list (front element is the fastest kernel).</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_MODE=FAST</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code></dt><dd><p>Check FindDb (Find database) if convolution problem is found return - else
immediate fallback mode (predict the performing kernel parameters based on
mathematical and AI models).</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">MIOPEN_FIND_MODE=HYBRID</span></code>, <code class="docutils literal notranslate"><span class="pre">3</span></code></dt><dd><p>Check FindDb if convolution problem is found return - else benchmark that
problem.</p>
</dd>
</dl>
<p>What does <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/develop/conceptual/finddb.html" title="(in MIOpen Documentation v3.3.0)"><span class="xref std std-doc">FindDb</span></a> look like?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>

 <span class="mi">2</span><span class="n">x128x56xNHWCxF</span><span class="p">,</span> <span class="p">[</span>
                  <span class="n">ConvAsm1x1U</span>          <span class="p">:</span>  <span class="mf">0.045</span> <span class="p">(</span><span class="n">time</span><span class="p">),</span> <span class="mi">12312</span> <span class="p">(</span><span class="n">workspace</span><span class="p">),</span> <span class="n">algo_type</span><span class="p">;</span>
                  <span class="n">ConvOclDirectFwd1x1</span>  <span class="p">:</span> <span class="mf">1.145</span> <span class="p">(</span><span class="n">time</span><span class="p">),</span> <span class="mi">0</span> <span class="p">(</span><span class="n">workspace</span><span class="p">),</span> <span class="n">algo_type</span><span class="p">;</span>
                  <span class="p">],</span>

<span class="mi">2</span><span class="n">x992x516xNHWCxF</span><span class="p">,</span> <span class="p">[</span>
                  <span class="n">ConvAsm1x1U</span>          <span class="p">:</span>  <span class="mf">2.045</span> <span class="p">(</span><span class="n">time</span><span class="p">),</span> <span class="mi">12312</span> <span class="p">(</span><span class="n">workspace</span><span class="p">),</span> <span class="n">algo_type</span><span class="p">;</span>
                  <span class="n">ConvOclDirectFwd1x1</span>  <span class="p">:</span> <span class="mf">1.145</span> <span class="p">(</span><span class="n">time</span><span class="p">),</span> <span class="mi">0</span> <span class="p">(</span><span class="n">workspace</span><span class="p">),</span> <span class="n">algo_type</span><span class="p">;</span>
                  <span class="p">]</span>
 <span class="o">...</span>
<span class="p">]</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://rocm.docs.amd.com/projects/MIOpen/en/develop/how-to/find-and-immediate.html" title="(in MIOpen Documentation v3.3.0)"><span>Using the find APIs and immediate mode</span></a> for more information.</p>
<p>For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">MIOPEN_FIND_ENFORCE</span><span class="o">=</span><span class="m">3</span><span class="w"> </span><span class="nv">MIOPEN_FIND_MODE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>./bin/MIOpenDriver<span class="w"> </span>convbfp16<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">1024</span><span class="w"> </span>-H<span class="w"> </span><span class="m">14</span><span class="w"> </span>-W<span class="w"> </span><span class="m">14</span><span class="w"> </span>-k<span class="w"> </span><span class="m">256</span><span class="w"> </span>-y<span class="w"> </span><span class="m">1</span><span class="w"> </span>-x<span class="w"> </span><span class="m">1</span><span class="w"> </span>-p<span class="w"> </span><span class="m">0</span><span class="w"> </span>-q<span class="w"> </span><span class="m">0</span><span class="w"> </span>-u<span class="w"> </span><span class="m">1</span><span class="w"> </span>-v<span class="w"> </span><span class="m">1</span><span class="w"> </span>-l<span class="w"> </span><span class="m">1</span><span class="w"> </span>-j<span class="w"> </span><span class="m">1</span><span class="w"> </span>-m<span class="w"> </span>conv<span class="w"> </span>-g<span class="w"> </span><span class="m">1</span><span class="w"> </span>-F<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</section>
</section>
<section id="rccl">
<span id="mi300x-rccl"></span><h3>RCCL<a class="headerlink" href="#rccl" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://rocm.docs.amd.com/projects/rccl/en/develop/index.html" title="(in RCCL Documentation v2.21.5)"><span class="xref std std-doc">RCCL</span></a> is a stand-alone library of standard collective
communication routines for GPUs, implementing all-reduce, all-gather, reduce,
broadcast, reduce-scatter, gather, scatter, and all-to-all. RCCL supports an
arbitrary number of GPUs installed in a single node or multiple nodes
and can be used in either single- or multi-process (such as MPI)
applications.</p>
<p>The following subtopics include information on RCCL features and optimization
strategies:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#mi300x-rccl-8-gpu"><span class="std std-ref">Use all eight GPUs</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-disable-numa"><span class="std std-ref">Disable NUMA auto-balancing</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-disable-acs"><span class="std std-ref">Disable ACS for multi-node RCCL</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-unittests"><span class="std std-ref">Run RCCL-Unittests</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-npkit"><span class="std std-ref">NPKit profiler</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-tests"><span class="std std-ref">RCCL-tests</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-one-process-per-gpu"><span class="std std-ref">Use one-process-per-GPU mode</span></a></p></li>
<li><p><a class="reference internal" href="#mi300x-rccl-e2e"><span class="std std-ref">RCCL in E2E workloads</span></a></p></li>
</ul>
<section id="use-all-eight-gpus">
<span id="mi300x-rccl-8-gpu"></span><h4>Use all eight GPUs<a class="headerlink" href="#use-all-eight-gpus" title="Link to this heading">#</a></h4>
<p>In an <a class="reference internal" href="#mi300x-node-level-arch-fig"><span class="std std-ref">MI300X architecture</span></a>, there are
dedicated links between each pair of GPUs in a fully connected topology.
Therefore, for collective operations, the best performance is achieved
when all 8 GPUs and, hence, all the links between them are used. In the
case of 2- or 4-GPU collective operations (generally less than 8 GPUs),
you can only use a fraction of the potential bandwidth on the node.</p>
<p>The following figure shows an
<a class="reference internal" href="../../../conceptual/gpu-arch/mi300.html"><span class="doc">MI300X node-level architecture</span></a> of a
system with AMD EPYC processors in a dual-socket configuration and eight
AMD Instinct MI300X accelerators. The MI300X OAMs attach to the host system via
PCIe Gen 5 x16 links (yellow lines). The GPUs use seven high-bandwidth,
low-latency AMD Infinity Fabric™ links (red lines) to form a fully connected
8-GPU system.</p>
<figure class="align-default" id="id6">
<span id="mi300x-node-level-arch-fig"></span><img alt="../../../_images/mi300-node-level-arch.png" src="../../../_images/mi300-node-level-arch.png"/>
<figcaption>
<p><span class="caption-text">MI300 series node-level architecture showing 8 fully interconnected MI300X
OAM modules connected to (optional) PCIe switches via re-timers and HGX
connectors.</span><a class="headerlink" href="#id6" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="disable-numa-auto-balancing">
<span id="mi300x-rccl-disable-numa"></span><h4>Disable NUMA auto-balancing<a class="headerlink" href="#disable-numa-auto-balancing" title="Link to this heading">#</a></h4>
<p>In order to reduce performance variability and also achieve better
performance, you need to make sure that NUMA auto-balancing is disabled
on the node.</p>
<p>Check whether NUMA auto-balancing is disabled, by running the
following command: <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/proc/sys/kernel/numa_balancing</span></code> and
checking whether the output is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
<p>If the output is <code class="docutils literal notranslate"><span class="pre">1</span></code>, you can disable NUMA auto-balancing by running the
following command: <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">sysctl</span> <span class="pre">kernel.numa_balancing=0</span></code>. For more
details, see <a class="reference internal" href="../../system-optimization/mi300x.html#mi300x-disable-numa"><span class="std std-ref">AMD Instinct MI300X system optimization</span></a>.</p>
</section>
<section id="disable-acs-for-multi-node-rccl">
<span id="mi300x-rccl-disable-acs"></span><h4>Disable ACS for multi-node RCCL<a class="headerlink" href="#disable-acs-for-multi-node-rccl" title="Link to this heading">#</a></h4>
<p>Check if ACS is disabled with <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">lspci</span> <span class="pre">-vvv</span> <span class="pre">\|</span> <span class="pre">grep</span> <span class="pre">-i</span> <span class="pre">"acsctl"</span></code>.
This will print many lines. Check if there are any that show <code class="docutils literal notranslate"><span class="pre">SrcValid+</span></code></p>
<p>If there are any <code class="docutils literal notranslate"><span class="pre">SrcValid+</span></code>, then use the following <code class="docutils literal notranslate"><span class="pre">disable_acs.sh</span></code> script
to disable ACS (requires <code class="docutils literal notranslate"><span class="pre">sudo</span></code>).</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#</span>

<span class="c1"># Disable ACS on every device that supports it</span>

<span class="c1">#</span>

<span class="nv">PLATFORM</span><span class="o">=</span><span class="k">$(</span>dmidecode<span class="w"> </span>--string<span class="w"> </span>system-product-name<span class="k">)</span>

logger<span class="w"> </span><span class="s2">"PLATFORM=</span><span class="si">${</span><span class="nv">PLATFORM</span><span class="si">}</span><span class="s2">"</span>

<span class="c1"># Enforce platform check here.</span>

<span class="c1">#case "${PLATFORM}" in</span>

<span class="c1">#"OAM"*)</span>

<span class="c1">#logger "INFO: Disabling ACS is no longer necessary for ${PLATFORM}"</span>

<span class="c1">#exit 0</span>

<span class="c1">#;;</span>

<span class="c1">#*)</span>

<span class="c1">#;;</span>

<span class="c1">#esac</span>

<span class="c1"># must be root to access extended PCI config space</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">"</span><span class="nv">$EUID</span><span class="s2">"</span><span class="w"> </span>-ne<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">"ERROR: </span><span class="nv">$0</span><span class="s2"> must be run as root"</span>

<span class="nb">exit</span><span class="w"> </span><span class="m">1</span>

<span class="k">fi</span>

<span class="k">for</span><span class="w"> </span>BDF<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="se">\`</span>lspci<span class="w"> </span>-d<span class="w"> </span><span class="s2">"*:*:*"</span><span class="w"> </span><span class="se">\|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">'{print $1}'</span><span class="sb">`</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>

<span class="c1"># skip if it doesn't support ACS</span>

setpci<span class="w"> </span>-v<span class="w"> </span>-s<span class="w"> </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="w"> </span>ECAP_ACS+0x6.w<span class="w"> </span>&gt;<span class="w"> </span>/dev/null<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$?</span><span class="w"> </span>-ne<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>

<span class="c1">#echo "${BDF} does not support ACS, skipping"</span>

<span class="k">continue</span>

<span class="k">fi</span>

logger<span class="w"> </span><span class="s2">"Disabling ACS on \`lspci -s </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="s2">`"</span>

setpci<span class="w"> </span>-v<span class="w"> </span>-s<span class="w"> </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="w"> </span>ECAP_ACS+0x6.w<span class="o">=</span><span class="m">0000</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$?</span><span class="w"> </span>-ne<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>

logger<span class="w"> </span><span class="s2">"Error enabling directTrans ACS on </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="s2">"</span>

<span class="k">continue</span>

<span class="k">fi</span>

<span class="nv">NEW_VAL</span><span class="o">=</span><span class="sb">`</span>setpci<span class="w"> </span>-v<span class="w"> </span>-s<span class="w"> </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="w"> </span>ECAP_ACS+0x6.w<span class="w"> </span><span class="se">\|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">'{print $NF}'</span><span class="se">\`</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">"</span><span class="si">${</span><span class="nv">NEW_VAL</span><span class="si">}</span><span class="s2">"</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="s2">"0000"</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>

logger<span class="w"> </span><span class="s2">"Failed to enabling directTrans ACS on </span><span class="si">${</span><span class="nv">BDF</span><span class="si">}</span><span class="s2">"</span>

<span class="k">continue</span>

<span class="k">fi</span>

<span class="k">done</span>

<span class="nb">exit</span><span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
</section>
<section id="run-rccl-unittests">
<span id="mi300x-rccl-unittests"></span><h4>Run RCCL-Unittests<a class="headerlink" href="#run-rccl-unittests" title="Link to this heading">#</a></h4>
<p>In order to verify RCCL installation and test whether all parts and
units of RCCL work as expected you can run the RCCL-Unittests which is
explained in <a class="github reference external" href="https://github.com/ROCm/rccl?tab=readme-ov-file#tests">ROCm/rccl</a>.</p>
</section>
<section id="npkit-profiler">
<span id="mi300x-rccl-npkit"></span><h4>NPKit profiler<a class="headerlink" href="#npkit-profiler" title="Link to this heading">#</a></h4>
<p>To collect fine-grained trace events in RCCL components, especially in
giant collective GPU kernels you can use the NPKit profiler explained
in <a class="github reference external" href="https://github.com/ROCm/rccl?tab=readme-ov-file#npkit">ROCm/rccl</a>.</p>
</section>
<section id="rccl-tests">
<span id="mi300x-rccl-tests"></span><h4>RCCL-tests<a class="headerlink" href="#rccl-tests" title="Link to this heading">#</a></h4>
<p>RCCL-tests are performance and error-checking tests for RCCL
maintained in <a class="github reference external" href="https://github.com/ROCm/rccl-tests">ROCm/rccl-tests</a>.</p>
<p>These tests are one of the best ways to check the performance of
different collectives provided by RCCL. You can select collectives,
message sizes, datatypes, operations, number of iterations, etc., for
your test, and then rccl-tests deliver performance metrics such as
latency, algorithm bandwidth, and bus bandwidth for each case.</p>
</section>
<section id="use-one-process-per-gpu-mode">
<span id="mi300x-rccl-one-process-per-gpu"></span><h4>Use one-process-per-GPU mode<a class="headerlink" href="#use-one-process-per-gpu-mode" title="Link to this heading">#</a></h4>
<p>RCCL delivers the best performance for collectives when it is configured
in a one-process-per-GPU mode. This is due to the fact that for a
one-process-per-multiple-GPUs configuration, you can run into kernel launch
latency issues. This is because ROCm serializes kernel launches on multiple GPUs
from one process which hurts performance.</p>
</section>
<section id="rccl-in-e2e-workloads">
<span id="mi300x-rccl-e2e"></span><h4>RCCL in E2E workloads<a class="headerlink" href="#rccl-in-e2e-workloads" title="Link to this heading">#</a></h4>
<p>Use the following environment variable to increase the number of
channels used by RCCL when using RCCL in end-to-end workloads to potentially
improve the performance:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export NCCL_MIN_NCHANNELS=112
</pre></div>
</div>
</section>
</section>
</section>
<section id="triton-kernel-performance-optimization">
<span id="mi300x-triton-kernel-performance-optimization"></span><h2>Triton kernel performance optimization<a class="headerlink" href="#triton-kernel-performance-optimization" title="Link to this heading">#</a></h2>
<p>Triton kernel optimization encompasses a variety of strategies aimed at
maximizing the efficiency and performance of GPU computations. These strategies
include
<a class="reference internal" href="#mi300x-triton-gpu-utilization"><span class="std std-ref">optimizing overall GPU resource utilization</span></a>,
<a class="reference internal" href="#mi300x-autotunable-kernel-config"><span class="std std-ref">tuning kernel configurations</span></a>, and
<a class="reference internal" href="#mi300x-assembly-analysis"><span class="std std-ref">leveraging specific hardware features</span></a> to
achieve higher throughput and lower latency.</p>
<section id="auto-tunable-kernel-configurations-and-environment-variables">
<span id="mi300x-autotunable-kernel-config"></span><h3>Auto-tunable kernel configurations and environment variables<a class="headerlink" href="#auto-tunable-kernel-configurations-and-environment-variables" title="Link to this heading">#</a></h3>
<p>Auto-tunable kernel configuration involves adjusting memory access and computational
resources assigned to each compute unit. It encompasses the usage of
<a class="reference internal" href="#mi300x-cu-fig"><span class="std std-ref">LDS</span></a>, register, and task scheduling on a compute unit.</p>
<p>The accelerator or GPU contains global memory, local data share (LDS), and
registers. Global memory has high access latency, but is large. LDS access has
much lower latency, but is smaller. It is a fast on-CU software-managed memory
that can be used to efficiently share data between all work items in a block.
Register access is the fastest yet smallest among the three.</p>
<figure class="align-default" id="id7">
<span id="mi300x-cu-fig"></span><img alt="../../../_images/compute-unit.png" src="../../../_images/compute-unit.png"/>
<figcaption>
<p><span class="caption-text">Schematic representation of a CU in the CDNA2 or CDNA3 architecture.</span><a class="headerlink" href="#id7" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The following is a list of kernel arguments used for tuning performance and
resource allocation on AMD accelerators, which helps in optimizing the
efficiency and throughput of various computational kernels.</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">num_stages=n</span></code></dt><dd><p>Adjusts the number of pipeline stages for different types of kernels. On AMD accelerators, set <code class="docutils literal notranslate"><span class="pre">num_stages</span></code>
according to the following rules:</p>
<ul class="simple">
<li><p>For kernels with a single GEMM, set to <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p>For kernels with two GEMMs fused (Flash Attention, or any other kernel
that fuses 2 GEMMs), set to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p>For kernels that fuse a single GEMM with another non-GEMM operator
(for example ReLU activation), set to <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p>For kernels that have no GEMMs, set to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">waves_per_eu=n</span></code></dt><dd><p>Helps to manage Vector General Purpose Registers (VGPR) usage to achieve
desired occupancy levels. This argument hints to the compiler to reduce VGPR
to achieve <code class="docutils literal notranslate"><span class="pre">n</span></code> occupancy where <code class="docutils literal notranslate"><span class="pre">n</span></code> is a number. The goal is to achieve a
certain occupancy level for each Execution Unit (EU, also called
<a class="reference internal" href="#mi300x-cu-fig"><span class="std std-ref">SIMD Unit</span></a>) to achieve better latency or throughput.
For more information on how to compute occupancy, see
<a class="reference internal" href="#mi300x-compute-kernel-occ"><span class="std std-ref">Compute the occupancy of a kernel</span></a>.</p>
<p>This argument is useful if:</p>
<ul class="simple">
<li><p>The occupancy of the kernel is limited by VGPR usage, and</p></li>
<li><p>The current VGPR usage is only a few above a boundary in
<a class="reference internal" href="#mi300x-occupancy-vgpr-table"><span class="std std-ref">Occupancy related to VGPR usage in an Instinct MI300X accelerator</span></a>.</p></li>
</ul>
</dd>
</dl>
<figure class="align-center" id="id8">
<span id="mi300x-occupancy-vgpr-table"></span><img alt="Occupancy related to VGPR usage in an Instinct MI300X accelerator." src="../../../_images/occupancy-vgpr.png"/>
<figcaption>
<p><span class="caption-text">Occupancy related to VGPRs usage on an Instinct MI300X accelerator</span><a class="headerlink" href="#id8" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For example, according to the table, the available VGPR is 512 per Execution
Unit (EU), and VGPU is allocated at the unit of 16. If the current VGPR usage
is 170, the actual requested VGPR will be 176, so the occupancy is only 2
waves per EU since <span class="math notranslate nohighlight">\(176 \times 3 &gt; 512\)</span>. So, if you set
<code class="docutils literal notranslate"><span class="pre">waves_per_eu</span></code> to 3, the LLVM backend tries to bring VGPR usage down so
that it might fit 3 waves per EU.</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">BLOCK_M</span></code>, <code class="docutils literal notranslate"><span class="pre">BLOCK_N</span></code>, <code class="docutils literal notranslate"><span class="pre">BLOCK_K</span></code></dt><dd><p>Tile sizes to be tuned to balance the memory-to-computation ratio. The goal
is to minimize the memory transfer from global to shared and reuse memory
across different threads. This needs to be tuned. The tile sizes should be
large enough to maximize the efficiency of the memory-to-computation
ratio but small enough to parallelize the greatest number of workgroups at
the grid level.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">matrix_instr_nonkdim</span></code></dt><dd><p>Experimental feature for Flash Attention-like kernels that determines the size of the Matrix Fused Multiply-Add
(MFMA) instruction used.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">matrix_instr_nonkdim</span> <span class="pre">=</span> <span class="pre">16</span></code>: <code class="docutils literal notranslate"><span class="pre">mfma_16x16</span></code> is used.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">matrix_instr_nonkdim</span> <span class="pre">=</span> <span class="pre">32</span></code>: <code class="docutils literal notranslate"><span class="pre">mfma_32x32</span></code> is used.</p></li>
</ul>
<p>For GEMM kernels on an MI300X accelerator, <code class="docutils literal notranslate"><span class="pre">mfma_16x16</span></code> typically outperforms <code class="docutils literal notranslate"><span class="pre">mfma_32x32</span></code>, even for large
tile/GEMM sizes.</p>
</dd>
</dl>
<p>The following is an environment variable used for tuning.</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">OPTIMIZE_EPILOGUE</span></code></dt><dd><p>Setting this variable to <code class="docutils literal notranslate"><span class="pre">1</span></code> can improve performance by removing the <code class="docutils literal notranslate"><span class="pre">convert_layout</span></code> operation in the epilogue.
It should be turned on (set to <code class="docutils literal notranslate"><span class="pre">1</span></code>) in most cases. Setting <code class="docutils literal notranslate"><span class="pre">OPTIMIZE_EPILOGUE=1</span></code> stores the MFMA instruction
results in the MFMA layout directly; this comes at the cost of reduced global store efficiency, but the impact on
kernel execution time is usually minimal.</p>
<p>By default (<code class="docutils literal notranslate"><span class="pre">0</span></code>), the results of MFMA instruction are converted to blocked layout, which leads to <code class="docutils literal notranslate"><span class="pre">global_store</span></code>
with maximum vector length, that is <code class="docutils literal notranslate"><span class="pre">global_store_dwordx4</span></code>.</p>
<p>This is done implicitly with LDS as the intermediate buffer to achieve
data exchange between threads. Padding is used in LDS to avoid bank
conflicts. This usually leads to extra LDS usage, which might reduce
occupancy.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This variable is not turned on by default because it only
works with <code class="docutils literal notranslate"><span class="pre">tt.store</span></code> but not <code class="docutils literal notranslate"><span class="pre">tt.atomic_add</span></code>, which is used in split-k and
stream-k GEMM kernels. In the future, it might be enabled with
<code class="docutils literal notranslate"><span class="pre">tt.atomic_add</span></code> and turned on by default.</p>
</div>
</dd>
</dl>
</section>
<section id="overall-gpu-resource-utilization">
<span id="mi300x-triton-gpu-utilization"></span><h3>Overall GPU resource utilization<a class="headerlink" href="#overall-gpu-resource-utilization" title="Link to this heading">#</a></h3>
<p>As depicted in the following figure, each XCD in
<a class="reference internal" href="../../../conceptual/gpu-arch/mi300.html"><span class="doc">MI300X</span></a> contains 40 compute units (CUs),
with 38 active. Each MI300X contains eight vertical XCDs, and a total of 304
active compute units capable of parallel computation. The first consideration is
the number of CUs a kernel can distribute its task across.</p>
<figure class="align-default" id="id9">
<img alt="../../../_images/xcd-sys-arch.png" src="../../../_images/xcd-sys-arch.png"/>
<figcaption>
<p><span class="caption-text">XCD-level system architecture showing 40 compute units,
each with 32 KB L1 cache, a unified compute system with 4 ACE compute
accelerators, shared 4MB of L2 cache, and a hardware scheduler (HWS).</span><a class="headerlink" href="#id9" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>You can query hardware resources with the command <code class="docutils literal notranslate"><span class="pre">rocminfo</span></code> in the
<code class="docutils literal notranslate"><span class="pre">/opt/rocm/bin</span></code> directory. For instance, query the number of CUs, number of
SIMD, and wavefront size using the following commands.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>rocminfo<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">"Compute Unit"</span>

rocminfo<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">"SIMD"</span>

rocminfo<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">"Wavefront Size"</span>
</pre></div>
</div>
<p>For the MI300X, the goal is to have a minimum of 1024 thread
blocks or workgroups in the grid (kernel), with a preference for
more.</p>
<p>Identifying additional parallelism within the algorithm is necessary to
enhance GPU utilization. For more information and examples, see
<a class="reference external" href="https://arxiv.org/pdf/2402.00025v1">Accelerating A Triton Fused Kernel For W4a16 Quantized Inference With
SplitK Work Decomposition</a>.</p>
</section>
<section id="mlir-analysis">
<span id="mi300x-mlir-analysis"></span><h3>MLIR analysis<a class="headerlink" href="#mlir-analysis" title="Link to this heading">#</a></h3>
<p>Triton includes the following layouts: <strong>blocked</strong>, <strong>shared</strong>, <strong>sliced</strong>, and <strong>MFMA</strong>.</p>
<p>Use the Triton GPU Intermediate Representation (IR) to identify the memory in
which each computation takes place.</p>
<p>Use the environment variable <code class="docutils literal notranslate"><span class="pre">MLIR_ENABLE_DUMP</span></code> to dump MLIR:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MLIR_ENABLE_DUMP</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
<p>The following is a snippet of IR from the Flash Attention decode <code class="docutils literal notranslate"><span class="pre">int4</span></code> KV program. It is to
de-quantize the <code class="docutils literal notranslate"><span class="pre">int4</span></code> key-value from the <code class="docutils literal notranslate"><span class="pre">int4</span></code> data type to <code class="docutils literal notranslate"><span class="pre">fp16</span></code>.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>%190 = tt.load %189 {cache = 1 : i32, evict = 1 : i32, isVolatile =
false} : tensor&lt;1x64xi32, #blocked6&gt; loc(#loc159)

%266 = arith.andi %190, %cst_28 : tensor&lt;1x64xi32, #blocked6&gt;
loc(#loc250)

%267 = arith.trunci %266 : tensor&lt;1x64xi32, #blocked6&gt; to
tensor&lt;1x64xi16, #blocked6&gt; loc(#loc251)

%268 = tt.bitcast %267 : tensor&lt;1x64xi16, #blocked6&gt; -&gt; tensor&lt;1x64xf16,
#blocked6&gt; loc(#loc252)

%269 = triton_gpu.convert_layout %268 : (tensor&lt;1x64xf16, #blocked6&gt;) -&gt;
tensor&lt;1x64xf16, #shared1&gt; loc(#loc252)

%270 = tt.trans %269 : (tensor&lt;1x64xf16, #shared1&gt;) -&gt; tensor&lt;64x1xf16,
#shared2&gt; loc(#loc194)

%276 = triton_gpu.convert_layout %270 : (tensor&lt;64x1xf16, #shared2&gt;) -&gt;
tensor&lt;64x1xf16, #blocked5&gt; loc(#loc254)

%293 = arith.mulf %276, %cst_30 : tensor&lt;64x1xf16, #blocked5&gt;
loc(#loc254)

%295 = arith.mulf %292, %294 : tensor&lt;64x32xf16, #blocked5&gt; loc(#loc264)

%297 = arith.addf %295, %296 : tensor&lt;64x32xf16, #blocked5&gt; loc(#loc255)

%298 = triton_gpu.convert_layout %297 : (tensor&lt;64x32xf16, #blocked5&gt;)
-&gt; tensor&lt;64x32xf16, #shared1&gt; loc(#loc255)

%299 = tt.trans %298 : (tensor&lt;64x32xf16, #shared1&gt;) -&gt;
tensor&lt;32x64xf16, #shared2&gt; loc(#loc196)

%300 = triton_gpu.convert_layout %299 : (tensor&lt;32x64xf16, #shared2&gt;) -&gt;
tensor&lt;32x64xf16, #triton_gpu.dot_op&lt;{opIdx = 1, parent = #mfma, kWidth
= 4}&gt;&gt; loc(#loc197)
</pre></div>
</div>
<p>From the IR snippet, you can see <code class="docutils literal notranslate"><span class="pre">i32</span></code> data is loaded from global memory to
registers (<code class="docutils literal notranslate"><span class="pre">%190</span></code>). With a few element-wise operations in registers, it is
stored in shared memory (<code class="docutils literal notranslate"><span class="pre">%269</span></code>) for the transpose operation (<code class="docutils literal notranslate"><span class="pre">%270</span></code>), which
needs data movement across different threads. With the transpose done, it is
loaded from LDS to register again (<code class="docutils literal notranslate"><span class="pre">%276</span></code>), and with a few more
element-wise operations, it is stored to LDS again (<code class="docutils literal notranslate"><span class="pre">%298</span></code>). The last step
loads from LDS to registers and converts to the dot-operand layout
(<code class="docutils literal notranslate"><span class="pre">%300</span></code>).</p>
<p>The IR snippet uses the LDS twice. The first is for the transpose, and
the second is to convert a blocked layout to a dot operand layout.
There’s an opportunity to optimize performance by using LDS once.</p>
</section>
<section id="isa-assembly-analysis">
<span id="mi300x-assembly-analysis"></span><h3>ISA assembly analysis<a class="headerlink" href="#isa-assembly-analysis" title="Link to this heading">#</a></h3>
<p>To generate ISA, <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">AMDGCN_ENABLE_DUMP=1</span></code> when running the Triton
program. The generated ISA will be printed as standard output. You can
dump it to a file for analysis.</p>
<ul class="simple">
<li><p>Ensure <code class="docutils literal notranslate"><span class="pre">global_load_dwordx4</span></code> is used in the ISA, especially when the
global memory load happens in the loop.</p></li>
<li><p>In most cases, the LDS load and store should use <code class="docutils literal notranslate"><span class="pre">_b128</span></code> to
minimize the number of LDS access instructions.</p></li>
<li><p>The AMD ISA has <code class="docutils literal notranslate"><span class="pre">s_waitcnt</span></code> instruction to synchronize the dependency
of memory access and computations. The <code class="docutils literal notranslate"><span class="pre">s_waitcnt</span></code> instructions can
typically have two signals in the Triton context:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">lgkmcnt(n)</span></code>: <code class="docutils literal notranslate"><span class="pre">lgkm</span></code> stands for LDS, GDS
(Global Data Share), Constant, and Message. It is often related to
LDS access. The <code class="docutils literal notranslate"><span class="pre">n</span></code> indicates the number of data accesses can still
be ongoing before moving on to the next step. For example, if <code class="docutils literal notranslate"><span class="pre">n</span></code> is
<code class="docutils literal notranslate"><span class="pre">0</span></code>, wait for all <code class="docutils literal notranslate"><span class="pre">lgkm</span></code> access to finish before continuing. If <code class="docutils literal notranslate"><span class="pre">n</span></code>
is <code class="docutils literal notranslate"><span class="pre">1</span></code>, move on even if <code class="docutils literal notranslate"><span class="pre">1</span></code> <code class="docutils literal notranslate"><span class="pre">lgkm</span></code> access is still running
asynchronously.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vmcnt(n)</span></code>: <code class="docutils literal notranslate"><span class="pre">vm</span></code> represents vector memory. This happens when
vector memory is accessed, for example, when global load moves
from global memory to vector memory. The variable <code class="docutils literal notranslate"><span class="pre">n</span></code> is the same as
the previous setting.</p></li>
</ul>
</li>
</ul>
<p>Generally recommended guidelines are as follows.</p>
<ul class="simple">
<li><p>Vectorize memory access as much as possible.</p></li>
<li><p>Ensure synchronization is done efficiently.</p></li>
<li><p>Overlap of instructions to hide latency, but it requires thoughtful
analysis of the algorithms.</p></li>
<li><p>If you find inefficiencies, you can trace it back to LLVM IR, TTGIR
and even TTIR to see where the problem comes from. If you find it
during compiler optimization, activate the MLIR dump
(<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MLIR_ENABLE_DUMP=1</span></code>) and check which optimization pass caused the
problem.</p></li>
</ul>
</section>
</section>
<section id="hip-performance-optimization">
<span id="mi300x-hip-optimization"></span><h2>HIP performance optimization<a class="headerlink" href="#hip-performance-optimization" title="Link to this heading">#</a></h2>
<p>This section summarizes the best practices described in the
<span class="xref std std-doc">Performance guidelines</span> section of the
HIP documentation.</p>
<p>Optimization areas of concern include:</p>
<ul class="simple">
<li><p>Parallel execution</p></li>
<li><p>Memory usage optimization</p></li>
<li><p>Optimization for maximum throughput</p></li>
<li><p>Minimizing memory thrashing</p></li>
</ul>
<section id="parallel-execution-and-gpu-hardware-utilization">
<h3>Parallel execution and GPU hardware utilization<a class="headerlink" href="#parallel-execution-and-gpu-hardware-utilization" title="Link to this heading">#</a></h3>
<p>The application should reveal and efficiently imply as much parallelism as
possible for optimal use to keep all system components active.</p>
</section>
<section id="memory-usage-optimization">
<h3>Memory usage optimization<a class="headerlink" href="#memory-usage-optimization" title="Link to this heading">#</a></h3>
<p>To optimize memory throughput, minimize low-bandwidth data transfers,
particularly between the host and device. Maximize on-chip memory, including
shared memory and caches, to reduce data transfers between global memory and the
device.</p>
<p>In a GPU, global memory has high latency but a large size, while local data
share (LDS) has lower latency but a smaller size, and registers have the fastest
but smallest access. Aim to limit load/store operations in global memory. If
multiple threads in a block need the same data, transfer it from global memory
to LDS for efficient access.</p>
<p>See <span class="xref std std-doc">HIP’s performance guidelines</span> for
greater detail.</p>
</section>
</section>
<section id="diagnostic-and-performance-analysis">
<h2>Diagnostic and performance analysis<a class="headerlink" href="#diagnostic-and-performance-analysis" title="Link to this heading">#</a></h2>
<section id="debug-memory-access-faults">
<span id="mi300x-rocr-debug-agent"></span><h3>Debug memory access faults<a class="headerlink" href="#debug-memory-access-faults" title="Link to this heading">#</a></h3>
<p>Identifying a faulting kernel is often enough to triage a memory access
fault. The ROCr Debug Agent can trap a memory access fault and provide a
dump of all active wavefronts that caused the error, as well as the name
of the kernel. For more information, see
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocr_debug_agent/en/amd-staging/index.html" title="(in rocr_debug_agent v2.0.4)"><span class="xref std std-doc">ROCr Debug Agent documentation</span></a>.</p>
<p>To summarize, the key points include:</p>
<ol class="arabic simple">
<li><p>Compiling with <code class="docutils literal notranslate"><span class="pre">-ggdb</span> <span class="pre">-O0</span></code> is recommended but not required.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HSA_TOOLS_LIB=/opt/rocm/lib/librocm-debug-agent.so.2</span> <span class="pre">HSA_ENABLE_DEBUG=1</span> <span class="pre">./my_program</span></code></p></li>
</ol>
<p>When the debug agent traps the fault, it produces verbose output of all
wavefront registers and memory content. Importantly, it also prints
something similar to the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Disassembly for function vector_add_assert_trap(int*, int*, int*):

code object:
file:////rocm-debug-agent/build/test/rocm-debug-agent-test#offset=14309&amp;size=31336

loaded at: [0x7fd4f100c000-0x7fd4f100e070]
</pre></div>
</div>
<p>The kernel name and the code object file should be listed. In the
example above, the kernel name is vector_add_assert_trap, but this might
also look like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Disassembly for function memory:///path/to/codeobject#offset=1234&amp;size=567:
</pre></div>
</div>
<p>In this case, it’s an in-memory kernel that was generated at runtime.
Using the environment variable <code class="docutils literal notranslate"><span class="pre">ROCM_DEBUG_AGENT_OPTIONS="--all</span> <span class="pre">--save-code-objects"</span></code>
will have the debug agent save all code objects to the current directory. Use
<code class="docutils literal notranslate"><span class="pre">--save-code-objects=[DIR]</span></code> to save them in another location.</p>
<p>The code objects will be renamed from the URI format with special
characters replaced by ‘_’. Use <code class="docutils literal notranslate"><span class="pre">llvm-objdump</span></code> to disassemble the
indicated in-memory code object that has been saved to disk. The name of
the kernel is often found in the disassembled code object.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>llvm-objdump<span class="w"> </span>--disassemble-all<span class="w"> </span>path/to/code-object.co
</pre></div>
</div>
<p>Disabling memory caching strategies within the ROCm stack and PyTorch is
recommended, where possible. This gives the debug agent the best chance
of finding the memory fault where it originates. Otherwise, it could be
masked by writing past the end of a cached block within a larger
allocation.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PYTORCH_NO_HIP_MEMORY_CACHING=1

HSA_DISABLE_FRAGMENT_ALLOCATOR=1
</pre></div>
</div>
</section>
<section id="compute-the-occupancy-of-a-kernel">
<span id="mi300x-compute-kernel-occ"></span><h3>Compute the occupancy of a kernel<a class="headerlink" href="#compute-the-occupancy-of-a-kernel" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Get the VGPR count, search for <code class="docutils literal notranslate"><span class="pre">.vgpr_count</span></code> in the ISA (for example,
<code class="docutils literal notranslate"><span class="pre">N</span></code>).</p></li>
<li><p>Get the allocated LDS following the steps (for example, L for the kernel).</p>
<ol class="loweralpha simple">
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MLIR_ENABLE_DUMP=1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rm</span> <span class="pre">-rf</span> <span class="pre">~/.triton/cache</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">kernel.py</span> <span class="pre">|</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">"triton_gpu.shared</span> <span class="pre">=</span> <span class="pre">"</span> <span class="pre">|</span> <span class="pre">tail</span> <span class="pre">-n</span> <span class="pre">1</span></code></p></li>
<li><p>You should see something like <code class="docutils literal notranslate"><span class="pre">triton_gpu.shared</span> <span class="pre">=</span> <span class="pre">65536</span></code>, indicating
65536 bytes of LDS are allocated for the kernel.</p></li>
</ol>
</li>
<li><p>Get number of waves per workgroup using the following steps (for example, <code class="docutils literal notranslate"><span class="pre">nW</span></code>).</p>
<ol class="loweralpha simple">
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MLIR_ENABLE_DUMP=1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rm</span> <span class="pre">-rf</span> <span class="pre">~/.triton/cache</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">kernel.py</span> <span class="pre">|</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">"triton_gpu.num-warps</span> <span class="pre">"</span> <span class="pre">|</span> <span class="pre">tail</span> <span class="pre">-n</span> <span class="pre">1</span></code></p></li>
<li><p>You should see something like <code class="docutils literal notranslate"><span class="pre">“triton_gpu.num-warps"</span> <span class="pre">=</span> <span class="pre">8</span></code>, indicating 8
waves per workgroup.</p></li>
</ol>
</li>
<li><p>Compute occupancy limited by VGPR based on N according to the
<a class="reference internal" href="#mi300x-occupancy-vgpr-table"><span class="std std-ref">preceding table</span></a>. For example, waves per
EU as <code class="docutils literal notranslate"><span class="pre">occ_vgpr</span></code>.</p></li>
<li><p>Compute occupancy limited by LDS based on L by: <code class="docutils literal notranslate"><span class="pre">occ_lds</span> <span class="pre">=</span> <span class="pre">floor(65536</span> <span class="pre">/</span> <span class="pre">L)</span></code>.</p></li>
<li><p>Then the occupancy is <code class="docutils literal notranslate"><span class="pre">occ</span> <span class="pre">=</span> <span class="pre">min(floor(occ_vgpr</span> <span class="pre">*</span> <span class="pre">4</span> <span class="pre">/</span> <span class="pre">nW),</span> <span class="pre">occ_lds)</span> <span class="pre">*</span> <span class="pre">nW</span> <span class="pre">/</span> <span class="pre">4</span></code></p>
<ol class="loweralpha simple">
<li><p><code class="docutils literal notranslate"><span class="pre">occ_vgpr</span> <span class="pre">\*</span> <span class="pre">4</span></code> gives the total number of waves on all 4 execution units (SIMDs)
per CU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">floor(occ_vgpr</span> <span class="pre">*</span> <span class="pre">4</span> <span class="pre">/</span> <span class="pre">nW)</span></code> gives the occupancy of workgroups per CU
regrading VGPR usage.</p></li>
<li><p>The true <code class="docutils literal notranslate"><span class="pre">occ</span></code> is the minimum of the two.</p></li>
</ol>
</li>
</ol>
<p>Find the full <code class="docutils literal notranslate"><span class="pre">occ.sh</span></code> at
<a class="github reference external" href="https://github.com/ROCm/triton/blob/triton-mlir/scripts/amd/occ.sh">ROCm/triton</a>.</p>
</section>
</section>
<section id="special-considerations">
<h2>Special considerations<a class="headerlink" href="#special-considerations" title="Link to this heading">#</a></h2>
<section id="multi-gpu-communications">
<h3>Multi-GPU communications<a class="headerlink" href="#multi-gpu-communications" title="Link to this heading">#</a></h3>
<p>Because of the characteristics of MI300X inter-GPU communication and
limitation of bandwidth between/among 2 GPUs and 4 GPUs, avoid running
workloads that use 2 or 4 GPU collectives. It’s optimal to either use a
single GPU (where no collective is required) or employ 8 GPU
collectives.</p>
</section>
<section id="multi-node-fsdp-and-rccl-settings">
<h3>Multi-node FSDP and RCCL settings<a class="headerlink" href="#multi-node-fsdp-and-rccl-settings" title="Link to this heading">#</a></h3>
<p>When using PyTorch’s FSDP (Full Sharded Data Parallel) feature, the HIP
streams used by RCCL and HIP streams used for compute kernels do not
always overlap well. To work around the issue, it is recommended to use
high-priority HIP streams with RCCL.</p>
<p>The easiest way to do that is to ensure you’re using the nightly PyTorch
wheels because <a class="reference external" href="https://github.com/pytorch/pytorch/pull/122830">this
PR</a> didn’t make it
into release 2.3 but is part of nightly wheels.</p>
<ul class="simple">
<li><p>Set environment variable <code class="docutils literal notranslate"><span class="pre">TORCH_NCCL_HIGH_PRIORITY=1</span></code> to force all RCCL
streams to be high-priority.</p></li>
<li><p>Set environment variable <code class="docutils literal notranslate"><span class="pre">GPU_MAX_HW_QUEUES=2</span></code> from HIP runtime
library.</p></li>
</ul>
<p>The hardware is most efficient when using 4 HIP streams (or less), and
these two environment variables force a maximum of two streams for
compute and two streams for RCCL. Otherwise, RCCL is often already tuned
for the specific MI300 systems in production based on querying the node
topology internally during startup.</p>
</section>
</section>
</section>
</article>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="system.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">AMD Instinct MI300X system optimization</p>
</div>
</a>
<a class="right-next" href="../../gpu-enabled-mpi.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">GPU-enabled Message Passing Interface</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> Contents
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#workload-tuning-strategy">Workload tuning strategy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measure-the-current-workload">Measure the current workload</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mi300x-profiling-start">Identify tuning requirements</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-profiling-tools">High-level profiling tools</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-level-profiling-tools">Kernel-level profiling tools</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analyze-and-tune">Analyze and tune</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-model-inference-with-vllm">Optimize model inference with vLLM</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-tunable-configurations">Auto-tunable configurations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-tuning">Manual tuning</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterate-and-validate">Iterate and validate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#profiling-tools">Profiling tools</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-profiler">PyTorch Profiler</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-profiling-tools">ROCm profiling tools</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rocprofiler">ROCProfiler</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#omniperf">Omniperf</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#omnitrace">Omnitrace</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-performance-optimization">vLLM performance optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximize-throughput">Maximize throughput</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-vllm-on-multiple-gpus">Run vLLM on multiple GPUs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choose-different-attention-backends">Choose different attention backends</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-fp8-kv-cache-data-type">Use fp8 KV-cache data type</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enable-chunked-prefill">Enable chunked prefill</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-tensor-parallelism-and-gemm-performance">Optimize tensor parallelism and GEMM performance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gemm-tuning-steps">GEMM tuning steps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-tunableop">PyTorch TunableOp</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-inductor-triton-tuning-knobs">PyTorch inductor Triton tuning knobs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rocm-library-tuning">ROCm library tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gemm-general-matrix-multiplication">GEMM (general matrix multiplication)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hipblaslt-benchmarking">hipBLASLt benchmarking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hipblaslt-backend-assembly-generator-tuning">hipBLASLt backend assembly generator tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#tune-hipblaslt-s-backend-assembly-generator">Tune hipBLASLt’s backend assembly generator</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#update-logic-yaml-files">Update logic YAML files</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tensile-optimization-and-performance-tuning">Tensile optimization and performance tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-composable-kernel-gemm-kernels">Optimizing Composable Kernel GEMM kernels</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#miopen">MIOpen</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution">Convolution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-in-miopen">Tuning in MIOpen</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-fastest-kernel">Finding the fastest kernel</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl">RCCL</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#use-all-eight-gpus">Use all eight GPUs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#disable-numa-auto-balancing">Disable NUMA auto-balancing</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#disable-acs-for-multi-node-rccl">Disable ACS for multi-node RCCL</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#run-rccl-unittests">Run RCCL-Unittests</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#npkit-profiler">NPKit profiler</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl-tests">RCCL-tests</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#use-one-process-per-gpu-mode">Use one-process-per-GPU mode</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rccl-in-e2e-workloads">RCCL in E2E workloads</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#triton-kernel-performance-optimization">Triton kernel performance optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-tunable-kernel-configurations-and-environment-variables">Auto-tunable kernel configurations and environment variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-gpu-resource-utilization">Overall GPU resource utilization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlir-analysis">MLIR analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#isa-assembly-analysis">ISA assembly analysis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hip-performance-optimization">HIP performance optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallel-execution-and-gpu-hardware-utilization">Parallel execution and GPU hardware utilization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-usage-optimization">Memory usage optimization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostic-and-performance-analysis">Diagnostic and performance analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#debug-memory-access-faults">Debug memory access faults</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-the-occupancy-of-a-kernel">Compute the occupancy of a kernel</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#special-considerations">Special considerations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gpu-communications">Multi-GPU communications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-fsdp-and-rccl-settings">Multi-node FSDP and RCCL settings</a></li>
</ul>
</li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
<p>
</p>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<footer class="rocm-footer">
<div class="container-lg">
<section class="bottom-menu menu py-45">
<div class="row d-flex align-items-center">
<div class="col-12 text-center">
<ul>
<li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
<li><a href="https://rocm.docs.amd.com/en/develop/about/license.html">ROCm Licenses and Disclaimers</a></li>
<li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
<li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
<li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
<li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
<li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
<li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
<!-- OneTrust Cookies Settings button start -->
<li><a class="ot-sdk-show-settings" href="#cookie-settings" id="ot-sdk-btn">Cookie Settings</a></li>
<!-- OneTrust Cookies Settings button end -->
</ul>
</div>
</div>
<div class="row d-flex align-items-center">
<div class="col-12 text-center">
<div>
<span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
</div>
</div>
</div>
</section>
</div>
</footer>
<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
</body>
</html>